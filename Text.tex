\documentclass[8pt]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsopn}
\usepackage{comment}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{bussproofs}
\usepackage[all, 2cell]{xy}
\usepackage[all]{xy}
\usepackage{rotating}
\usepackage{lscape}
\usepackage{minted}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]

\theoremstyle{definition}
\newtheorem{claim}{Claim}[section]

\theoremstyle{definition}
\newtheorem{ex}{Example}[section] 

\theoremstyle{definition}
\newtheorem{cons}{Construction}[section] 

\theoremstyle{definition}
\newtheorem{rem}{Remark}[section] 


\theoremstyle{definition}
\newtheorem{prop}{Proposition}[section]

\theoremstyle{definition}
\newtheorem{lemma}{Lemma}[section]

\theoremstyle{definition}
\newtheorem{fact}{Fact}[section]

\theoremstyle{definition}
\newtheorem{remark}{Remark}[section]

\theoremstyle{definition}
\newtheorem{notation}{Notation}[section]

\theoremstyle{definition}
\newtheorem{example}{Example}[section]

\theoremstyle{definition}
\newtheorem{col}{Corollary}[section]

\theoremstyle{question}
\newtheorem{question}{Question}

\let\strokeL\L
\renewcommand\L{\mathbf{L}}

\title{Some Notes on Proof Theory and Elements of Ordinal Analysis}
\author{Daniel Rogozin}
\date{ }

\begin{document}

\maketitle

\tableofcontents

\newpage

\section{Provable Recursion in ${\bf I}\Delta_0(\operatorname{exp})$}

${\bf I}\Delta_0(\operatorname{exp})$ is a theory in first-order logic in the language:
\begin{center}
  $\{ =, 0, S, P, +, \dot{-}, \cdot, exp_2 \}$
\end{center}
where $S$ and $P$ are successor and precessor functions respectively.
Further, we will denote $S(x)$ and $P(x)$ as $x + 1$ and $x \dot{-} 1$ respectively.
$2^x$ stands for $exp_2(x)$.

The non-logical axioms of ${\bf I}\Delta_0(\operatorname{exp})$ are the following list:

\vspace{\baselineskip}

\begin{minipage}{0.45\textwidth}
  \begin{itemize}
    \item $x + 1 \neq 0$
    \item $0 \dot{-} 1 = 0$
    \item $x + 0 = x$
    \item $x \dot{-} 0 = x$
    \item $x \cdot 0 = 0$
    \item $2^0 = 1$
  \end{itemize}
\end{minipage}%
\hfill
\begin{minipage}{0.45\textwidth}
  \begin{itemize}
    \item $x + 1 = y + 1 \to x = y$
    \item $(x + 1) \dot{-} 1 = x$
    \item $x + (y + 1) = (x + y) + 1$
    \item $x \dot{-} (y + 1) = x \dot{-} y \dot{-} 1$
    \item $x \cdot (y + 1) = x \cdot y + x$
    \item $2^{x + 1} = 2^x + 2^x$
  \end{itemize}
\end{minipage}

\vspace{\baselineskip}

along with the bounded induction scheme:
\begin{center}
  $B(0) \land \forall x (B (x) \to B(x + 1)) \to \forall x B(x)$
\end{center}
where $B$ is a \emph{$\Delta$-formula}, that is a formula one of the following forms (with bounded quantifiers only):
\begin{itemize}
  \item $B \eqcirc \forall x < t P(x) \equiv \forall x (x < t \to P(x))$ 
  \item $B \eqcirc \exists x < t P(x) \equiv \exists x (x < t \land P(x))$
\end{itemize}

A $\Sigma_1$-formula is a formula of the form:
\begin{center}
  $\exists \vec{x} B(\vec{x})$
\end{center}
where $B(\vec{x}) \in \Delta_0$.

\begin{lemma}
  ${\bf I}\Delta_0(\operatorname{exp})$ proves (the universal closures of):
  \begin{enumerate}
    \item $x = 0 \lor x = (x \dot{-} 1) + 1$
    \item $x + (y + z) = (x + y) + z$
    \item $x \cdot (y \cdot z) = (x \cdot y) \cdot z$
    \item $x \cdot (y + z) = x \cdot y + x \cdot z$
    \item $x + y = y + x$
    \item $x \cdot y = y \cdot x$
    \item $x \dot{-} (y + z) = (x \dot{-} y) \dot{-} z$
    \item $2^{x + y} = 2^x \cdot 2^y$
  \end{enumerate}
\end{lemma}

\begin{proof}
$ $

  \begin{enumerate}
    \item This is self-evident.
    \item If $z = 0$, then $x + y = x + y$. If $z = z' + 1$, then, by applying the IH and the relevant axioms:
    \begin{center}
      $(x + (y + (z' + 1))) = (x + ((y + z') + 1)) = (x + (y + z')) + 1 = ((x + y) + z') + 1 = (x + y) + (z' + 1)$
    \end{center}
    \item If $z = 0$, then $x \cdot (y \cdot 0) = (x \cdot y) \cdot 0$. If $z = z' + 1$, then:
    \begin{center}
    $x \cdot (y \cdot (z' + 1)) = x \cdot (y \cdot z' + y) = x \cdot (y \cdot z') + x \cdot y =
    (x \cdot y) \cdot z' + x \cdot y = (x \cdot y) \cdot (z' + 1)$
    \end{center}
    \item The rest of the cases are shown by induction on $z$. Consider the exponentiation law.
    If $y = 0$, then

    \begin{center}
    $2^{x + 0} = 2^{x} = 0 + 2^{x} = 2^{x} \cdot 0 + 2^{x} = 2^{x} \cdot (0 + 1) = 2^x \cdot 2^0$
    \end{center}

    If $y = y' + 1$, then:
    \begin{center}
      $2^{x + (y' + 1)} = 2^{(x + y') + 1} = 2^x \cdot 2^y + 2^x \cdot 2^y = 2^{x} \cdot 2^{y + 1}$
    \end{center}
  \end{enumerate}
\end{proof}

\begin{lemma}
  ${\bf I}\Delta_0(\operatorname{exp})$ proves (the universal closures of):

  \begin{enumerate}
    \item $\neg x < 0$
    \item $x \leq 0 \leftrightarrow x = 0$
    \item $0 \leq x$
    \item $x \leq x$
    \item $x < x + 1$
    \item $x < y + 1 \leftrightarrow x \leq y$
    \item $x \leq y \leftrightarrow x < y \lor x = y$ 
    \item $x \leq y \land y \leq z \to x \leq z$
    \item $x < y \land y < z \to x < z$
    \item $x \leq y \lor y < x$
    \item $x < y \to x + z < y + z$
    \item $x < y \to x \cdot (z + 1) < y \cdot (z + 1)$
    \item $x < 2^x$
    \item $x < y \to 2^x < 2^y$
  \end{enumerate}
\end{lemma}

\begin{proof}
  Straightforward induction.
\end{proof}

\begin{definition}
  A function $f : \mathbb{N}^k \to \mathbb{N}$ is \emph{provably $\Sigma_1$} or \emph{provably recursive}
  in an arithmetical theory if there is a $\Sigma_1$ formula $F(\vec{x}, y)$, a ``defining formula'' of $f$, such that:
  \begin{enumerate}
    \item $f(\vec{n}) = m$ iff $\omega \models f(\vec{n}) = m$
    \item $T \vdash \exists y F(\vec{x}, y)$
    \item $T \vdash F(\vec{x}, y) \land F(\vec{x}, y') \to y = y'$
  \end{enumerate}
\end{definition}
If a defining formula $F \in \Delta_0$, then a function $f$ is \emph{provably bounded} 
in $T$ if there is a term $t(\vec{x})$ such that $T \vdash F(\vec{x}, y) \to y < t(\vec{x})$.

\begin{theorem}
  Let $f$ be a provably recursive in $T$, then we can conservatively extend $T$
  by adding a new function symbol $f$ along with the defining axiom $F(\vec{x}, f(\vec{x}))$.
\end{theorem}

\begin{proof}
  Let $\mathcal{M} \models T$, $\mathcal{M}$ can be made into a model $(\mathcal{M}, f)$ where
  we interpret $f$ as the function which is uniquely determined by the second and third conditions
  of the definitions above.
  Let $\varphi$ be a statement not involving $f$ such that $\varphi$ is true
  in $(\mathcal{M}, f)$, so $\varphi$ is true in $\mathcal{M}$ as well.
  By compactness $T$ proves $\varphi$.
\end{proof}

\begin{lemma}
  Each term defines a provably bounded function of ${\bf I}\Delta_0(\operatorname{exp})$.
\end{lemma}
\begin{proof}
  Let $f$ be a function defined by some ${\bf I}\Delta_0(\operatorname{exp})$-term $t$, 
  that is, $f(\vec{x}) = t(\vec{x})$.
  Take $y = t(\vec{x})$ as the defining formula for $f$ since 
  $\exists y \: (y = t(\vec{x}))$ is derivable.
  If $y' = t(\vec{x}) \land y = t(\vec{x})$, then $y = y'$ by transitivity.
  A formula $y = t(\vec{x})$ is bounded and $y = t$ implies $y < t + 1$.
  Thus $f$ is provably bounded.
\end{proof}

\begin{lemma}~\label{upper:bound:elem}
  Define $2_k(x)$ as $2_0(x) = x$ and $2_{n + 1}(x) = 2^{2_n(x)}$. 
  Then for every term $t(x_1, \dots, x_n)$ built up from the constants $0, S, P, +, \dot{-}, \cdot, exp_2$ there exists $k < \omega$ such that:
  \begin{center}
    ${\bf I}\Delta_0(\operatorname{exp}) \vdash t(x_1, \dots, x_n) < 2_k(\sum \limits_{k = 0}^n x_k)$
  \end{center}
\end{lemma}

\begin{proof}
Let $t$ be a term constructed from subterms $t_0$ and $t_1$ by using one of the function constants.
Assume that inductively $t_0 < 2_{k_0}(s_0)$ and $t_1 < 2_{k_1}(s_1)$ are both provable for some $k_0, k_1 < \omega$, where
$s_i$ is the sum of the variables of $t_i$ for $i = 0, 1$.

Let $s$ be the sum of all variables appearing in either $t_0$ or $t_1$ and let $k = \max(k_0, k_1)$.
Then one can prove $t_0 < 2_{k}(s)$ and $t_1 < 2_{k}(s)$. So one needs to show the following:
\begin{enumerate}
  \item $t_0 + 1 < 2_{k + 1}(s)$
  \item $t_0 \dot{-} 1 < 2_{k}(s)$
  \item $t_0 \dot{-} t_1 < 2_{k}(s)$
  \item $t_0 \cdot t_1 < 2_{k}(s)$
  \item $t_0 + t_1 < 2_{k}(s)$
  \item $2^{t_0} < 2_{k}(s)$
\end{enumerate}
So ${\bf I}\Delta_0(\operatorname{exp}) \vdash t < 2_{k + 1}(s)$.
\end{proof}

\begin{lemma}
  Let $f$ be a function defined by composition:
  \begin{center}
    $f(\vec{x}) = g_0(g_1(\vec{x}), \dots, g_m(\vec{x}))$
  \end{center}
  where $g_0, g_1, \dots, g_m$ are functions each of which is provably bounded in ${\bf I}\Delta_0(\operatorname{exp})$.
  Then $f$ is provably bounded in ${\bf I}\Delta_0(\operatorname{exp})$.
\end{lemma}

\begin{proof}
  Each $g_i$ has a defining formula $G_i$ and, by Lemma~\ref{upper:bound:elem}, there is a number $k_i < \omega$ such that:
  \begin{center}
    ${\bf I}\Delta_0(\operatorname{exp}) \vdash \exists y < 2_{k_i}(s) \: G_i (\vec{x}, y)$
  \end{center}
  where $s$ is the sum of elements of $\vec{x}$. And for $i = 0$ one has:
  \begin{center}
    ${\bf I}\Delta_0(\operatorname{exp}) \vdash \exists y < 2_{k_0}(s_0) \: G_0 (y_1, \dots, y_m, y)$
  \end{center}
  where $s_0$ is the sum of $y_1, \dots, y_m$.

  Let $k = \max \{ k_i < \omega \: | \: i < m + 1 \}$ and let $F(\vec{x}, y)$ be the bounded formula:
  \begin{center}
    $\exists y_1 < 2_{k}(s) \: \dots \: \exists y_m < 2_{k}(s) \: C(\vec{x}, y_1, \dots, y_m, y)$
  \end{center}
  where $C(\vec{x}, y_1, \dots, y_m, y)$ is the conjunction:
  \begin{center}
    $G_1(\vec{x}, y_1) \land \dots \land G_m(\vec{x}, y_m) \land G_0 (y_1, \dots, y_m, y)$
  \end{center}

  $F$ is clearly a defining formula for $f$ such that ${\bf I}\Delta_0(\operatorname{exp}) \vdash \exists y F(\vec{x}, y)$.

  Moreover, each $G_i$ is unique, so ${\bf I}\Delta_0(\operatorname{exp})$ also proves:

  \vspace{\baselineskip}

  $\begin{array}{lll}
    & C(\vec{x}, y_1, \dots, y_m, y) \land C(\vec{x}, z_1, \dots, z_m, z) \to & \\
    & \to \bigwedge \limits_{j = 1}^m y_j = z_j \land G_0(y_1, \dots, y_m, y) \land G_0(y_1, \dots, y_m, z) \to & \\
    & \to y = z&
  \end{array}$

  \vspace{\baselineskip}

  so we have (by first order logic):
  \begin{center}
    ${\bf I}\Delta_0(\operatorname{exp}) \vdash F(\vec{x}, y) \land F(\vec{x}, z) \to y = z$
  \end{center}

  Thus $f$ is provably $\Sigma_1$ in ${\bf I}\Delta_0(\operatorname{exp})$, so the rest is to find its bounding term.

  ${\bf I}\Delta_0(\operatorname{exp})$ proves the following:

  \begin{center}
    $C(\vec{x}, y_1, \dots, y_m, y) \to \bigwedge \limits_{j = 1}^m y_j < 2_k(s) \land y < 2_k(y_1 + \dots + y_m)$
  \end{center}

  and

  \begin{center}
    $\bigwedge \limits_{j = 1}^m y_j < 2_k(s) \to y_1 + \dots + y_m < 2_k(s) \cdot m$
  \end{center}

  Put $t(\vec{x}) = 2_k(2_k(s) \cdot m)$, then we obtain

  \begin{center}
    ${\bf I}\Delta_0(\operatorname{exp}) \vdash C(\vec{x}, y_1, \dots, y_m, y) \to y < t(\vec{x})$
  \end{center}

  and so

  \begin{center}
    ${\bf I}\Delta_0(\operatorname{exp}) \vdash F(\vec{x}, y) \to y < t(\vec{x})$
  \end{center}
\end{proof}

\begin{lemma}
  Suppose $f$ is defined by bounded minimisation
  \begin{center}
    $f(\vec{n}, m) = \mu_{k < m} (g(\vec{n}, k) = 0)$
  \end{center}
  from a function $g$ which is provably bounded in ${\bf I}\Delta_0(\operatorname{exp})$.
  Then $f$ is provably bounded in ${\bf I}\Delta_0(\operatorname{exp})$.
\end{lemma}

\begin{proof}
   Let $G$ be a defining formula for $g$. Let $F(\vec{x}, z, y)$ be the bounded formula
   \begin{center}
    $y \leq z \land \forall i < y \neg G(\vec{x}, i, 0) \land (y = z \lor G(\vec{x}, y, 0))$
   \end{center}

   $\omega \models F(\vec{n}, m, k)$ iff either $k$ is the least number less than $m$ such that $g(\vec{n}, k) = 0$ or 
   there is no such and $k = m$. Thus it means that $k$ is the value of $f(\vec{n}, m)$, so $F$ is a defining formula for $f$.

   Furthermore
   \begin{center}
   ${\bf I}\Delta_0(\operatorname{exp}) \vdash F(\vec{x}, z, y) \to y < z + 1$
   \end{center}
   so $t(\vec{x}, z) = z + 1$ can be taken as a bounding term for $f$.

   We can prove:
   \begin{center}
    $F(\vec{x}, z, y) \land F(\vec{x}, z, y') \land y < y' \to G(\vec{x}, y, 0) \land \neg G(\vec{x}, y, 0)$
   \end{center}
   and similarly for interchanged $y$ and $y'$. So we can prove:
   \begin{center}
    $F(\vec{x}, z, y) \land F(\vec{x}, z, y') \to \neg y < y' \land \neg y' < y$
   \end{center}
   As far as $y < y' \lor y' < y \lor y = y'$, we have
   \begin{center}
    $F(\vec{x}, z, y) \land F(\vec{x}, z, y') \to y = y'$
   \end{center}

   Now we have to check that ${\bf I}\Delta_0(\operatorname{exp}) \vdash \exists y F(\vec{x}, z, y)$.
   We construct such $y$ by bounded induction on $z$.

   \begin{enumerate}
    \item $z = 0$.

    $F(\vec{x}, 0, 0)$ is provable since $y = 0 \leftrightarrow y \leq 0$ and $\neg i < 0$. 
    So ${\bf I}\Delta_0(\operatorname{exp}) \vdash F(\vec{x}, 0, y)$ is provable.
    \item Assume $\exists y F(\vec{x}, z, y)$ is provable, let show that that $\exists y F(\vec{x}, z + 1, y)$ is provable.

    We can show $y \leq z \to y + 1 \leq z + 1$ and, via $i < y + 1 \leftrightarrow i < y \lor i = y$,
    \begin{center}
      $\forall i < y \: \neg G(\vec{x}, i, 0) \land ((y = z) \land \neg G(\vec{x}, y, 0)) \to \forall i < y + 1 \: \neg G(\vec{x}, i, 0) \land y + 1 = z + 1$
    \end{center}
    Therefore
    \begin{center}
    $F(\vec{x}, z, y) \to F(\vec{x}, z + 1, y + 1) \lor F(\vec{x}, z + 1, y)$
    \end{center}
    and thus:
    \begin{center}
      $\exists y F(\vec{x}, z, y) \to \exists y F(\vec{x}, z + 1, y)$
    \end{center}
   \end{enumerate}
\end{proof}

\begin{theorem}~\label{provablysigma1:1}
  Every elementary function is provably bounded in ${\bf I}\Delta_0(\operatorname{exp})$.
\end{theorem}

\begin{proof}
  As we know from recursion theory, the class of elementary functions can be characterised
  as those functions which are definable from $0$, $S$, $P$, $\cdot$, $+$, $exp_2$, $\dot{-}$ and $\cdot$
  by composition and minimisation. And then we apply above lemmas.
\end{proof}

\subsection{Proof-theoretic Characterisation}

For this section we shall be using a Tait-style formalisation of ${\bf I}\Delta_0(\operatorname{exp})$.
We have the following logical rules:

\vspace{\baselineskip}

\begin{prooftree}
  \AxiomC{$ $}
  \RightLabel{{\bf Ax}}
  \UnaryInfC{$\Gamma, R\vec{t}, \neg R\vec{t}$}
\end{prooftree}
\begin{minipage}{0.45\textwidth}
  \begin{prooftree}
    \AxiomC{$\Gamma, A_0, A_1$}
    \RightLabel{$\vee$}
    \UnaryInfC{$\Gamma, A_0 \vee A_1$}
  \end{prooftree}

  \begin{prooftree}
    \AxiomC{$\Gamma, A(t)$}
    \RightLabel{$\exists$}
    \UnaryInfC{$\Gamma, \exists x A(x)$}
  \end{prooftree}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
  \begin{prooftree}
    \AxiomC{$\Gamma, A_0$}
    \AxiomC{$\Gamma, A_1$}
    \RightLabel{$\land$}
    \BinaryInfC{$\Gamma, A_0 \land A_1$}
  \end{prooftree}

  \begin{prooftree}
    \AxiomC{$\Gamma, A$}
    \RightLabel{$\forall$}
    \UnaryInfC{$\Gamma, \forall x A$}
  \end{prooftree}
\end{minipage}

\vspace{\baselineskip}

where $R\vec{t}$ is an atomic formula and $x$ is not free in $A$ in the $\forall$ rule.
Here $\Gamma$ stores all non-logical axioms of ${\bf I}\Delta_0(\exp)$ along with its negations.
We also have the bounded induction rule:
  \begin{prooftree}
    \AxiomC{$\Gamma, B(0)$}
    \AxiomC{$\Gamma, \neg B(n), B(n + 1)$}
    \RightLabel{${\bf BInd}$}
    \BinaryInfC{$\Gamma, B(t)$}
  \end{prooftree}
where $B$ is a bounded formula and $t$ is any term. 

Of course, the cut rule is admissible:
  \begin{prooftree}
    \AxiomC{$\Gamma, A$}
    \AxiomC{$\Gamma, \neg A$}
    \RightLabel{${\bf cut}$}
    \BinaryInfC{$\Gamma$}
  \end{prooftree}


\begin{definition}
  Let $\exists \vec{z} B(\vec{z})$ be a closed $\Sigma_1$-formula, then it is \emph{true at $m$}, written as 
  $m \models \exists \vec{z} B(\vec{z})$, if there exist natural numbers $m_1, \dots, m_l$ such that each 
  $m_i < m$ and $B(\vec{m})$ is true in the standard model.

  A finite set $\Gamma$ of closed $\Sigma_1$-formulas is true at $m$, written as $m \models \Gamma$ if at least one of them is true at $m$.
\end{definition}

If $\Gamma(x_1, \dots, x_k)$ is a finite set of $\Sigma_1$-formulas whose free variables occur amongst $x_1, \dots, x_k$.
Let $f : \mathbb{N}^{k} \to \mathbb{N}$, then $f \models \Gamma(x_1, \dots, x_k)$ we have $f(\vec{n}) \models \Gamma(x_1 := n_1, \dots, x_k := n_k)$
for each $\vec{n} = (n_1, \dots, n_k)$.

\begin{fact} {\bf (Persistence)}

  \begin{enumerate}
    \item If $m \leq m'$, then $m \models \exists \vec{z} B(\vec{z})$ implies $m' \models \exists \vec{z} B(\vec{z})$.
    \item If $\forall \vec{n} \in \mathbb{N}^{k}$ $f(\vec{n}) \leq f'(\vec{n})$, 
    then $f(\vec{n}) \models \Gamma(x_1 := n_1, \dots, x_k := n_k)$ implies $f'(\vec{n}) \models \Gamma(x_1 := n_1, \dots, x_k := n_k)$.
  \end{enumerate}
\end{fact}

\begin{lemma}~\label{truth:idelta0}
  Let $\Gamma(\vec{x})$ be a finite set of $\Sigma_1$ formulas such that 
  \begin{center}
  ${\bf I}\Delta_0(\operatorname{exp}) \vdash \bigvee \limits_{\gamma(\vec{x}) \in \Gamma(\vec{x})} \gamma(\vec{x})$.
  \end{center}
  Then there is an elementary function $f$ such that $f \models \Gamma(\vec{x})$ and $f$ is strongly increasing on its variables.
\end{lemma}

\begin{proof}

  If $\Gamma$ is provable in ${\bf I}\Delta_0(\operatorname{exp})$, 
  then it is provable in the Tait-style version of ${\bf I}\Delta_0(\operatorname{exp})$, where all cut formulas are $\Sigma_1$.

  If $\Gamma$ is classically derivable from non-logical axioms $A_1, \dots, A_s$, then there is a cut-free proof
  in the Tait calculus of $\neg A_1, \Delta, \Gamma$, where $\Delta = \neg A_2, \dots, \neg A_s$. Let us show how to cancel $\neg A_1$ using a $\Sigma_1$-cut.

  If $A_1$ is an induction axiom on some formula $B$, then we have a cut-free proof of:
  \begin{center}
    $B(0) \land \forall y (\neg B(y) \lor B(y + 1)) \land \exists x \neg B(x), \Delta, \Gamma$
  \end{center}

  Thus we also have cut-free proofs of $B(0), \Delta, \Gamma$, $\neg B(y), B(y + 1), \Delta, \Gamma$
  and $\exists x \neg B(x), \Delta, \Gamma$. So we have
  \begin{prooftree}
    \AxiomC{$\Delta, \Gamma, B(0)$}
    \AxiomC{$\Delta, \Gamma, \neg B(y), B(y + 1)$}
    \RightLabel{${\bf BInd}$}
    \BinaryInfC{$\Delta, \Gamma, B(x)$}
    \RightLabel{$\forall$}
    \UnaryInfC{$\Delta, \Gamma, \forall x B(x)$}
    \AxiomC{$\exists x \neg B(x), \Delta, \Gamma$}
    \RightLabel{$\Sigma_1$-{\bf cut}}
    \BinaryInfC{$\Delta, \Gamma$}
  \end{prooftree}

  We can similarly cancel each of $\neg A_2, \dots, \neg A_s$ and so obtain the proof of $\Gamma$ with $\Sigma_1$-cuts only.

  Now we choose a proof of $\Gamma(\vec{x})$ and proceed by induction on the height of the proof and determine
  an elementary function $f$ such that $f \models \Gamma$.

  \begin{enumerate}
    \item If $\Gamma(\vec{x})$ is an axiom, then for all $\vec{n}$ $\Gamma(\vec{n})$ contains a true atom.
    So for any $f$ $f \models \Gamma$. Let us choose $f(\vec{n}) = n_1 + \dots + n_k$.
    \item If $\Gamma, B_0 \vee B_1$ is derivable, so is $\Gamma, B_0, B_1$. Note that $B_0$ and $B_1$ are both bounded.
    Let $f \models \Gamma, B_0, B_1$, then $f \models \Gamma, B_0 \vee B_1$.
    \item Assume $\Gamma, B_0 \land B_1$ is derivable, then $\Gamma, B_0$ and $\Gamma, B_1$
    By the induction hypothesis we have $f_0 \models \Gamma, B_0$ and $f_1 \models \Gamma, B_1$, so, by persistence,
    we have $\lambda \vec{n}. f_0(\vec{n}) + f_1(\vec{n}) \models \Gamma, B_0 \land B_1$.
    \item Assume $\Gamma, \forall y B(y)$ is derivable, then $\Gamma, B(y)$ is derivable and $y$ is not free in $\Gamma$.
    Since all the formulas are $\Sigma_1$, $\forall x B(y)$ must be bounded, so $B(y) \eqcirc \neg (y < t) \lor B'(y)$ for some
    term $t$ and for some bounded formula $B'$.
    By the induction hypothesis, assume $f_0 \models \Gamma, \neg (y < t), B'(y)$ for some increasing elementary function $f_0$.
    Then we have:
    \begin{center}
      $f_0(\vec{n}, k) \models \Gamma(\vec{n}), \neg (k < t(\vec{n})), B'(\vec{n}, k)$
    \end{center}
    Let $g$ be an increasing elementary function bounding $t$, define
    \begin{center}
      $f(\vec{n}) = \sum \limits_{k < g(\vec{n})} f(\vec{n}, k)$
    \end{center}
    We have either $f(\vec{n}) \models \Gamma(\vec{n})$ or, by persistence, $B'(\vec{n}, k)$ is true for every $k < t(\vec{n})$.
    So $f \models \Gamma, \forall y B(y)$ and $f$ is elementary.
    \item Assume $\Gamma, \exists y A(y, \vec{x})$ is derivable, so $\Gamma, A(t, \vec{x})$ is derivable for some term $t$.
    By the IH, there is elementary $f_0$ such that for all $\vec{n}$ one has
    \begin{center}
      $f_0(\vec{n}) \models \Gamma(\vec{n}), A(t(\vec{n}), \vec{n})$
    \end{center}
    Then either $f_0(\vec{n}) \models \Gamma(\vec{n})$ or else $f_0(\vec{n})$ bounds true witnesses for all
    existential quantifiers in $A(t(\vec{n}), \vec{n})$. Choose an elementary function $g$ which is bounding for $t$.
    Define $f(\vec{n}) = f_0(\vec{n}) + g(\vec{n})$, then for all $\vec{n}$ either 
    $f(\vec{n}) \models \Gamma(\vec{n})$ or $f(\vec{n}) \models \exists y A(y, \vec{n})$.
    \item Assume $\Gamma$ comes about by the cut rule with $\Sigma_1$ formula 
    $C \eqcirc \exists \vec{z} B(\vec{z})$, so the premises are
    $\Gamma, \forall \vec{z} \neg B(\vec{z})$ and $\Gamma, \exists \vec{z} B(\vec{z})$.

    Without increasing the height of a proof, we can invert all universal quantifiers
    in the first premise. So we have $\neg B(\vec{z})$. $B$ is bounded, so the induction hypothesis
    can be applied to this formula to obtain an elementary function $f_0$ such that, 
    for all assignments $[\vec{x} := \vec{n}]$ and $[\vec{z} := \vec{m}]$
    \begin{center}
      $f_0(\vec{n}, \vec{m}) \models \Gamma(\vec{n}), \neg B(\vec{n}, \vec{m})$
    \end{center}
    Now we apply the induction hypothesis to the second premise of the cut rule, so we have an elementary function
    $f_1$ such that for all $\vec{n}$ either 
    $f_1(\vec{n}) \models \Gamma(\vec{n})$ or there are fixed witnesses $\vec{m} < f_1(\vec{n})$
    such that $B(\vec{n}, \vec{m})$ is true.

    Define $f$ the following way:
    \begin{center}
      $f(\vec{n}) = f_0(\vec{n}, f_1(\vec{n}), \dots, f_1(\vec{n}))$
    \end{center}
    Furthermore $f \models \Gamma$. For otherwise there would be a tuple $\vec{n}$ such that
    $\Gamma(\vec{n})$ is not true at $f(\vec{n})$, so, by persistence, 
    $\Gamma(\vec{n})$ is not true at $f_1(\vec{n})$.
    Thus $B(\vec{n}, \vec{m})$ is true for particular numbers $\vec{m} < f_1(\vec{n})$.
    But then $f_0(\vec{n}, \vec{m}) < f(\vec{n})$, so, by persistence, $\Gamma(\vec{n})$ cannot be true at 
    $f_0(\vec{n}, \vec{m})$. Thus $B(\vec{n}, \vec{m})$ is false, so we have a contradiction.
    \item Finally suppose $\Gamma(\vec{x}), B(\vec{x}, t)$ comes from the induction rule on a bounded formula $B$.
    The premises of the rule $\Gamma(\vec{x}), B(\vec{x}, 0)$ and $\Gamma(\vec{x}), \neg B(\vec{x}, y), B(\vec{x}, y + 1)$.

    Let us apply the induction hypothesis to each of the premises, and then we obtain
    increasing elementary functions $f_0$ and $f_1$ such that for all $\vec{n}$ and for all $k$

    \begin{center}
      $f_0(\vec{n}) \models \Gamma(\vec{n}), B(\vec{n}, 0)$

      $f_1(\vec{n}, k) \models \Gamma(\vec{n}), \neg B(\vec{n}, k), B(\vec{n}, k + 1)$
    \end{center}

    Now let
    \begin{center}
      $f(\vec{n}) = f_0(\vec{n}) + \sum \limits_{k < g(\vec{n})} f_1(\vec{n}, k)$
    \end{center}
    where $g$ is an increasing elementary function which is bounding for the term $t$.
    $f$ is elementary and increasing, and, by persistence for $f_0$ and $f_1$, we have either
    $f(\vec{n}) \models \Gamma(\vec{n})$ or else
    $B(\vec{n}, 0)$ and $B(\vec{n}, k) \to B(\vec{n}, k + 1)$ are true for all $k < t(\vec{n})$.
    In either case, we have $f \models \Gamma(\vec{x}), B(\vec{x}, t(\vec{x}))$.
  \end{enumerate}
\end{proof}

\begin{theorem}
  A number-theoretic function is elementary iff $f$ is provably $\Sigma_1$ in ${\bf I}\Delta_0(\operatorname{exp})$.
\end{theorem}

\begin{proof}
  The only if part is in Theorem~\ref{provablysigma1:1}, so we show the if part only.
  Assume $f$ is provably $\Sigma_1$ in ${\bf I}\Delta_0(\operatorname{exp})$. Then we have a formula
  \begin{center}
    $F(\vec{x}, y) = \exists z_1 \dots \exists z_k B(\vec{x}, y, z_1, \dots, z_k)$
  \end{center}
  which defines $f$ and such that
  \begin{center}
  ${\bf I}\Delta_0(\operatorname{exp}) \models \exists y F(\vec{x}, y)$
  \end{center}
  By Lemma~\ref{truth:idelta0}, there exists an elementary function $g$ such that for every
  tuple of arguments $\vec{n}$ there are numbers $m_0, \dots, m_k$
  less that $g(n)$ satisfying the bounded formula $B(\vec{n}, m_0, m_1, \dots, m_k)$.
  Apply the elementary sequence coding:
  \begin{center}
    $h(\vec{n}) = \langle g(\vec{n}), g(\vec{n}), \dots, g(\vec{n})\rangle$
  \end{center}
  so that if $m = \langle m_0, m_1, \dots, m_k \rangle$
  where $m_i < g(\vec{n})$ for each $i \in n + 1$, so $m < h(\vec{n})$.

  As far as $f(\vec{n})$ is the unique $m_0$ for which there are $m_1, \dots, m_k$ satisfying
  $B(\vec{n}, m_0, \dots, m_k)$, we define $f$ as:
  \begin{center}
    $f(\vec{n}) = (\mu_{m < h(\vec{n})} B(\vec{n}, (m)_0, (m)_1, \dots, (m)_k))_0$.
  \end{center}

  $B$ is a bounded formula of ${\bf I}\Delta_0(\operatorname{exp})$, $B$ is elementarily 
  decidable. Moreover, elementary functions are closed under composition and bounded minimisation,
  so $f$ is elementary.
\end{proof}

\section{Primitive Recursion and ${\bf I}\Sigma_1$}

${\bf I}\Sigma_1$ is an arithmetical theory where the induction scheme is
restructed to $\Sigma_1$ formulas.

\begin{lemma}
  Every primitive recursion is provably recursive in ${\bf I}\Sigma_1$.
\end{lemma}

\begin{proof}
  We have to show represent each primitive recursive function $f$ with a $\Sigma_1$ formula 
  $F(\vec{x}, y) := \exists z C(\vec{x}, y, z)$ such that:
  \begin{enumerate}
    \item $f(\vec{n}) = m$ iff $\omega \models F(\vec{x}, y)$.
    \item ${\bf I}\Sigma_1 \vdash \exists y F(\vec{x}, y)$.
    \item ${\bf I}\Sigma_1 \vdash F(\vec{x}, y) \land F(\vec{x}, y') \to y = y'$.
  \end{enumerate}

  In each case $C(\vec{x}, y, z)$ will be a $\Delta_0(exp)$-formula
  constructed via sequence encoding in ${\bf I}\Delta_0(\operatorname{exp})$.
  Such a formula expresses that $z$ is a uniquely determined sequence number encoding the computation of
  $f(\vec{x}) = y$ and containing the output value $y$ as its final element, so $y = \pi_2(z)$.

  Condition 1 will hold by the definition of $C$. 
  Condition 3 will be satisfied by the uniqueness of $z$. We consider five definitional schemes by which $f$ could be
  introduced:
  \begin{enumerate}
    \item $f$ is the constant-zero function, that is, $f(x) = 0$, no matter what $x$ is.
    Then we take $C := y = 0 \land z = \langle 0 \rangle$. All the conditions are obviously satisfied.
    \item If $f$ is the successor function $f(x) = x + 1$, we let
    \begin{center}
      $C(x,y,z) := y = x + 1 \land z = \langle x + 1 \rangle$
    \end{center}
    All the conditions are obvious.
    \item Now assume $f$ is the projection function $f(x_0, \dots, x_n) = x_i$ for some $i \in n + 1$.
    We let
    \begin{center}
      $C(\vec{x},y,z) := y = x_i \land z = \langle x_i \rangle$
    \end{center}
    \item Now assume $f$ is defined by substitution from previously generated primitive recursive functions $f_0, f_1, f_2$:
    \begin{center}
      $f(\vec{x}) = f_0(f_1(\vec{x}), f_2(\vec{x}))$
    \end{center}

    By the induction hypothesis, assume thatb $f_0, f_1, f_2$ are provably recursive and we have 
    $\Delta_0(exp)$-formulas $C_0, C_1, C_2$ encoding their computations ($\operatorname{len}(z) = 4$).
    For the function $f$ define:
    \begin{center}
      $C(\vec{x}, y, z) := \bigwedge \limits_{i \in \{ 1, 2\}} C_i(\vec{x}, \pi_2((z)_i), (z)_i) \land 
      C_0(\pi_2((z)_1), \pi_2((z)_2), y, (z)_0) \land (z)_3 = y$.
    \end{center}

    Let us check the required conditions:
    \begin{enumerate}
      \item Condition 1 holds since $f(\vec{n}) = m$ iff there are numbers $m_1$ and $m_2$ such that
      $f_1(\vec{n}) = m_1$, $f_2(\vec{n}) = m_2$ and $f_0(m_1, m_2) = m$.
      These hold if and only if there are number $k_1, k_2, k_0$ such that 
      $C_1(\vec{n}, m_1, k_1)$, $C_2(\vec{n}, m_2, k_2)$ and $C_0(m_1, m_2, m, k_0)$ are all true.
      And these hold if and only if $C(\vec{n}, m, \langle k_0, k_1, k_2, m \rangle)$ is true.
      Thus $f(\vec{n}) = m $ iff and only if $F(\vec{n}, m) = \exists z C(\vec{n}, m, z)$ is true.
      \item Condition 2 holds since from $C_1(\vec{x}, y_1, z_1)$, $C_2(\vec{x}, y_2, z_2)$
      and $C(y_1, y_2, y, z_0)$ we can derive 
      $C(\vec{x}, y, \langle z_0, z_1, z_2, y \rangle)$ in ${\bf I}\Delta_0$.
      So provided $\exists y \exists z C_1(\vec{x}, y, z)$, $\exists y \exists z C_2(\vec{x}, y, z)$ and
      $\forall y_1 \forall y_2 \exists y \exists z C(y_1, y_2, y, z)$, we can prove $\exists y F(\vec{x}, y) := 
      C(\vec{x}, y, z)$.
      \item Condition 3 is self-evident.
    \end{enumerate}
    \item Now assume that $f$ is defined from $f_1$ and $f_2$ by primitive recursion:
    \begin{center}
      $f(\vec{v}, 0) = f_0(\vec{v})$

      $f(\vec{v}, x + 1) = f_1(\vec{v}, x, f(\vec{v}, x))$
    \end{center}
    By the induction hypothesis $f_0$ and $f_1$ are provably recursive and they have associated $\Delta_0$-formulas
    $C_0$ and $C_1$. Define
    \begin{center}
    $\begin{array}{lll}
      & C(\vec{v}, x, y, z) := C_0(\vec{v}, \pi_2((z)_0), (z)_0) \land & \\
      & \:\:\:\: \forall i < x \:\: (C_i (\vec{v}, i, \pi_2((z)_i), \pi_2((z)_{i + 1}))) \land & \\
      & \:\:\:\: (z)_{x + 1} = y \land \pi_2((z)_x) = y &
    \end{array}$
  \end{center}

  Let us check that all the conditions are satisfied:
  \begin{enumerate}
    \item Condition 1 holds since $f(\vec{l}, n) = m$ if and only if there is a sequence number
    $k = \langle k_0, \dots, k_n, m \rangle$ such that $k_0$ encodes the computation of 
    $f(\vec{l}, 0)$ with the value $\pi_2(k_0)$, and for each $i < n$, 
    $k_{i + 1}$ codes the computation of $f(\vec{l}, i + 1) = f_1(\vec{l}, i, \pi_2(k_i))$
    with values $\pi_2(k_{i + 1})$ and $\pi_2(k_n) = m$.
    This is equivalent to $\models F(\vec{l}, n, m) \leftrightarrow \exists z C(\vec{l}, n,m, z)$.
    \item To show Condition 2 we have to prove the following in ${\bf I}\Delta_0$
    \begin{center}
      $C_0(\vec{v}, y, z) \to C(\vec{v}, 0, y, \langle z, y \rangle)$
    \end{center}
    and
    \begin{center}
      $C(\vec{v}, x, y, z) \land C_1(\vec{v}, x, y,y',z') \to C(\vec{v}, x + 1, y', t)$
    \end{center}
    for a suitable term $t$ which removes the end component $y$ of $z$ and replaces it by $z'$, 
    and then adds the final component $y'$. More specifically
    \begin{center}
      $t = \pi(\pi(\pi_1(z), z'), y')$
    \end{center}
    Hence from $\exists y \exists z C_0(\vec{v},y,z)$ we obtain 
    $\exists y \exists z C(\vec{v}, 0, y, z)$, and from $\forall y \exists y' \exists z' C_1(\vec{v}, x, y, y', z')$ one can derive
    \begin{center}
      $\exists y \exists z C(\vec{v}, x,y,z) \to \exists y \exists z C(\vec{v}, x + 1, y, z)$
    \end{center}
    We have assumed that $f_0$ and $f_1$ are primitive recursive, we can prove $\exists y F(\vec{v}, 0, y)$
    and $\exists y F(\vec{v}, x, y) \to \exists y F(\vec{v}, x + 1, y)$.
    Then we derive $\exists y F(\vec{v}, x, y)$ by using $\Sigma_1$-induction.
    \item To show Condition 3 assume $C(\vec{v}, x, y, z)$ and $C(\vec{v}, x, y',z')$, where
    $z$ and $z'$ are sequence numbers of the same length $x + 2$.
    Furthermore we have $C_0(\vec{v}, \pi_2((z)_0), (z)_0)$
    and $C_0(\vec{v}, \pi_2((z')_0), (z')_0)$, so we have $(z)_0 = (z')_0$.

    Similarly we have $\forall i < x \:\: C_1(\vec{v}, i, \pi_2((z)_i), \pi_2((z)_{i + 1}), (z)_{i + 1})$
    and the same formula where $z$ is replaced by $z'$.
    So if $(z)_i = (z')_i$, and one can deduce $(z)_{i + 1} = (z')_{i + 1}$ using the uniquness assumption
    for $C_1$. By $\Delta_0(exp)$-induction we obtain $\forall i \leq x \:\: ((z)_i = (z')_i)$.

    The final conjuncts in $C$ give $(z)_{x + 1} = \pi_2((z)_x) = y$ and the same formulas where $z$ is replaced by $z'$ 
    and where $y$ is replaced by $y'$. But since $(z)_x = (z')_x$ we have $y = y'$, since all the components are equal,
    $z = z'$. Thus we have $F(\vec{v}, x, y) \land F(\vec{v}, x, y') \to y = y'$.
  \end{enumerate}
  \end{enumerate}
\end{proof}

\subsection{${\bf I}\Sigma_1$ provable functions are primitive recursive}

\begin{definition}
  A closed $\Sigma_1$-formula $\exists \vec{z} B(z)$ with $B \in \Delta_0(exp)$ is said to be
  ``true at $m$'' (denoted as $m \models \exists \vec{z} B(z)$) if there are numbers
  $\vec{m} = (m_1, \dots, m_l)$ such that all $m_i < m$ for $i \in \{1, \dots, l\}$
  such that $B(\vec{m})$ is true in the standard model.
  
  A finite set of formulas $\Gamma$ of closed $\Sigma_1$-formulas is ``true at $m$'' 
  (denoted as $m \models \Gamma$) if at least one of them is true at $m$.

  If $\Gamma(x_1, \dots, x_k)$ is a finite set of $\Sigma_1$-formulas all of whose
  free variables occur amongst $x_1, \dots, x_k$ and if $f : \mathbb{N}^k \to \mathbb{N}$, then we write 
  $f \models \Gamma$ if for each assignments $\vec{n} = (n_1, \dots, n_k)$ to the variables  $x_1, \dots, x_k$
  we have $f(\vec{n}) \models \Gamma(\vec{n})$.
\end{definition}

Note that we have the persistence property for $\models$ 
which completely repeats persistence for ${\bf I}\Delta_0(\operatorname{exp})$.

We shall be using a Tait-style formalisation of ${\bf I}\Sigma_0$ where the induction rule

\begin{prooftree}
  \AxiomC{$\Gamma, A(0)$}
  \AxiomC{$\Gamma, \neg A(y), A(y + 1)$}
  \BinaryInfC{$\Gamma, A(t)$}
\end{prooftree}

where $y$ is not free in $\Gamma$, $t$ is any term and $A$ is any $\Sigma_1$-formula.

\begin{lemma} ($\Sigma_1$-induction)
  Let $\Gamma(\vec{x})$ be a finite set of $\Sigma_1$-formulas such that
  \begin{center}
    ${\bf I}\Sigma_1 \vdash \bigvee \Gamma(\vec{x})$
  \end{center}
  then there is a primitive recursive function $f$ such that $f \models \Gamma$ and $f$ is strictly increasing
  on its variables.
\end{lemma}

\begin{proof}
  We note that if $\Gamma$ is provable in this formalisation, then
it has a proof in which all the non-atomic cut formulas are induction $\Sigma_1$-formulas.
If $\Gamma$ is classically derivable from non-logical axioms $A_1, \dots, A_s$,
then there is a cut-free proof (\`{a} la Tait) of $\neg A_1, \Delta, \Gamma$ where $\Delta = A_2, \dots, A_s$.
Then if $A_1$ is an induction axiom on a formula $F$, then we have have a cut-free proof of
\begin{center}
  $F(0) \land \forall y (\neg F(y) \lor F(y + 1)) \land \neg F(t), \Delta, \Gamma$
\end{center}
and thus, by inversion, we have cut-free proofs of $F(0), \Delta, \Gamma$, 
$\neg F(y), F(y + 1), \Delta, \Gamma$ and $\neg F(t), \Delta, \Gamma$.

So we obtain $F(t), \Delta, \Gamma$ by the induction rule and then we obtain $\Delta, \Gamma$ by cutting $F(t)$.
One can detach $\neg A_2, \dots, \neg A_s$, so we finally have a proof of $\Gamma$ which uses cuts only
on $\Sigma_1$-induction formulas or on atoms arising from non-logical axioms. Such proofs are said to be ``free-cut'' free.

Let us choose such a proof for $\Gamma(\vec{x})$ and show by induction on the height of a proof that there exists
a primitive recursive function satisfying $f \models \Gamma$.

\begin{enumerate}
  \item Let $\Gamma(\vec{x})$ be an axiom, the for all $\vec{n}$ $\Gamma(\vec{n})$ contains a true atom.
  Choose $f(\vec{n}) = n_1 + \dots + n_k$, and $f$ is clearly primitive recursive, strictly incrasing and $f \models \Gamma$.
  \item Assume we have
  \begin{prooftree}
    \AxiomC{$\Gamma, B_0, B_1$}
    \RightLabel{$\lor$}
    \UnaryInfC{$\Gamma, B_0 \lor B_1$}
  \end{prooftree}
  Then both $B_0$ and $B_1$ are both $\Delta_0(exp)$-formulas, so any function $f$ satisfying 
  $f \models \Gamma, B_0, B_1$ also satisfies $\Gamma, B_0 \lor B_1$.
  \item Assume we have
  \begin{prooftree}
    \AxiomC{$\Gamma, B_0$}
    \AxiomC{$\Gamma, B_1$}
    \RightLabel{$\land$}
    \BinaryInfC{$\Gamma, B_0 \land B_1$}
  \end{prooftree}
  By the induction hypothesis we have $f_i(\vec{n}) \models \Gamma(\vec{n}), B_i(\vec{n})$ where $i \in \{0,1\}$ for all $\vec{n}$.
  By the persistence property, $\lambda \vec{n}. f_0(\vec{n}) + f_1(\vec{n}) \models \Gamma, B_0 \land B_1$.
  \item Assume we have
  \begin{prooftree}
    \AxiomC{$\Gamma, B(y)$}
    \RightLabel{$\forall$}
    \UnaryInfC{$\Gamma, \forall y B(y)$}
  \end{prooftree}
  where $y$ is not free in $\Gamma$. As far as all formulas are $\Sigma_1$, $\forall y B(y)$ 
  must be ${\bf I}\Delta_0(\operatorname{exp})$, so $B(y) \eqcirc \neg(y < t) \lor B'(y)$ for some elemetary or primitive recursive term $t$.
  Assume we have $f_0 \models \Gamma, \neg(y < t) \lor B'(y)$ for some increasing primitive recursive function $f_0$.
  Then, for any assignments $\vec{x} \mapsto \vec{n}$ and $y \mapsto k$, we have
  \begin{center}
    $f_0(\vec{n}, k) \models \Gamma(\vec{n}), \neg (k < t(\vec{n})), B'(\vec{n}, k)$.
  \end{center}
  We let
  \begin{center}
    $f(\vec{n}) = \sum \limits_{k < g(\vec{n})} f_0(\vec{n}, k)$
  \end{center}
  for some function $g$, which is increasing primitive recursive bounding the values of term $t$.
  So we have either $f(\vec{n}) \models \Gamma$ or $B'(\vec{n}, k)$ is true for every $k < t(\vec{n})$.
  Thus $f \models \Gamma, \forall y B(y)$ as required.
  \item Suppose we have
  \begin{prooftree}
    \AxiomC{$\Gamma, A(t)$}
    \RightLabel{$\exists$}
    \UnaryInfC{$\Gamma, \exists y A(y)$}
  \end{prooftree}
  where $A$ is a $\Sigma_1$-formula. By the induction hypothesis we have a function $f_0$ such that for all $\vec{n}$
  \begin{center}
    $f_0(\vec{n}) \models \Gamma(\vec{n}), A(t(\vec{n}), \vec{n})$
  \end{center}
  Then either $f_0(\vec{n}) \models \Gamma(\vec{n})$ or otherwise $f_0(\vec{n})$ 
  bounds true witnesses for all the existential quantifiers already in $A(t(\vec{n}, \vec{n}))$.
  Choose an elementary bounding function $g$ for the term $t$ and define 
  $f(\vec{n}) = f_0(\vec{n}) + g(\vec{n})$, so we have either 
  $f(\vec{n}) \models \Gamma(\vec{n})$ or $f(\vec{n}) \models \exists y A(y, \vec{n})$ for all $\vec{n}$.
  \item Assume we have
  \begin{prooftree}
    \AxiomC{$\Gamma, \forall \vec{z} \neg B(\vec{z})$}
    \AxiomC{$\Gamma, \exists \vec{z} B(\vec{z})$}
    \RightLabel{{\bf cut}}
    \BinaryInfC{$\Gamma$}
  \end{prooftree}
  where $\exists \vec{z} B(\vec{z})$ is a cut $\Sigma_1$-formula.
  
  Note that we have
  \begin{prooftree}
    \AxiomC{$\Gamma, \neg B(\vec{z})$}
    \RightLabel{$\forall$}
    \UnaryInfC{$\Gamma, \forall \vec{z} \neg B(\vec{z})$}
  \end{prooftree}
  Note $B$ is a $\Delta_0(\exp)$-formula, so let us apply the induction hypothesis to obtain a
  primitive recursive function $f_0$ such that for each assignments $\vec{x} \mapsto \vec{n}$ and $\vec{z} \mapsto \vec{m}$
  \begin{center}
    $f_0(\vec{n}, \vec{m}) \models \Gamma(\vec{n}), \neg B(\vec{n}, \vec{m})$.
  \end{center}
  We apply the induction hypothesis to the second premise to obtain a primitive recursive function $f_1$ such that
  for all $\vec{n}$ either $f_1(\vec{n}) \models \Gamma(\vec{n})$ or otherwise there are fixed witnesses 
  $\vec{m} < f_1(\vec{n})$ s.t. $B(\vec{n}, \vec{m})$ is true.
  Let us define $f$ by substitution:
  \begin{center}
    $f(\vec{n}) = f_0(\vec{n}, f_1(\vec{n}), \dots, f_1(\vec{n}))$
  \end{center}
  where $f$ is primitive recursive, greater or equal that $f_1$ (pointwise) and strictly increasing.
  Furthermore $f \models \Gamma$.
  
  For otherwise, let us suppose there exists a tuple $\vec{n}$ such that $\Gamma(\vec{n})$ is not true
  $f(\vec{n})$ and, thus, by persistence at $f_1(\vec{n})$. So $B(\vec{n}, \vec{m})$ is true for
  some $\vec{m} < f_1(\vec{n})$. Thus $f_0(\vec{n}, \vec{m}) < f(\vec{n})$, and then, by persistence, $\Gamma(\vec{n})$
  cannot be true at $f_0(\vec{n}, \vec{m})$. Then $B(\vec{n}, \vec{m})$, so we have a contradiction.
  \item Suppose we have
  \begin{prooftree}
    \AxiomC{$\Gamma(\vec{x}), A(\vec{x}, 0)$}
    \AxiomC{$\Gamma, \neg A(\vec{x}, y), A(\vec{x}, y + 1)$}
    \BinaryInfC{$\Gamma, A(\vec{x}, t)$}
  \end{prooftree}
  where $A(\vec{x}, y)$ is an induction $\Sigma_1$-formula of the form $\exists \vec{z} B(\vec{x}, y, \vec{z})$.
  Let us invert universal quantifiers in $\neg A(\vec{x}, y)$, the second premise of the rule becomes
   \begin{center}
     $\Gamma(\vec{x}), \neg B(\vec{x}, y, \vec{z}), A(\vec{x}, y + 1)$
   \end{center}
  which is now a set $\Sigma_1$-formulas. We can apply the induction hypothesis to each of the premises to
  have primitive recursive function $f_0$ and $f_1$ such that for each $\vec{n}$, $k$ and $\vec{m}$
  \begin{center}
    $f_0(\vec{n}) \models \Gamma(\vec{n}), A(\vec{n}, 0)$

    $f_1(\vec{n}, k, \vec{m}) \models \Gamma(\vec{n}), \neg B(\vec{n}, k, \vec{m}), A(\vec{n}, k + 1)$
  \end{center}
  Define $f$ by primitive recursion from $f_0$ and $f_1$ the following way
  \begin{center}
    $f(\vec{n}, 0) = f_0(\vec{n})$

    $f(\vec{n}, k + 1) = f_1(\vec{n}, k, f(\vec{n}, k), \dots, f(\vec{n}, k))$
  \end{center}

  Then for all $\vec{n}$ and for all $\vec{k}$ one has $f(\vec{n}, k) \models \Gamma(\vec{n}), A(\vec{n}, k)$ which is shown
  by induction on $k$.
  The base case holds by the definition of $f_0(\vec{n})$. For the induction step assume that 
  $f(\vec{n}, k) \models \Gamma(\vec{n}), A(\vec{n}, k)$. If $\Gamma(\vec{n})$ is not true at 
  $f(\vec{n}, k + 1)$. By persistence it is not true at $f(\vec{n}, k)$ and thus 
  $f(\vec{n}, k) \models A(\vec{n}, k)$.
  Therefore there are numbers $\vec{m} < f(\vec{n}, k)$ such that $B(\vec{n}, k, \vec{m})$ is true.
  Thus $f_1(\vec{n}, k, \vec{m}) \models \Gamma(\vec{n}), A(\vec{n}, k + 1)$ and since 
  $f_1(\vec{n}, k, \vec{m}) \leq f(\vec{n}, k + 1)$ we have, by persistence, $f(\vec{n}, k + 1) \models \Gamma(\vec{n}), A(\vec{n}, k + 1)$
  as required.

  So we substitute for the final argument $k$ in $f$ an elementary (or primitive recursive) function $g$
  which bounds the values of $t$, so that $f'(\vec{n}) = f(\vec{n}, g(\vec{n}))$, and thus
  we have $f(\vec{n}, t(\vec{n})) \models \Gamma(\vec{n}), A(\vec{n}, t(\vec{n}))$ for all $\vec{n}$ and thus, by persistence, $f' \models \Gamma(\vec{x}), A(\vec{x}, t)$.
\end{enumerate}
\end{proof}

\begin{theorem}
  The provably recursive functions of ${\bf I}\Sigma_1$ are exactly primitive recursive functions.
\end{theorem}

\begin{proof}
  We have already shown that all primitive recursive functions are provably recursive in ${\bf I}\Sigma_1$,
  so let us show the converse.

  Let $g : \mathbb{N}^k \to \mathbb{N}$ is $\Sigma_1$ be a function defined by a $\Sigma_1$-formula
  $F(\vec{x}, y) := \exists z C(\vec{x}, y, z)$ where $C$ is $\Delta_0(exp)$ and
  ${\bf I}\Sigma_1 \models \exists y F(\vec{x}, y)$.
  By the lemma above, there exists a primitive recursive function $f$ such that for all $n \in \mathbb{N}^k$
  \begin{center}
    $f(\vec{n}) \models \exists y \exists z C(\vec{n}, y, z)$.
  \end{center}
  That is, for every $\vec{n}$ there is an $m < f(\vec{n})$ and a $k < f(\vec{n})$
  such that $C(\vec{n}, m, k)$ is true and this $m$ is the value of $g(\vec{n})$.

  $g$ can be defined by primitive recursion from $f$ the following way:

  \begin{center}
    $g(\vec{n}) = (\mu_{m < h(\vec{n})} C(\vec{n}, (m)_0, (m)_1))$
  \end{center}
  where $h(\vec{n}) = \langle f(\vec{n}), f(\vec{n}) \rangle$.
\end{proof}

\section{$\varepsilon_0$-recursion in Peano Arithmetic}

We show that the provably recursive functions of Peano arithmetic are $\varepsilon_0$-recursive functions, that is, 
functions definable from the primitive recursive functions by substitutions and recursion over well-orderings
of natural numbers with order types strictly less than the ordinal
\begin{center}
  $\varepsilon_0 = \sup \{ \omega, \omega^{\omega}, \omega^{\omega^{\omega}}, \dots \}$
\end{center}
Equivalently, $\varepsilon_0$ can be defined as the least fixed point of the mapping $\alpha \mapsto \omega^{\alpha}$
where $\alpha$ is an ordinal.

Let us discuss first how one can represent ordinals below $\varepsilon_0$.

\subsection{Ordinals below $\varepsilon_0$}

Every ordinal $\alpha < \varepsilon_0$ is either $0$ or $\alpha$ can be represented uniquely in \emph{Cantor normal form}:
\begin{center}
  $\alpha = \omega^{\gamma_1} \cdot c_1 + \omega^{\gamma^{\gamma_1}} \cdot c_2 + \dots + \omega^{\gamma_k} \cdot c_k$
\end{center}
where $k < \omega$, $\gamma_k < \dots < \gamma_2 < \gamma_1 < \alpha$ and $c_1, \dots, c_k < \omega$ are coefficients.
If $\gamma_k = 0$, then $\alpha$ is a successor ordinal, written $\operatorname{Succ}(\alpha)$, 
and its predecessor $\alpha - 1$ the representation
\begin{center}
  $\alpha = \omega^{\gamma_1} \cdot c_1 + \omega^{\gamma^{\gamma_1}} \cdot c_2 + \dots + \omega^{\gamma_{k - 1}} \cdot c_{k - 1}$.
\end{center}

Otherwise $\alpha$ is a limit ordinal, written $\operatorname{Lim}(\alpha)$, and it has infinitely many possible
increasing sequences of smaller ordinals whose limit is $\alpha$.

We shall pick out one concrete sequence $\{ \alpha(n) \: | \: n < \omega \}$ for each limit ordinal $\alpha$ the following way.
First write $\alpha$ as $\delta + \omega^{\gamma}$ where
\begin{center}
  $\delta = \omega^{\gamma_1} \cdot c_1 + \dots + \omega^{\gamma_k} \cdot (c_k - 1)$

  $\gamma = \gamma_k$.
\end{center}

By induction we can assume that when $\gamma$ is a limit ordinal, its fundamental sequence 
$\{ \gamma(n) \: | \: n < \omega \}$ has been already specified. We let for each $n < \omega$
\begin{center}
  $\alpha(n) = \begin{cases}
    \delta + \omega^{\gamma - 1} \cdot (n + 1), \text{if $\operatorname{Succ}(\gamma)$} \\
    \delta + \omega^{\gamma(n)}, \text{if $\operatorname{Lim}(\gamma)$}.
  \end{cases}$
\end{center}
Clearly
\begin{center}
  $\alpha = \lim \limits_{n \to \omega} \alpha(n)$.
\end{center}

\begin{definition}
  Let $\alpha < \varepsilon_0$ and $n < \omega$, define a finite set of ordinals $\alpha[n]$ the following way:

  \begin{center}
    $\alpha[n] = \begin{cases}
      \emptyset, \text{if $\alpha = 0 $} \\
      (\alpha - 1)[n] \cup \{ \alpha - 1 \}, \text{if $\operatorname{Succ}(\alpha)$} \\
      \alpha(n)[n], \text{ if $\operatorname{Lim}(\alpha)$}
    \end{cases}$
  \end{center}
\end{definition}

\begin{lemma}~\label{ord:sum:1}
  For each $\alpha = \delta + \omega^{\gamma}$ and for each $n < \omega$
  \begin{center}
    $\alpha[n] = \delta[n] \cup \{ \delta + \omega^{\gamma_1} \cdot c_1 + \dots + \omega^{\gamma_k} \cdot c_k \: | \: \forall i (\gamma_i \in \gamma[n] \land c_i \leq n) \}$.
  \end{center}
\end{lemma}

\begin{proof}
  Induction on $\gamma$.

  \begin{enumerate}
    \item $\gamma = 0$, then $\gamma[n] = \emptyset$ and the right hand side is 
    $\delta[n] \cap \{ \delta \}$, which is the same as $\alpha[n] = (\delta + 1)[n]$.
    \item If $\gamma$ is limit, then $\gamma[n] = \gamma(n)[n]$, so the right hand side
    is the same as the one with $ \gamma(n)[n]$ instead of $\gamma[n]$. 
    By the induction hypothesis applied to $\alpha(n) = \delta + \omega^{\gamma(n)}$, which is equal to 
    $\alpha(n)[n]$, which is $\alpha[n]$ by definition.
    \item Suppose $\gamma$ is a successor. Then $\alpha$ is a limit and $\alpha[n] = \alpha(n)[n]$,
    where $\alpha(n) = \delta + \omega^{\gamma - 1} \cdot (n + 1)$.
    So we can write $\alpha(n) = \alpha(n - 1) + \omega^{\gamma - 1}$, where $\alpha(-1) = \delta$ when $n = 0$.
    By the induction hypothesis for $\gamma - 1$, the set $\alpha[n]$ equals
    \begin{center}
      $\alpha(n - 1)[n] \cup \{ \alpha(n - 1) + \omega^{\gamma_1} \cdot c_1 + \dots + \omega^{\gamma_k} \cdot c_k \: | \: \forall i (\gamma_1 \in (\gamma - 1)[n] \land c_i \leq n)\}$
    \end{center}
    and similarly for each $\alpha(n - 1)[n], \alpha(n - 2)[n], \dots, \alpha(1)[n]$. 
    For each $m \leq n$, $\alpha(m - q) = \delta + \omega^{\gamma - 1} \cdot m$. In turn, this last set is the same
    as
    \begin{center}
      $\delta[n] \cup \{ \delta + \omega^{\gamma - 1} \cdot m + \omega^{\gamma_1} \cdot c_1 + \dots + \omega^{\gamma_k} \cdot c_k \: | \: \forall i (\gamma_i \in (\gamma - 1)[n] \land c_i \leq n) \land m \leq n\}$
    \end{center}
    and this is the set since $\gamma[n] = (\gamma - 1)[n] \cup \{ \gamma - 1\}$.
  \end{enumerate}
\end{proof}

\begin{col}
  Let $\alpha < \varepsilon_0$ be a limit ordinal, then for every $0 \neq n < \omega$ $\alpha(n) \in \alpha[n + 1]$.
  Furthermore if $\beta \in \gamma[n]$, then $\omega^{\beta} \in \omega^{\gamma}[n]$.
\end{col}

\begin{definition}
The \emph{maximum coefficient} of $\beta = \omega^{\beta_1} \cdot b_1 + \dots + \omega^{\beta_l} \cdot b_l$
is defined by induction to be the maximum of all the $b_i$'s and all the maximum coefficients of the exponents $\beta_i$'s.
\end{definition}

\begin{lemma}
  If $\beta < \alpha$ and the maximum coefficient of $\beta$ is $\leq n$, so $\beta \in \alpha[n]$. 
\end{lemma}

\begin{proof}
  By induction on $\alpha$. Let $\alpha = \delta + \omega^{\gamma}$. If $\beta < \delta$,
  then $\beta \in \delta[n]$ by the induction hypothesis and $\delta[n] \subseteq \alpha[n]$ by Lemma~\ref{ord:sum:1}.
  Otherwise
  \begin{center}
    $\beta = \delta + \omega^{\beta_1} \cdot b_1 + \dots + \omega^{\beta_k} \cdot b_k$
  \end{center}
  for $\alpha > \gamma > \beta_1 > \dots > \beta_k$ and $b_i \leq n$. By induction hypothesis 
  $\beta_i \in \gamma[n]$, so $\beta \in \alpha[n]$ by Lemma~\ref{ord:sum:1}.
\end{proof}

\begin{definition}
  Let $G_{\alpha}(n)$ denote the cardinality of the finite set $\alpha[n]$.
  We have

  \begin{center}
  $G_{\alpha}(n) = \begin{cases}
    0, \text{if $\alpha = 0$} \\
    G_{\alpha - 1}(n + 1), \text{ if $\operatorname{Succ}(\alpha)$} \\
    G_{\alpha(n)}(n), \text{if $\operatorname{Lim}(\alpha)$} 
  \end{cases}$
  \end{center}
\end{definition}

The hierarchy of functions $G_{\alpha}$ is the \emph{slow-growing} hierarchy.

\begin{lemma} If $\alpha = \delta + \omega^{\gamma}$, then for all $n < \omega$
  \begin{center}
    $G_{\alpha}(n) = G_{\delta}(n) + (n + 1)^{G_{\gamma}(n)}$.
  \end{center}
  Thus for each $\alpha < \varepsilon_0$, $G_{\alpha}(n)$ is the elementary function
  which results by substituting $n + 1$ for every occurence of $\omega$ in the Cantor normal form $\omega$.
\end{lemma}

\begin{proof}
  Induction on $\gamma$.

  \begin{enumerate}
    \item If $\gamma = 0$, then $\alpha = \delta + 1$, thus
    \begin{center}
      $G_{\alpha}(n) = G_{\delta}(n) + 1 =  G_{\delta}(n) + (n + 1)^0$.
    \end{center}
    \item If $\gamma$ is a successor, then $\alpha = \delta + \omega^{\gamma}$ is limit
    and $\alpha(n) = \delta + \omega^{\gamma - 1} \cdot (n + 1)$, so
    we apply the induction hypothesis for $\gamma - 1$ $n + 1$ times and thus we have
    \begin{center}
      $G_{\alpha}(n) = G_{\alpha(n)}(n) = G_{\delta}(n) + (n + 1)^{G_{\gamma - 1}(n)} \cdot (n + 1) = G_{\delta}(n) + (n + 1)^{G_{\gamma}(n)}$
    \end{center}
    since $G_{\gamma - 1}(n) + 1 = G_{\gamma}(n)$.
    \item If $\gamma$ is a limit ordinal, then $\alpha(n) = \delta + \omega^{\gamma(n)}$, 
    so let us apply the induction hypothesis to $\gamma(n)$, then we have
    \begin{center}
      $G_{\alpha}(n) = G_{\alpha(n)}(n) = G_{\delta}(n) + (n + 1)^{G_{\gamma(n)}(n)}$
    \end{center}
    which gives the result since $\Gamma_{\gamma(n)}(n) = G_{\gamma}(n)$.
  \end{enumerate}
\end{proof}

\begin{definition} {\bf (Coding ordinals)}

  Let $\beta = \omega^{\beta_1} \cdot b_1 + \dots \omega^{\beta_l} \cdot b_l$ be an ordinal.
  A \emph{coding ordinal} is the sequence number $\overline{\beta}$ constructed recursively the following way
  \begin{center}
    $\overline{\beta} = \langle \langle \overline{\beta_1}, b_1 \rangle, \dots, \langle \overline{\beta_l}, b_l \rangle \rangle$.
  \end{center}
\end{definition}
where $0$ is coded by the empty sequence number. $\overline{\beta}$ is numerically greater than the maximum
coefficient of $\beta$ and greater than the codes $\overline{\beta_i}$ of all its exponents and their exponents, etc.

\begin{lemma}
  $ $
  \begin{enumerate}
    \item There exists an elementary function $h : \mathbb{N} \times \mathbb{N} \to \mathbb{N}$ such that, for each
    ordinal $\beta = \omega^{\beta_1} \cdot b_1 + \dots \omega^{\beta_l} \cdot b_l$:
    \begin{center}
      $h(\overline{\beta}, n) = \begin{cases}
        0, \text{if $\beta = 0$} \\
        \overline{\beta - 1}, \text{if $\operatorname{Succ}(\beta)$} \\
        \overline{\beta(n)}, \text{if $\operatorname{Lim}(\beta)$}
      \end{cases}$
    \end{center}
    \item For each ordinal $\alpha < \varepsilon_0$ there exists an elementary well-ordering 
    $\prec_{\alpha} \subset \mathbb{N} \times \mathbb{N}$ such that
    \begin{center}
      $\forall b, c \in \mathbb{N} \:\: b \prec_{\alpha} c \leftrightarrow \exists \beta, \gamma < \alpha \:\: \beta < \gamma \: \& \: b = \overline{\beta} \: \& \: c = \overline{\gamma}$.
    \end{center}
  \end{enumerate}
\end{lemma}

\begin{proof} $ $

  \begin{enumerate}
    \item First let
    \begin{center}
      $h(0, n) = 0$
    \end{center}
    for any $n$. Then let $0 < m < \omega$ be a non-zero sequence number.
    We first should see if the rightmost component $\pi_2$ is a pair $(m', n')$. 
    If so and $m' = 0$ and $n' \neq 0$, then $\beta$ is a successor and the code of its predecessor, 
    $h(m, n)$, is defined as the new sequence number that we obtain by reducing $n'$ by one or by removing
    this final component if $n' = 1$.

    If $\pi_2(m) = \langle m', n' \rangle$ where both $m'$ and $n'$ are non-zero, then $\beta$ is a limit ordinal of then form
    $\delta + \omega^{\gamma} \cdot n'$ where $m' = \overline{\gamma}$. Let $k$ be the code of
    $\delta + \omega^{\gamma} \cdot (n' - 1)$, which is obtained by reducing $n'$ by one inside $m$ 
    (or by deleting the final component from $m$ when $n' = 1$).

    At the ``right hand end'' of $\beta$ we have a ``spare'' $\omega^{\gamma}$ which must be either reduced to
    $\omega^{\gamma - 1} \cdot (n + 1)$ when $\operatorname{Succ}(\gamma)$ or to $\omega^{\gamma(n)}$ if $\operatorname{Lim}(\gamma)$.
    In either case we are able to produce $\beta(n)$. Thus the required code $h(m, n)$ of $\beta(n)$
    will be obtained by tagging on to the end of the sequence number $k$ one additinal pair encoding this additional term.

    If we assume inductively that $h(m', n)$ has been already defined for $m' < m$, then such an additional component
    is either $\langle h(m',n), n + 1\rangle$ if $\operatorname{Succ}(\gamma)$ or $\langle h(m',n), 1\rangle$
    if $\operatorname{Lim}(\gamma)$.

    This defines $h(m,n)$, but such a definition is actually primitive recursive so far. Let us chech that $h$ is
    elementarily bounded, i.e. $h$ is defined by limited recursion from elementary functions.
    Note that $h(m, n) < m$ whenever $m$ codes a successor ordinal. If $m$ codes a limit ordinal, 
    $h(m,n)$ is obtained from the sequence number $k < m$ by adding a new pair on the end.
    An extra item $i$ is tagged on the end of a sequence number $k$ by the function $\pi(k, i)$ which is quadratic in both argument.
    If the item added is the pair $\langle h(m', n), n + 1 \rangle$ where $\operatorname{Succ}(\gamma)$,
    then $h(m', n) < m$, so $h(m, n)$ is numerically bounded by some fixed polynomial in $m$ and $n$.
    In the other case, we can say that $h(m, n)$ is numerically bounded by some fixed polynomial of $m$ and $h(m',n)$.
    Since $m'$ codes an exponent in the Cantor normal form encoded by $m$, the second polynomial
    is iterated at most $d$ times, where $d$ is the ``exponential height'' of the normal form.
    Thus $h(m, n)$ is bounded by some $d$-times iterated polynomial of $m + n$.
    $d < m$, so $h(m, n)$ is bounded by the elementary function $2^{2^{c \cdot (m + n)}}$ for some $c < \omega$.
    Therefore $h$ is elementary as it is defined by bounded recursion.
    \item Let $\alpha < \varepsilon_0$ and let $d$ be the exponential height of its Cantor normal form.
    We use the function $h$ from the previous part, but we apply it to codes below $\alpha$ only. They have the exponential
    height $\leq d$, so we can consider $h$ as being bounded by some fixed polynomial of its two arguments.
    Define $g(0, n) = \overline{\alpha}$ and $g(i+1,n) = h(g(i, n), n)$ and notice that $g$ is therefore bounded by an $i$-times iterated polynomial,
    so $g$ is defined by an elementarily limited recursion from $h$, so it is elementary.
    
    Define $b \prec_{\alpha} c$ if and only if $c \neq 0$ and there are $i$ and $j$ such that 
    $0 < i < j \leq G_{\alpha}(\max(b, c) + 1)$ and $g(i, \max(b,c)) = c$ and $g(j, \max(b,c)) = b$.
    The function $g$ and $G_{\alpha}$ are elementary, so is the relation $\prec_{\alpha}$ since the quantifiers are
    bounded. By the properties of $h$ it is clear that if $i < j$ then $g(j, \max(b,c))$ codes an ordinal greater than 
    $g(j, \max(b,c))$. Hence $b \prec_{\alpha} c$, then $b = \overline{\beta}$ and $c = \overline{\gamma}$ for some
    $\beta < \gamma < \alpha$.

    Now assume $b = \overline{\beta}$, $c = \overline{\gamma}$ and $\beta < \gamma < \alpha$.
    The code of an ordinal is greater than its maximal coefficient, so we have
    $\beta \in \alpha[\max(b,c)]$ and $\gamma \in \alpha[\max(b,c)]$. Thus the sequence starting
    with $\alpha$ and at each stage descending from a $\delta$ to either $\delta - 1$ if
    $\operatorname{Succ}(\delta)$ or $\delta(\max(b,c))$ if $\operatorname{Lim}(\delta)$ necessarily 
    passes through $\gamma$ and then through $\beta$.
    In turn, it means there are $i, j < \omega$ such that $0 < i < j$, $g(i, \max(b, c)) = c$, $g(j, \max(b, c)) = b$.
    So $b \prec_{\alpha} c$ holds if we can show that $j \leq G_{\alpha}(\max(b,c) + 1)$.
    In the sequence described above, only the successor stages contribute an element $\delta - 1$ to
    $\alpha[\max(b,c)]$. At the limit stages $\delta(\max(b, c))$ does not get put it.
    Although $\delta(n)$ does not belong to $\delta[n]$, it does belong to $\delta[n + 1]$.
    Therefore all the ordinals in the descending sequence lie in $\alpha[\max(b,c) + 1]$, so $j$ can not be bigger
    than the cardinality of this set, which is $G_{\alpha}(\max(b,c) + 1)$.
  \end{enumerate}
\end{proof}

The moral is that the principles of transfinite induction and recursion over the initials segments of ordinals below
$\varepsilon_0$ can be expressed by means of ${\bf I}\Delta_0(\operatorname{exp})$.

\subsection{Introducing the fast-growing hierarchy}

\begin{definition}
  The \emph{Hardy hierarchy} $\{ H_{\alpha}\}_{\alpha < \varepsilon_0}$ is defined by recursion on $\alpha$:

  \begin{center}
    $H_{\alpha}(n) = \begin{cases}
      n, \text{ if $\alpha = 0 $} \\
      H_{\alpha - 1}(n + 1), \text{if $\operatorname{Succ}(\alpha)$} \\
      H_{\alpha(n)}(n), \text{if $\operatorname{Lim}(\alpha)$}
    \end{cases}$
  \end{center}

  The \emph{fast-growing hierarchy} $\{ F_{\alpha} \}_{\alpha < \varepsilon_0}$ is defined by recursion on $\alpha$:

  \begin{center}
    $F_{\alpha}(n) = \begin{cases}
      n + 1, \text{if $\alpha = 0$} \\
      F^{n + 1}_{\alpha - 1}(n), \text{if $\operatorname{Succ}(\alpha)$} \\
      F_{\alpha(n)}(n), \text{if $\operatorname{Lim}(\alpha)$}
    \end{cases}$
  \end{center}
  where $F^{n + 1}_{\alpha - 1}(n)$ is the $(n + 1)$-times iteration of $F_{\alpha - 1}$ on $n$.
\end{definition}

Note that $H_{\alpha}$ and $F_{\alpha}$ could be equivalently defined by purely number-theoretic means
by working over the well-orderings $\prec_{\alpha}$ instead of working over ordinals directly. So $H_{\alpha}$ and
$F_{\alpha}$ are $\varepsilon_0$-recursive.

\begin{lemma}
  For all $\alpha, \beta < \varepsilon_0$ and for all $n < \omega$,

  \begin{enumerate}
    \item $H_{\alpha + \beta}(n) = H_{\alpha}(H_{\beta}(n))$,
    \item $H_{\omega^{\alpha}}(n) = F_{\alpha}(n)$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  The first part is proved by induction on $\beta$. If $\beta = 0$, then the equation trivially holds.
  Assume $\operatorname{Succ}(\beta)$ and the induction hypothesis for $\beta -1$, then
  we have:
  \begin{center}
    $H_{\alpha + \beta}(n) = H_{\alpha + (\beta - 1)}(n + 1) = H_{\alpha}(H_{\beta - 1}(n + 1)) = H_{\alpha}(H_{\beta}(n))$.
  \end{center}

  If $\operatorname{Lim}(\beta)$, then we have (by using the induction hypothesis for $\beta(n)$):
  \begin{center}
    $H_{\alpha + \beta}(n) = H_{\alpha + \beta(n)}(n) = H_{\alpha}(H_{\beta(n)}(n)) = H_{\alpha}(H_{\beta}(n))$.
  \end{center}

  The second part is proven by induction on $\alpha$. If $\alpha = 0$, then

  \begin{center}
    $H_{\omega^0}(n) = H_1(n) = n + 1 = F_0(n)$
  \end{center}

  If $\operatorname{Succ}(\alpha)$, then

  \begin{center}
    $H_{\omega^{\alpha}}(n) = H_{\omega^{\alpha - 1} \cdot (n + 1)}(n) = H^{n + 1}_{\omega^{\alpha - 1}}(n) = F^{n + 1}_{\alpha - 1}(n) = F_{\alpha}(n)$.
  \end{center}

  The limit case is immediate.
\end{proof}

\begin{lemma}
  For each $\alpha < \varepsilon_0$, $H_{\alpha}$ is strictly increasing and $H_{\beta}(n) < H_{\alpha}(n)$
  for $\beta \in \alpha[n]$. The same holds for $F_{\alpha}$ for $n \neq 0$, for when $n = 0$ we have $F_{\alpha}(0) = 1$
  for each $\alpha$.
\end{lemma}

\begin{proof}
  Induction on $\alpha$. The case $\alpha = 0$ is trivial since $H_0$ is the identity function and $0[n] = \emptyset$.
  If $\operatorname{Succ}(\alpha)$, then $H_{\alpha}$ is $H_{\alpha - 1}$ composed with the successor function,
  it is strictly increasing by the induction hypothesis. 
  Take $\beta \in \alpha[n]$, then either $\beta \in (\alpha -1)[n]$ or $\beta = \alpha - 1$, thus, by using the induction hypothesis
  \begin{center}
    $H_{\beta}(n) \leq H_{\alpha - 1}(n) < H_{\alpha - 1}(n + 1) = H_{\alpha}(n)$.
  \end{center}

  If $\operatorname{Lim}(\alpha)$ then 
  \begin{center}
  $H_{\alpha}(n) = H_{\alpha(n)}(n) < H_{\alpha(n)}(n + 1)$
  \end{center}
  but $\alpha(n) \in \alpha[n + 1] = \alpha(n + 1)[n + 1]$, thus
  \begin{center}
    $H_{\alpha(n)}(n + 1) < H_{\alpha(n + 1)}(n + 1) = H_{\alpha}(n + 1)$
  \end{center}
  Thus $H_{\alpha}(n) < H_{\alpha}(n + 1)$. Furthermore if $b \in \alpha[n]$,
  then $\beta \in \alpha(n)[n]$ so $H_{\beta}(n) < H_{\alpha(n)}(n) = H_{\alpha}(n)$ by the induction hypothesis
  for $\alpha(n)$.

  The same holds for $F_{\alpha} = H_{\omega^{\alpha}}$ since if $\beta \in \alpha[n]$ we then have
  $\omega^{\beta} \in \omega^{\alpha}[n]$.
\end{proof}

\begin{lemma}
If $\beta \in \alpha[n]$, then $F_{\beta+1}(m) \leq F_{\alpha}(m)$ for all $m \geq n$.
\end{lemma}

\begin{proof}
Induction on $\alpha$. The zero case is trivial.
If $\operatorname{Succ}(\alpha)$, then either $\beta \in (\alpha - 1)[n]$ or $\beta = \alpha - 1$. In either case
we apply the induction hypothesis. If $\alpha$ is a limit, then we have $\beta \in \alpha(n)[n]$, so
by induction hypothesis $F_{\beta + 1}(m) \leq F_{\alpha(n)}(m)$, but $F_{\alpha(n)}(m) \leq F_{\alpha}(m)$.
\end{proof}

\subsection{$\alpha$-recursion and $\varepsilon_0$-recursion}

\begin{definition}[\bf $\alpha$-recursion]
  $ $

  \begin{enumerate}
    \item An \emph{$\alpha$-recursion} if a function definition of the following form, 
    defining $f : \mathbb{N}^{k + 1} \to \mathbb{N}$ from functions $g_0, g_1, \dots, g_s$ by the following equations:
    \begin{center}
      $f(0, \vec{m}) = g_0(\vec{m})$

      $f(n, \vec{m}) = T(g_1, \dots, g_s, f_{\prec n}, n, \vec{m})$ provided $n \geq 1$.
    \end{center}
    where $T(g_1, \dots, g_s, f_{\prec n}, n, \vec{m})$ is a fixed term built up from the number variables
    $n$ and $\vec{m}$ by applying functions $g_1, \dots, g_s$ and the function $f_{\prec n}$ defined as
    \begin{center}
      $f_{\prec n}(n', \vec{m}) = \begin{cases}
        f(n', \vec{m}), \text{if $n' \prec_{\alpha} n$} \\
        0, \text{otherwise}
      \end{cases}$
    \end{center}
    Note that it is assumed that $\alpha > 0$.
    \item An \emph{unnested} $\alpha$ is one of the special form:

    \begin{center}
      $f(0, \vec{m}) = g_0(\vec{m})$

      $f(n, \vec{m}) = g_1(n, \vec{m}, f(g_2(n, \vec{m}), \dots, g_{k + 1}(n, \vec{m})))$
    \end{center}
    with a single recursive call of $f$ where $g_2(n, \vec{m}) \prec_{\alpha} n$ for all $n$ and $\vec{m}$.
    \item Let $\varepsilon_0(0) = \omega$ and $\varepsilon_0(i + 1) = \omega^{\varepsilon_0(i)}$.
    For each particular $i$, a function is \emph{$\varepsilon_0(i)$-recursive} if it can be defined
    from primitive recursive functions by successive substitutions and $\alpha$-recursions with 
    $\alpha < \varepsilon_0(i)$. It is \emph{unnested $\varepsilon_0(i)$-recursive} if all the $\alpha$-recursions
    are unnested. It is \emph{$\varepsilon_0$-recursive} if it is $\varepsilon_0(i)$-recursive for some (any) $i$.
  \end{enumerate}
\end{definition}

\begin{lemma}[\bf Bounds for $\alpha$-recursion]
  Let $f$ be a function defined from $g_1, \dots, g_s$ by an $\alpha$-recursion:
  \begin{center}
    $f(0, \vec{m}) = g_0(\vec{m})$

    $f(n, \vec{m}) = T(g_1, \dots, g_s, f_{\prec n}, n, \vec{m})$
  \end{center}
  where for each $i \leq s$ $g_i(\vec{a}) < F_{\beta}(k + \max \vec{a})$ for all numerical arguments $\vec{a}$.
  Then there is a constant $d$ such that for all $n$, $\vec{m}$
  \begin{center}
    $f(n, \vec{m}) < F_{\alpha + \beta}(k + 2 d + \max(n, \vec{m}))$.
  \end{center}
\end{lemma}
Note that $\beta$ and $k$ are arbitrary constants, but it is assumed that the last exponent in the Cantor normal
form of $\beta$ is $\geq$ the first exponent in the normal form of $\alpha$, so that $\beta + \alpha$ is in Cantor normal
form by default.

\begin{proof}
  The constant $d$ will be actually the depth of nesting of the term $T$, where variables
  have depth have depth $0$ and each compositional term $g(T_1, \dots, T_l)$ has depth greater than the
  maximum depth of nesting of the subterms $T_j$.

  Assume $n$ lies in the field of the well-ordering $\prec_{\alpha}$. 
  Then $n = \overline{\gamma}$ for some $\gamma < \alpha$. Let us claim by induction on $\gamma$ that
  \begin{center}
    $f(n, \vec{m}) < F_{\beta + \gamma + 1}(k + 2 d+ \max(n, \vec{m}))$.
  \end{center}
  This is immediate when $n = 0$, because $g_0(\vec{m}) < F_{\beta}(k + \max \vec{m})$
  and $F_{\beta}$ is strictly increasing and bounded by $F_{\beta + 1}$.
  Assume $n \neq 0$ and assume the claim for all $n' = \vec{\delta}$ where $\delta < \gamma$.

  Let $T'$ be any subterm of $T(g_1, \dots, g_s, f_{\prec n},n, \vec{m})$ with depth of nesting $d'$,
  built up by application of one of the functions $g_1, \dots, g_s$ or $f_{\prec n}$ to subterms $T_1, \dots, T_l$.
  Assume (for a sub-induction on $d'$) that each of these $T_j$'s has numerical value $v_j$ less that 
  $F^{2 (d' - 1)}_{\beta + \gamma}(k + 2 d + \max(n, \vec{m}))$.

  If $T'$ is obtained by application of one of the functions $g_i$ then its numerical value will be
  \begin{center}
    $g_i(v_1, \dots, v_l) < F_{\beta}(k + 2^{d'-1}_{\beta + \gamma})(k + 2 d + \max(n, \vec{m})) < F^{2 d'}_{\beta + \gamma}(k + 2 d + \max(n, \vec{m}))$
  \end{center}
  since $k < u$ then $F_{\beta}(k + u) < F_{\beta}(2 u) < F^2_{\beta}(u)$ provided $\beta \neq 0$. On the other hand,
  if $T'$ is obtained by application of the function $f_{\prec n}$, its value will be $f(v_1, \dots, v_l)$
  if $v_1 \prec_{\alpha} n$ or $0$ otherwise. Suppose $v_1 = \overline{\delta} \prec_{\alpha} \overline{\gamma}$.
  So by the induction hypothesis:
  \begin{center}
    $f(v_1, \dots, v_l) < F_{\beta + \delta + 1}(k + 2 d + \max \vec{v}) \leq F_{\beta + \gamma}(k + 2 d + \max \vec{v})$
  \end{center}
  because $v_1$ is greater than the maximum coefficient of $\delta$, so $\delta \in \gamma[v_1]$, so
  $\beta + \delta \in (\beta + \gamma)[v_1]$ and hence $F_{\beta + \gamma + 1}$ is bounded by $F_{\beta + \gamma}$
  on arguments $\geq v_1$. Therefore insertingn the assumed bounds for the $v_j$, we have
  \begin{center}
    $F(v_1, \dots, v_l) < F_{\beta + \gamma}(k + 2d + F^{2(d' - 1)}_{\beta + \gamma}(k + 2d + \max(n, \vec{m})))$
  \end{center}
  and thus we have
  \begin{center}
    $f(v_1, \dots, v_l) < F^{2d'}_{\beta + \gamma}(k + 2d + \max(n, \vec{m}))$.
  \end{center}
  So we have just shown that the value of every subterms of $T$ with depth of nesting $d'$
  is less than $F^{2d'}_{\beta + \gamma}(k + 2d + \max(n, \vec{m}))$. Applying this to $T$
  itself with depth of nesting $d$ we obtain
  \begin{center}
    $f(n, \vec{m}) < F^{2d}_{\beta + \gamma}(k + 2d + \max(n, \vec{m})) < F_{\beta + \gamma + 1}(k+2d+\max(n,\vec{m}))$
  \end{center}
  So we have proved the claim.

  Now we derive the result of the lemma. Assume $n = \overline{\gamma}$ lies in the field of
  $\prec_{\alpha}$, then $\beta + \gamma \in (\beta + \alpha)[n]$ and thus
  \begin{center}
    $f(n, \vec{m}) < F_{\beta + \gamma + 1}(k + 2d + \max(n, \vec{m})) \leq F_{\beta + \alpha}(k + 2d + \max(n, \vec{m}))$.
  \end{center}

  If $n$ does not lie in the field of $\prec_{\alpha}$, then $f_{\prec n}$ is the constant zero function, and thus
  in evaluating $f(n, \vec{m})$ by the term $T$ only applications of the $g_i$-functions are required.
  Thus we have
  \begin{center}
    $f(n, \vec{m}) < F^{2d}_{\beta}(k + 2d + \max(n, \vec{m})) < F_{\beta + \alpha}(k + 2d + \max(n, \vec{m}))$.
  \end{center}
  since $\alpha$ is non-zero.
\end{proof}

\begin{theorem}
  For each $i$, a function is $\varepsilon_0(i)$-recursive if and only if it is a register-machine computable
  in a number of steps bounded by $F_{\alpha}$ for some $\alpha < \epsilon_0(i)$.
\end{theorem}

\begin{proof}

  \begin{enumerate}
    \item The "if" part.

    If a function $g$ is register-machine computable, then there is an elementary function $U$ such that for all
    arguments $\vec{m}$, if $s(\vec{m})$ bounds the number of steps required to compute $g(\vec{m})$,
    then $g(\vec{m}) = U(\vec{m}, s(\vec{m}))$. So if $g$ is computable in a number of steps bounded by $F_{\alpha}$, then
    $g$ can be defined from $F_{\alpha}$ by the following substitution
    \begin{center}
      $g(\vec{m}) = U(\vec{m}, F_{\alpha}(\max \vec{m}))$.
    \end{center} 

    So if $F_{\alpha}$ is $\varepsilon_0(i)$-recursive, so is $g$. Let us show that if $\alpha < \varepsilon_0(i)$ then
    $F_{\alpha}$ is $\varepsilon_0(i)$-recursive.

    The claim holds for $i = 0$ since then all $\alpha$'s are finite, but the finite levels of $F$ hierarchy
    are primitive recursive and thus $\varepsilon_0(0)$-recursive. Since $i > 0$ and
    $\alpha = \omega^{\gamma_1} \cdot c_1 + \dots + \omega^{\gamma_k} \cdot c_k < \varepsilon_0(i)$.
    
    Let us add one to each exponent and insert a successor term at the end, so we produce the ordinal $\beta = \alpha' + n$,
    where $\alpha'$ is the limit $\omega^{\gamma_1 + 1} \cdot c_1 + \dots + \omega^{\gamma_k + 1} \cdot c_k$.
    $i > 0$, so we have $\beta < \varepsilon_0(i)$. From the code of $\alpha$, denoted as $a$, we can compute the code
    for $\alpha$, denoted as $a'$. So $b = \pi(a', \langle 0, n \rangle)$ is the code for $\beta$. And conversely, we are able
    to decode $\alpha$, $\alpha'$ and $n$ from $\beta$. 

    Let us choose a large enough $\delta < \varepsilon_0(i)$ such that $\beta < \delta$, let us define $f(b, m)$ by $\delta$-recursion
    such that if $b$ is the code for $\beta = \alpha' + n$, then $f(b, m) = F^n_{\alpha}(m)$.
    Let us expose the components from which $b$ is constructed as $b = (a, n)$, so we can define $f(a, n, m)$ using the elementary function
    $h(a,n)$ that returns the code for $\alpha - 1$ for $\operatorname{Succ}(\alpha)$ or $\alpha(n)$ for $\operatorname{Lim}(\alpha)$:

    \begin{center}
    $f(a, n, m) = \begin{cases}
      m + n, \text{$a = 0$ or $n = 0$} \\
      f(h(a, m), m + 1, m), \text{if $\operatorname{Succ}(a)$ and $n = 1$} \\
      f(h(a, m), 1, m), \text{if $\operatorname{Lim}(a)$ and $n = 1$} \\
      f(a, 1, f(a, n - 1, m)), \text{if $n > 1$} \\
      0, \text{otherwise}
    \end{cases}$
  \end{center}

    Then $f$ is $\varepsilon_0(i)$-recursive and $F_{\alpha}(m) = f(\overline{\alpha}, 1, m)$, so 
    $F_{\alpha}$ is $\varepsilon_0(i)$-recursive for every $\alpha < \varepsilon_0(i)$.
    \item The "only if" part.

    Note that the number of steps needed to compute a compositional term $g(T_1, \dots, T_l)$
    is the sum of the numbers of steps needed to compute subterms $T_1, \dots, T_l$ plus the number of steps
    required to compute $g(v_1, \dots, v_l)$ where $v_j$ is the value of $T_j$.

    Furthermore, the values $v_j$ are bounded by the number of computation steps plus the maximal input. 
    So we can compute a bound on the computation steps for any such term. 
    Moreover, we can do that elementarily from given bounds for the input data. Now suppose
    \begin{center}
      $f(n, \vec{m}) = T(g_1, \dots, g_s, f_{\prec n}, n, \vec{m})$
    \end{center}
    is any recursion-step of an $\alpha$-recursion. So if we have bounding functions on the numbers
    of steps to compute each of the $g_i$'s and we assume inductively that we already have a bound on the number
    of steps to compute $f(n',-)$ for $n' \prec_{\alpha} n$. So we can elementarily estimate a bound on the steps to compute
    $f(n, \vec{m})$. So for any function defined by an $\alpha$-recursion from functions $\vec{g}$, a bounding
    function is also definable by $\alpha$-recursion by bounding functions for $\vec{g}$. 
    We have the same for primitive recursion. All in all, every $\varepsilon_0(i)$-function is
    register-machine computable in a number of steps bounded by some $F_{\gamma}$ for $\gamma < \varepsilon_0(i)$.
  \end{enumerate}
\end{proof}

\begin{col}
For each $i$, a function is $\varepsilon_0(i)$-recursive if and only if it is unnested $\varepsilon_0(i + 1)$-recursive.
\end{col}

\begin{proof}
Every $\varepsilon_0(i)$-recursive function is computable in the number of steps bounded
by $F_{\alpha} = H_{\omega^{\alpha}}$ where $\alpha < \varepsilon_0(i)$. Thus it is primitive recusrively definable
from $H_{\omega^{\alpha}}$. But $H_{\omega^{\alpha}}$ itself is defined an unnested 
$\omega^{\alpha}$-recursion and $\omega^{\alpha} < \varepsilon_0(i + 1)$. So arbitrarily nested 
$\varepsilon_0(i)$-recursions are reducible to unnested $\varepsilon_0(i + 1)$-recursions.

Conversely, assume $f$ is defined from functions $g_0, g_1, \dots, g_{k + 2}$ by an unnested $\alpha$-recursion where
$\alpha < \varepsilon_0(i + 1)$:

\begin{center}
  $f(0, \vec{m}) = g_0(\vec{m})$

  $f(n, \vec{m}) = g_1(n, \vec{m}, f(g_2(n, \vec{m}), \dots, g_{k + 2}(n, \vec{m})))$
\end{center}
with $g_2(n, \vec{m}) \prec_{\alpha} n$ for all $n$ and $\vec{m}$. Then the number of recursion-steps
needed to compute $f(n, m)$ is $f'(n, \vec{m})$ where

\begin{center}
  $f'(0, \vec{m}) = 0$

  $f'(n, \vec{m}) = 1 + f'(g_2(n,\vec{m}), \dots, g_{k + 2}(n, \vec{m}))$
\end{center}
and $f$ is thus definable from $g_2, \dots, g_{k + 2}$ by primitive recursion and bound for $f'$.
Assume that the given functions $g_j$ are all primitive recursively definable from, and bounded by, $H_{\beta}$ where
$\beta < \varepsilon_0(i + 1)$. Now let us provide bounds for $\alpha$-recursion and show that $f'(n, \vec{m})$
is bounded by $H_{\beta \cdot \gamma}$ where $n = \overline{\gamma}$ since
\begin{center}
  $H_{\beta \cdot (\gamma + 1)}(x) = H_{\beta \cdot \gamma + \beta}(x) = H_{\beta \cdot \gamma}(H_{\beta}(x))$.
\end{center}

Thus $f$ is definable from $H_{\beta}$ and $H_{\beta \cdot \alpha}$. Clearly since 
$\beta, \alpha < \varepsilon_0(i + 1)$ we can choose $\beta = \omega^{\beta'}$ and $\alpha = \omega^{\alpha'}$
for $\alpha' \leq \beta' < \varepsilon_0(i)$. Thus $H_{\beta} = H_{\beta'}$ and 
$H_{\beta \cdot \alpha} = F_{\beta' + \alpha'}$ where $\beta' + \alpha' < \varepsilon_0(i)$. 
Therefore $f$ is $\varepsilon_0(i)$-recursive.
\end{proof}

\subsection{Provable recursiveness of $F_{\alpha}$ and $H_{\alpha}$}

In this subsection we will show that for every $\alpha < \varepsilon_0(i)$ for $i < \omega$,
the function $F_{\alpha}$ is provably recursive in the theory ${\bf I}\Sigma_{i+1}$.

The required machinery for coding ordinals below $\varepsilon_0$ is elementary, 
so one can assume that it can be defined in ${\bf I}\Delta_0(\operatorname{exp})$. 
We will make use of the function $h$ such that if $a$ codes a successor ordinal $\alpha$, then $h(a, n)$
codes $\alpha - 1$ and $a$ codes a limit ordinal $\alpha$, then $h(a, n)$ codes $\alpha(n)$.
One can decide whether $a$ codes a successor ordinal ($\operatorname{Succ}(\alpha)$) or a limit
ordinal ($\operatorname{Lim}(\alpha)$) by asking whether $h(a, 0) = h(a, 1)$ or not. It is a bit easier
to show the provable recursiveness of the Hardy functions $H_{\alpha}$ first of all since the Hardy functions
are defined involving no nested recursion. After that one can conclude the provable recursiveness of
the fast-growing hierarchy by using the equation $F_{\alpha} = H_{\omega^{\alpha}}$.

\begin{definition}
  Let $H(a, x, y, z)$ be a $\Delta_0(exp)$-formula of the following form:

  \vspace{\baselineskip}

  $\begin{array}{lll}
    &(z)_0 = \langle 0, y \rangle \land \pi_2(z) = \langle a, x \rangle \land \: & \\
    &\forall i<\operatorname{lh}(z) \:\: (\operatorname{lh}((z)_i) = 2 \land (i < 0 \to (z)_{i, 0} > 0 )) \: \land & \\
    &\forall 0 < i < \operatorname{lh}(z) \:\: (\operatorname{Succ}((z)_{i, 0}) \to (z)_{i - 1, 0} = h((z)_{i, 0}, (z)_{i,1}) & \\
    & \:\:\:\:\:\:\:\: \land (z)_{i - 1, 1} = (z)_{i, 1} + 1) \: \land & \\
    &\forall 0 < i < \operatorname{lh}(z) \:\: (\operatorname{Lim}((z)_{i, 0}) \to (z)_{i - 1, 0} = h((z)_{i, 0}, (z)_{i, 1}) \land (z)_{i - 1, i} = (z)_{i, 1}) & 
  \end{array}$
\end{definition}

\begin{lemma}[Definability of $H_{\alpha}$]
$H_{\alpha}(n) = m$ iff $\exists z H(\overline{\alpha}, n, m, z)$ is true. For each $\alpha < \varepsilon_0$ one show
\begin{center}
  ${\bf I}\Sigma_1 \vdash \exists z H(\overline{\alpha}, x, y, z) \land \exists z H(\overline{\alpha}, x, y', z) \to y = y'$.
\end{center}
\end{lemma}
 
\begin{proof}
  The meaning of the formula $\exists z H(\overline{\alpha}, n, m, z)$ is that there is a finite
  sequence of pairs $\langle \alpha_i, n_i \rangle$, beginning with $\langle 0, m \rangle$ and ending with 
  $\langle \alpha, n \rangle$ such that at each $i > 0$ if $\operatorname{Succ}(\alpha_i)$ then 
  $\alpha_{i - 1} = \alpha_i - 1$ and $n{i - 1} = n_i + 1$ and if $\operatorname{Lim}(\alpha_i)$ then
  $\alpha_{i - 1} = \alpha_i(n_i)$ and $n_{i - 1} = n_i$.

  Thus by induction up along the sequence and by using the original definition of $H_{\alpha}$
  one can easily see that for each $i > 0$ $H_{\alpha_i}(n_i) = m$ and thus $H_{\alpha}(n) = m$.
  But if $H_{\alpha}(n) = m$, then there exists a required computation sequence, so the first part of the lemma is shown.

  As regards the second part, notice that one can show the following by induction for each $n,m,m',s,s'$
  \begin{center}
    $H(\overline{\alpha}, n, m, s) \to H(\overline{\alpha}, n, m', s') \to s = s' \land m = m'$
  \end{center}
  This proof can be formalised in ${\bf I}\Delta_0(\operatorname{exp})$ to give
  \begin{center}
    $H(\overline{\alpha}, x, y, z) \to H(\overline{\alpha}, x, y', z') \to z = z' \land y = y'$
  \end{center}
  and hence
  $\exists z H(\overline{\alpha}, x, y, z) \to \exists z H(\overline{\alpha}, x, y', z') \to z = z' \land y = y'$
\end{proof}

\begin{lemma}
  ${\bf I}\Delta_0(\operatorname{exp})$ proves the following formula
  \begin{center}
    $\exists z H(\omega^a, x,y,z) \to \exists z H(\omega^a c, y, w, z) \to \exists z H(\omega^a(c + 1), x, w, z)$
  \end{center}
  where $\omega^a c$ is the elementary term $\langle \langle a, c \rangle \rangle$ which constructs, from the code of
  $\alpha$ the code for $\omega^{\alpha} \cdot c$.
\end{lemma}

\begin{proof}
Assume we have sequences $s$ and $s'$ satisfying $H(\omega^{a}, x, y, s)$ and 
$H(\omega^{a} c, x, y, s)$. Add $\omega^{a} c$ to the first component of each pair in $s$.
Then the last pair in $s'$ and the last pair in $s$ are identical. We concatenate $s$ and $s'$ 
by taking the repeating pair only once and construct an elementary term $t(s, s')$
satisfying $H(\omega^a(c + 1), x, w, t)$. Then one can show
\begin{center}
  $H(\omega^a, x, y, s) \to H(\omega^a c, y, w, s') \to H(\omega^a(c + 1), x, w, t)$
\end{center}
in a conservative extension of ${\bf I}\Delta_0(\operatorname{exp})$ and thus derive the following in ${\bf I}\Delta_0(\operatorname{exp})$
\begin{center}
  $\exists z H(\omega^a, x, y, z) \to \exists z H(\omega^a c, y, w, z) \to \exists z H(\omega^a(c + 1), x, w, z)$.
\end{center}
\end{proof}

\begin{lemma}
  Let $H(a)$ be the $\Pi_2$-formula $\forall x \exists y \exists z H(a, x, y, z)$, then one can show the following by
  $\Pi_2$-induction:
  \begin{enumerate}
    \item $H(\omega^0)$.
    \item $\operatorname{Succ}(a) \to H(\omega^{h(a, 0)}) \to H(\omega^a)$.
    \item $\operatorname{Lim}(a) \to \forall x H(\omega^{h(a, x)}) \to H(\omega^a)$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  The term $t_0 = \langle \langle 0, x + 1 \rangle, \langle 1, x \rangle \rangle$
  witnesses $H(\omega^0, x, x + 1, t_0)$ in ${\bf I}\Delta_0(\operatorname{exp})$, so we have $H(\omega^0)$.

  Further one can derive the following
  \begin{center}
    $H(\omega^{h(a, 0)}) \to H(\omega^{h(a, 0)c}) \to H(\omega^{h(a, 0)}(c + 1))$.
  \end{center}

  So we obtain by $\Pi_2$-induction
  \begin{center}
    $H(\omega^{h(a, 0)}) \to H(\omega^{h(a, 0)}(x + 1))$
  \end{center}
  and
  \begin{center}
    $H(\omega^{h(a, 0)}) \to \exists y \exists z H(\omega^{h(a, 0)}(x + 1), x,y,z)$.
  \end{center}
  But there is an elementary term $t_1$ with the property
  \begin{center}
    $\operatorname{Succ}(a) \to H(\omega^{h(a, 0)}(x + 1), x, y, z) \to H(\omega^a, x, y, t_1)$
  \end{center}
  as far as $t_1$ needs to tag on to the end of the sequence $z$ the new pair $\langle \omega^a, x \rangle$
  and thus $t_1 = \pi (z, \langle \omega^a, x \rangle)$. Thus
  \begin{center}
    $\operatorname{Succ}(a) \to H(\omega^{h(a, 0)}) \to H(\omega^a)$.
  \end{center}
  The final case is straightforward, we have
  \begin{center}
    $\operatorname{Lim}(a) \to H(\omega^{h(a, x)}, x, y, z) \to H(\omega^a, x, y, t_1)$
  \end{center}
  and so by the Bernays rules we have
  \begin{center}
    $\operatorname{Lim}(a) \to \forall x H(\omega^{a, x}) \to H(\omega^a)$.
  \end{center}
\end{proof}

\begin{definition}[Structural transifinite induction]

  The \emph{structural progressiveness} of a formula $A(a)$ is expressed by $\operatorname{SProg}_a A$, which
  is the conjunction of the formulas $A(0)$, $\forall a (\operatorname{Succ}(a) \to A(h(a, 0)) \to A(a))$ and
  $\forall a (\operatorname{Lim}(a) \to \forall x A(h(a, x)) \to A(a))$.

  The principle of \emph{structural transfinite induction} up to an ordinal $\alpha$ is the following axiom schema, for
  all formulas $A$:
  \begin{center}
    $\operatorname{SProg}_a A \to \forall a \prec \overline{\alpha} A(a)$
  \end{center}
  where $a \prec \overline{\alpha}$ means $a$ lies in the field of the well-ordering $\prec_{\alpha}$, i.e.
  $a = 0 \lor 0 \prec_{\alpha} a$.
\end{definition}
In particular, the previous lemma shows that the $\Pi_2$-formula $H(\omega^a)$ is structurally progressive
and one can show that with $\Pi_2$-induction.

\begin{definition}[Transfinite Induction]

  The (general) \emph{progressiveness} of a formula $A(a)$ is
  \begin{center}
    $\operatorname{Prog}_a A := \forall a (\forall b \prec a A(b) \to A(a))$
  \end{center}

  The principle of a \emph{transfinite induction} up to an ordinal $\alpha$ is the schema
  \begin{center}
    $\operatorname{Prog}_a A \to \forall a \prec \overline{\alpha} A (a)$
  \end{center}
  where $a \prec \overline{\alpha}$ means that $a$ lies in the field of the well-ordering
  $\prec_{\alpha}$.
\end{definition}

\begin{lemma}
  Structural transfinite induction up to $\alpha$ is derivable from transfinite induction up to $\alpha$.
\end{lemma}

\begin{proof}
  Let $A$ be an arbitrary formula and assume $\operatorname{SProg}_a A$. Let us show
  $\forall a \prec \overline{\alpha} A(a)$. Let us transfinite induction for the formula 
  $a \prec \overline{\alpha} \to A(a)$, then it is sufficient to prove the following
  \begin{center}
    $\forall a (\forall b \prec a, \overline{\alpha} A(b) \to a \prec \overline{\alpha} \to A(a))$
  \end{center}
  which is equivalent to
  \begin{center}
    $\forall a \prec \overline{\alpha} (\forall b \prec a A(b) \to A(a))$.
  \end{center}
  The latter is proved from $\operatorname{SProg}_a A$ and the properties of the $h$ function.
\end{proof}

We also have induction over an arbitrary well-ordered set as a consequence. 
Comparisons are made using a ``measure function'' $\mu$ into an initial segment of the ordinals. 
The principle of ``general induction'' up to $\alpha$ is
\begin{center}
  $\operatorname{Prog}^{\mu}_x A(x) \to \forall x (\mu(x) \prec \overline{\alpha} \to A(x))$
\end{center}
where $\operatorname{Prog}^{\mu}_x A(x)$ expresses ``$\mu$-progressiveness'' with respect to the measure function $\mu$
and the ordering $\prec_{\alpha}$
\begin{center}
  $\operatorname{Prog}^{\mu}_x A(x) := \forall a (\forall y (\mu(y) \prec a \to A(y) \to \forall x (\mu(x) = a \to A(x))))$
\end{center}

We claim that general induction up to an ordinal $\alpha$ is provable from transfinite
induction up to $\alpha$. Indeed, assume $\operatorname{Prog}^{\mu}_x A(x)$. Let us show 
$\forall x (\mu(x) \prec \overline{\alpha} \to A(x))$. Consider $B(a) := \forall x(\mu(x) = a \to A(x))$.
It it sufficient to prove $\forall a \prec \overline{\alpha}B(a)$, which is 
$\forall a \prec \overline{\alpha} \forall x (\mu(x) = a \to A(x))$. By transfinite induction it is sufficient to
prove $\operatorname{Prog}_a B$, which is, in turn,
\begin{center}
  $\forall a (\forall b \prec a \forall y (\mu(y) = b \to A(y) \to \forall x (\mu(x) = a \to A(x))))$
\end{center}

But that follows from the assumption $\operatorname{Prog}^{\mu}_x A(x)$.

\subsection{Gentzen's theorem on transfinite induction in $\operatorname{PA}$}

We make use of Gentzen's result on the provability of transfinite induction up $\varepsilon_0$
to complete provable recursiveness of $H_{\alpha}$ and $F_{\alpha}$.
We will need some properties of $\prec$ and $\oplus$, the elementary function on ordinal codes
such that $\overline{\alpha} \oplus \overline{\beta} = \overline{\alpha + \beta}$.

\begin{lemma}~\label{plus:code} The following facts are provable in ${\bf I}\Delta_0(\operatorname{exp})$:

  \begin{enumerate}
    \item $a \prec 0 \to A$,
    \item~\label{plus:code:2} $c \prec b \oplus \omega^0 \to (c \prec b \to A) \to (c = b \to A) \to A$,
    \item~\label{plus:code:3} $a \oplus 0 = 0 \oplus a = a$,
    \item~\label{plus:code:4} $a \oplus (b \oplus c) = (a \oplus b) \oplus c$,
    \item~\label{plus:code:5} $\omega^a 0 = 0$,
    \item~\label{plus:code:6} $\omega^a(x + 1) = \omega^a x \oplus \omega^a$,
    \item~\label{plus:code:7} $a \neq 0 \to c \prec b \oplus \omega^a \to c \prec b \oplus w^{{\bf e}(a, b,c)} {\bf m}(a, b, c)$,
    \item~\label{plus:code:8} $a \neq c \to c \prec b \oplus \omega^a \to {\bf e}(a, b, c) \prec a$.
  \end{enumerate}
  where ${\bf e}$ and ${\bf m}$ denote appropriate elementary function constants.
\end{lemma}

\begin{theorem}[Gentzen, Parsons]

  For every $\Pi_2$-formula $F$ and each $i > 0$ we can prove in ${\bf I}\Sigma_{i + 1}$ the 
  principle of transfinite induction up to $\alpha$ for all $\alpha < \varepsilon_0(i)$.
\end{theorem}

\begin{proof}
  Let $A(a)$ be a $\Pi_j$ formula, let
  \begin{center}
    $A^+(a) := \forall b (\forall c \prec b A(c) \to \forall c \prec b \oplus \omega^a A(c))$
  \end{center}
  $A$ is $\Pi_j$, then by reduction to prenex form, $A^+$ is equivalent to a
  $\Pi_{j + 1}$-formula. The crucial point is that
  \begin{center}
    ${\bf I}\Sigma_j \vdash \operatorname{Prog}_a A(a) \to \operatorname{Prog}^+_a A(a)$.
  \end{center}
  Assume $\operatorname{Prog}_a A(a)$, i.e. $\forall a (\forall b \prec a A(b) \to A(a))$ and
  $\forall b \prec a A^{+}(b)$. We have got to show $A^+(a)$.
  So assume $\forall c \prec b A(c)$ and $c \prec b \oplus \omega^a$. Let us show $A(c)$.

  Let $a = 0$, then $c \prec b \oplus \omega^0$. By Lemma~\ref{plus:code}.~\ref{plus:code:2}, it
  is sufficient to derive $A(c)$ from $c \prec b$ as well as from $a = b$.
  If $c \prec b$, then $A(c)$ follows from $\forall c \prec b A(c)$ by quantifier elimination. If
  $c = b$, then $A(c)$ follows from $\operatorname{Prog}_a A(a)$ and $\forall c \prec b A(c)$.

  Assume $a \neq 0$ and $c \prec b \oplus \omega^a$, then we obtain the following by Lemma~\ref{plus:code}.~\ref{plus:code:7}
  \begin{center}
  $c \prec b \oplus \omega^{{\bf e}(a, b, c)}{\bf m}(a, b, c)$
  \end{center}
  and ${\bf e}(a, b, c)$ by Lemma~\ref{plus:code}.~\ref{plus:code:8}. From $\forall b \prec a A^{+}(b)$
  we get $A^+({\bf e}(a, b, c))$. By the definition of $A^+$ we have
  \begin{center}
    $\forall u \prec b \oplus \omega^{{\bf e}(a,b,c)} x \:\: A(u) 
    \to \forall u \prec (b \oplus \omega^{{\bf e}(a,b,c)} x) \oplus \omega^{{\bf e}(a, b, c) A(u)}$.
  \end{center}
  By using Lemma~\ref{plus:code}.~\ref{plus:code:4} and Lemma~\ref{plus:code}.~\ref{plus:code:6} we obtain
  \begin{center}
    $\forall u \prec b \oplus \omega^{{\bf e}(a,b,c)} x A (u)
    \to \forall u \prec (b \oplus \omega^{{\bf e}(a,b,c)}x) \oplus \omega^{{\bf e}(a, b, c)} A(u)$.
  \end{center}
  So we obtain by $\forall c \prec b A(c)$, Lemma~\ref{plus:code}.~\ref{plus:code:3} and Lemma~\ref{plus:code}.~\ref{plus:code:5}
  \begin{center}
    $\forall u \prec b \oplus \omega^{{\bf e}(a,b,c)}0 \:\: A(u)$
  \end{center}
  So by $\Pi_j$-induction we conclude $\forall u \prec b \oplus \omega^{{\bf e}(a,b,c)} {\bf m}(a,b,c) A(u)$
  and thus $A(c)$. Thus ${\bf I}\Sigma_j \vdash \operatorname{Prog}_a A(a) \to \operatorname{Prog}_a A^+(a)$.

  Take $i > 0$ and let $\prec$ denote the well-ordering $\prec_{\varepsilon_0(i)}$.
  Let $F(v)$ be $\Pi_2$-formula, define $A(a)$ to be $\forall v \prec a \: F(v)$. Thus $A$ is also $\Pi_2$
  and also the implication $\operatorname{Prog}_vF(v) \to \operatorname{Prog}_a A(a)$ is derivable
  in ${\bf I}\Delta_0(\operatorname{exp})$. Let us iterate the above procedure $i$ times starting with $j = 2$,
  we obtain the formulas $A^+, A^{++}, \ldots, A^{(i)}$, where $A^{(i)}$ is $\Pi_{i + 2}$ and
  \begin{center}
    ${\bf I}\Sigma_{i + 1} \vdash \operatorname{Prog}_v F(v) \to \operatorname{Prog}_u A^{(i)} (u)$.
  \end{center}

  Fix $\alpha < \varepsilon_0(i)$ and choose $k$ such that $\alpha \leq \varepsilon_0(i)(k)$. Apply the progressiveness
  of $A^{(i)}(u)$ $k + 1$ times, we obtain $A^{(i)}(\overline{k + 1})$. Thus
  \begin{center}
    ${\bf I}\Sigma_{i + 1} \vdash \operatorname{Prog}_v F(v) \to A^{(i)} (\overline{k + 1})$.
  \end{center}
  Let us instantiate the outermost universally quantified variable of $A^{(i)}$ to zero to obtain
  \begin{center}
  $A^{(i)}(\overline{k + 1}) \to A^{(i - 1)}(\omega^{\overline{ k + 1}})$.
  \end{center}
  Again, let us instantiate the outermost universally quantified variable in $A^{(i - 1)}$ to obtain
  \begin{center}
    $A^{(i - 1)}(\omega^{\overline{k + 1}}) \to A^{(i - 2)}(\omega^{\omega^{\overline{k + 1}}})$.
  \end{center}
  Continue this way and note that $\varepsilon_0(i)(k)$ consists of an exponential stack of $i$ $\omega$'s
  with $k + 1$ on the top, we finally get down to
  \begin{center}
    ${\bf I}\Sigma_{i + 1} \vdash \operatorname{Prog}_v F(v) \to A(\overline{\varepsilon_0(i)(k)})$.
  \end{center}
  But $A(\overline{\varepsilon_0(i)(k)})$ is just 
  \begin{center}
  $\forall v \prec \overline{\varepsilon_0(i)(k)} F(v)$.
  \end{center}
  We thus have proved the transfinite induction principle for $F$ up to 
  $\varepsilon_0(i)(k)$ in ${\bf I}\Sigma_{i + 1}$ and thus up to $\alpha$.
\end{proof}

\begin{theorem}
  For each $i$ and for every $\alpha < \varepsilon_0(i)$, the fast-growing function $F_{\alpha}$
  is provably recursive in ${\bf I}\Sigma_{i + 1}$.
\end{theorem}

\begin{proof}
  If $i = 0$, then $\alpha$ is finite and $F_{\alpha}$ is primitive recursive and thus
  $F_{\alpha}$ is provably recursive in ${\bf I}\Sigma_1$. Suppose $i > 0$. 
  As far as $F_{\alpha} = H_{\omega^{\alpha}}$, we need to show that for every 
  $\alpha < \varepsilon_0(i)$ that $H_{\omega^{\alpha}}$ is provably recursive in ${\bf I}\Sigma_{i + 1}$.
  The lemma above shows that the defining $\Pi_2$-formula $H(\omega^a)$ is provably (structurally) progressive
  in ${\bf I}\Sigma_2$. Thus, by, Gentzen's result
  \begin{center}
    ${\bf I}\Sigma_{i + 1} \vdash \forall a \prec \overline{\alpha} \:\: H(\omega^a)$.
  \end{center}
  Apply the progressiveness and obtain
  \begin{center}
    ${\bf I}\Sigma_{i + 1} \vdash H(\omega^{\overline{\alpha}})$
  \end{center}
  Thus ${\bf I}\Sigma_{i + 1}$ proves the $\Sigma_1$-definability of $H_{\omega^{\alpha}}$.
\end{proof}

\begin{col}
  Any $\varepsilon_0(i)$-recursive function is provably recursive in ${\bf I}\Sigma_{i + 1}$.
\end{col}

\begin{proof}
  We have already showed that each $\varepsilon_0(i)$-recursive function is register-machine computable
  in a number of steps bounded by some $F_{\alpha}$ with $\alpha < \varepsilon_0(i)$.
  Thus each such function is primitive recursively in ${\bf I}\Sigma_{i + 1}$. Thus we can show the
  $\Sigma_1$-definability of all $\varepsilon_0(i)$-recursive functions.
\end{proof}

\subsection{Ordinal bounds for provable recursion in $\operatorname{PA}$}

\subsection{The Infinitary System}

In this section we will be dealing with the infinitary system with 
sequents of the form $n : N \vdash^{\alpha} \Gamma$ where
\begin{enumerate}
  \item $n : N$ is an atomic formula declaring a bound on numerical inputs from
  which terms appearing in $\Gamma$ are computed according to the $N$-rules and axioms.
  \item $\Gamma$ is any finite set of closed formulas either of the form
  $m : N$ or formulas in the language of arithmetic based on the standard arithmetic signature.
  \item Ordinals $\alpha, \beta, \gamma, \cdots$ denote the heights of derivations.
  \item Note that any occurence of a number $n$ in a formula in such sequents 
  is the corresponding numeral.
\end{enumerate}

The system is axiomatised with the following inference rules:

\begin{prooftree}
  \AxiomC{$\alpha < \varepsilon_0$}
  \AxiomC{$m \leq n + 1$}
  \RightLabel{$N1$}
  \BinaryInfC{$n : N \vdash^{\alpha} \Gamma, m : N$}
\end{prooftree}

Let $\beta, \beta' \in \alpha[n]$:
\begin{prooftree}
  \AxiomC{$n : N \vdash^{\beta} n' : N$}
  \AxiomC{$n' : N \vdash^{\beta'}$}
  \RightLabel{$N2$}
  \BinaryInfC{$n : N \vdash^{\alpha} \Gamma$}
\end{prooftree}

Let $\Gamma$ be a context with a true atom, then:
\begin{prooftree}
  \AxiomC{$ $}
  \UnaryInfC{$n : N \vdash^{\alpha} \Gamma$}
\end{prooftree}

Let $\beta \in \alpha[n]$:
\begin{prooftree}
  \AxiomC{$n : N \vdash^{\beta} \Gamma, A, B $}
  \UnaryInfC{$n : N \vdash^{\alpha} \Gamma, A \lor B$}
\end{prooftree}

Let $\beta, \beta' \in \alpha[n]$:
\begin{prooftree}
  \AxiomC{$n : N \vdash^{\beta} \Gamma, A$}
  \AxiomC{$n : N \vdash^{\beta'} \Gamma, B$}
  \BinaryInfC{$n : N \vdash^{\alpha} \Gamma, A \land B$}
\end{prooftree}

For $\beta, \beta' \in \alpha[n]$:
\begin{prooftree}
  \AxiomC{$n : N \vdash^{\beta} m : N$}
  \AxiomC{$n : N \vdash^{\beta'} \Gamma, A(m)$}
  \BinaryInfC{$n : N \vdash^{\alpha} \Gamma, \exists x A(x)$}
\end{prooftree}

Let $\beta_i \in \alpha[\max(n, i)]$ for each $i$:
\begin{prooftree}
  \AxiomC{$i < \omega$}
  \AxiomC{$\max(n, i) : N \vdash^{\beta_i} \Gamma, A(i)$}
  \BinaryInfC{$n : N \vdash^{\alpha} \Gamma, \forall x A(x)$}
\end{prooftree}

Let $\beta, \beta' \in \alpha[n]$, then:
\begin{prooftree}
  \AxiomC{$n : N \vdash^{\beta} \Gamma, C$}
  \AxiomC{$n : N \vdash^{\beta'} \Gamma', \neg C$}
  \BinaryInfC{$n : N \vdash^{\alpha} \Gamma, \Gamma'$}
\end{prooftree}

\begin{definition}

  The functions $B_{\alpha}$ are defined by recursion:

  \begin{enumerate}
    \item $B_0(n) = n + 1$
    \item $B_{\alpha + 1}(n) = B_{\alpha}(B_{\alpha} (n))$
    \item $B_{\lambda}(n) = B_{\lambda(n)}(n)$
  \end{enumerate}
  where $\lambda$ denotes any limit ordinal with assigned fundamental sequence $\lambda(n)$.
\end{definition}

At successor stages $B_{\alpha}$ is composed with itself once, comparison with the fast-growing 
$F_{\alpha}$ shows that $B_{\alpha}(n) \leq F_{\alpha}(n)$ for each non-zero $n$. One can also
observe that every primitive recursive function is bounded by $B_{\omega \cdot k}$ for some $k$.
Just as for $H_{\alpha}$ and $F_{\alpha}$, $B_{\alpha}$ is strictly increasing and 
$B_{\beta}(n) < B_{\alpha}(n)$ whenever $\beta \in \alpha[n]$.

\begin{lemma}
  $m \leq B_{\alpha}(n)$ if and only if $n : N \vdash^{\alpha} m : N$
  is derivable by using the first two rules only.
\end{lemma}

\begin{proof}
  $ $

  \begin{enumerate}
    \item The ``if'' part: First of all, if $m \leq n + 1$, then we clearly have $m \geq B_{\alpha}(n)$.
    Secondly, if $n : N \vdash^{\alpha} m : N$ arises by the second rule from premises $n : N \vdash^{\beta} n' : N$
    and $n' : N \vdash^{\beta'} m : N$ where $\beta, \beta' \in \alpha[n]$ then, assuming inductively
    that $m \leq B_{\beta'}(n')$ and $n' \leq B_{\beta}(n)$. We have $m \leq B_{\beta'}(B_{\beta}(n))$
    and hence $m \leq B_{\alpha}(n)$.
    \item The ``only if'' part: proceed by induction on $\alpha$ and assume that $m \leq B_{\alpha}(n)$.
    If $\alpha = 0$ then $m \leq n + 1$ and thus $n : N \vdash^{\alpha} m : N$ by the first axiom.
    Assume $\alpha = \beta + 1$, then $m \leq B_{\beta}(n')$ where $n' = B_{\beta}(n)$, so by the induction
    hypothesis $n : N \vdash^{\beta} n' : N$ and $n' : N \vdash^{\beta} m : N$. And thus 
    $n : N \vdash^{\beta} m : N$ as far as $\beta \in \alpha[n]$.
    If $\alpha$ is limit, then $m \leq B_{\alpha(n)}$ and so $n : N \vdash^{\alpha(n)} m : N$
    by the induction hypothesis. As far as $\alpha[n] = \alpha(n)[n]$, the ordinal bounds $\beta$
    on the premises of the last derivation also lie in $\alpha[n]$, so $n : N \vdash^{\alpha} m : N$.
  \end{enumerate}
\end{proof}

\begin{definition}
  A sequent $n : N \vdash^{\alpha} \Gamma$ is \emph{term controlled} if every closed
  term from $\Gamma$ has a numerical value bounded by $B_{\alpha}(n)$. An infinitary derivation is \emph{term controlled}
  if every of its sequents is term controlled.
\end{definition}

\begin{lemma}[Bounding Lemma]~\label{bounding}
  Let $\Gamma$ be a set of $\Sigma_1$-formulas or atoms of the form $m : N$. If $n : N \vdash^{\alpha} \Gamma$
  has a term controlled derivation where all cut formulas are $\Sigma_1$, then $\Gamma$ is true at $B_{\alpha + 1}(n)$.
  The definition of ``true at'' is extended to include atoms $m : N$ by saying that $m : N$
  is true at $k$ if $m < k$.
\end{lemma}

\begin{proof}
  Induction over $\alpha$ according to the generation of the sequent $n : N \vdash^{\alpha} \Gamma$ which we will denote by $S$ below.

  \begin{enumerate}
    \item If $S$ is either a logical axiom or $N1$, then $\Gamma$ contains either a true atomic (in)equation or else an atom $m : N$
    where $m < n + 2$, so $\Gamma$ is true at $B_{\alpha + 1}(n)$.
    \item If $S$ is obtained by the N2 tule from $n : N \vdash^{\beta} n' : N$ and $n' : N \vdash^{\beta'} \vdash \Gamma$ for 
    $\beta, \beta' \in \alpha[n]$. By the induction hypothesis, $\Gamma$ is true at $B_{\beta' + 1}(n')$ 
    where $n' < B_{\beta + 1}(n)$. By persistence, $\Gamma$ is true at $B_{\beta' + 1}(B_{\beta + 1}(n))$, but we have
    \begin{center}
      $B_{\beta' + 1}(B_{\beta + 1}(n)) \leq B_{\alpha}(B_{\alpha (n)}) = B_{\alpha + 1}(n)$.
    \end{center}
    Thus $\Gamma$ is true at $B_{\alpha + 1}(n)$.
    \item The cases of $\land$ and $\lor$ are trivial.
    \item Note that $\forall$-rule can be applied in the case when we have $\Gamma = \Gamma', \forall x(x < t \to A(x))$,
    where $t$ is a closed term and $A(x)$ is a $\Delta_0(\operatorname{exp})$-formula. Assume that $S$ arises by the $\forall$-rule
    from the premises $\max(n, i) : N \vdash^{\beta_i} \Gamma', i \not < t \lor A(i)$ for $\beta_i \in \alpha[\max(n, i)]$ for each $i < \omega$.
    The derivation is term controlled, so the value of $t$ is less than or equal to $B_{\alpha}(n)$.
    By the induction hypothesis and persistence, for every $i < t$, the set $\Gamma'$, $A(i)$ is true at 
    $B_{\beta_i + 1}(B_{\alpha}(n))$. We have $\beta_i \in \alpha[B_{\alpha}(n)]$, so
    \begin{center}
    $B_{\beta_i + 1}(B_{\alpha(n)}) \leq B_{\alpha}(B_{\alpha}(n)) = B_{\alpha + 1}(n)$
    \end{center}
    So $\Gamma$ is true $B_{\alpha + 1}(n)$ by the persistence property.
    \item Assume $\Gamma$ contains a $\Sigma_1$-formulas $\exists x A(x)$ and $S$ arises by the $\exists$-rule
    from premises $n : N \vdash^{\beta} m : N$ and $n : N \vdash^{\beta'} \Gamma', A(m)$, then
    by the induction hypothesis $\Gamma, A(m)$ is true at $B_{\beta' + 1}(n)$ where $m < B_{\beta + 1}(n)$.
    Therefore, $\Gamma, A(m)$ is true at any number greater than $B_{\beta + 1}(n)$ and $B_{\beta' + 1}(n)$.
    As far as $\beta, \beta' \in \alpha[n]$ and both of them are less than $B_{\alpha + 1}(n)$, so $\Gamma$ is true at $B_{\alpha + 1}(n)$.
    \item Suppose $S$ is obtained by a cut on the $\Sigma_1$-formula $C := \exists \vec{x} D(\vec{x})$, where $D(\vec{x})$ 
    is a $\Delta(\operatorname{exp})$. So the premises are $n : N \vdash^{\beta} \Gamma, C$ and $n : N \vdash^{\beta'} \Gamma, \neq C$
    for $\beta, \beta' \in \alpha[n]$. We apply the induction hypothesis to the first premise to obtain numbers
    $\vec{m} < B_{\beta + 1}(n)$ such that $\Gamma, D(\vec{m})$ is true at $B_{\beta + 1}(n)$.
    From the second premise one can see, by induction on $\beta'$, that the universal quantifiers in 
    $\neg C := \forall \vec{x} \neg D(\vec{x})$ may be instantiated at $\vec{m}$ to give 
    $\max(n, \vec{m}) : N \vdash^{\beta'} \Gamma', \neg D(\vec{m})$. By the induction hypothesis we have $\Gamma', \neg D(\vec{m})$
    true at $B_{\beta' + 1}(\max(n, \vec{m}))$ which is less than $B_{\beta' + 1}(B_{\beta + 1}(n)) \leq B_{\alpha + 1}(n)$.
    Thus, by persistence, $\Gamma, \Gamma'$ is true at $B_{\alpha + 1}(n)$, for otherwise $D(\vec{m})$ and $\neg D(\vec{m})$.
  \end{enumerate}
\end{proof}

\subsection{Embedding of ${\bf PA}$}

We shall use Lemma~\ref{bounding} to embed Peano Arithmetic into the infinitary system and reduce
all the cuts to $\Sigma_1$-form.

\begin{lemma}[Weakening]
  Let $n : N \vdash^{\alpha} \Gamma$, $n \leq n'$, $\Gamma \subseteq \Gamma'$ and $\alpha[m] \subseteq \alpha'[m]$
  for every $m \geq n'$, then $n' : N \vdash^{\alpha'} \Gamma'$.
  If the given derivation of $n : N \vdash^{\alpha} \Gamma$ is term controlled, so will be the derivation $n' : N \vdash^{\alpha'} \Gamma'$
  whenever every new closed term is suitably bounded.
\end{lemma}

\begin{proof}
  Induction on $\alpha$. Note that if $n : N \vdash^{\alpha} \Gamma$ is an axiom, then both
  $\Gamma$ and $\Gamma'$ contain either a true atom or a declaration $m : N$ for $m \leq n + 1$. 
  Thus $n' : N \vdash^{\alpha} \Gamma'$.
  \begin{enumerate}
    \item If $n : N \vdash^{\alpha} \Gamma$ is obtained by the N2 rule from the premises
    $n : N \vdash^{\beta} m : N$ and $m : N \vdash^{\beta'} \Gamma$ for 
    $\beta, \beta' \in \alpha[n]$. By the induction hypothesis applied to each premise, we can increase $n$
    to $n'$ and $\Gamma$ to $\Gamma'$. But we have $\alpha[n] \subseteq \alpha[n'] \subseteq \alpha'[n']$,
    so we can apply the N2 rule to yield $n' : N \vdash^{\alpha'} \Gamma'$.
    \item Assume $n : N \vdash^{\alpha} \Gamma$ is derived from the $\exists$ rule from premises
    $n : N \vdash^{\beta} m : N$ and $n : N \vdash^{\beta'} \Gamma, A(m)$ where $\exists x A(x) \in \Gamma$
    and $\beta, \beta' \in \alpha[n]$ then, by applying the induction hypothesis to each premise,
    we increase $n$ to $n'$ and $\Gamma$ to $\Gamma'$ and we then apply the $\exists$ rule once more.
    \item Suppose $n : N \vdash^{\alpha} \Gamma$ arises by the $\forall$ rule from premises
    \begin{center}
      $\max(n, i) : N \vdash^{\beta_i} \Gamma, A(i)$
    \end{center}
    where $\forall x A(x) \in \Gamma$ and $\beta_i \in \alpha[\max(n, i)]$ for every $i$.
    We then apply the induction hypothesis to each of those premises, so we increase $n$ to $n'$ and $\Gamma$ to $\Gamma'$.
    So we re-apply the $\forall$ rule to obtain $n' : N \vdash^{\alpha'} \Gamma'$ as far as for each $i$ one has
    $\beta_i \in \alpha[\max(n', i)] \subseteq \alpha'[\max(n', i)]$.
  \end{enumerate}
\end{proof}

\begin{theorem}
  Suppose ${\bf PA} \vdash \Gamma(x_1, \ldots, x_k)$. Then there is a fixed number $d < \omega$
  such that for all numerical instantiations $n_1, \ldots, n_k$ of the free variables, we have a term controlled
  derivation of
  \begin{center}
    $\max(n_1, \ldots, n_k) : N \vdash^{\omega \cdot d} \Gamma(n_1, \ldots, n_k)$.
  \end{center}
\end{theorem}

\begin{proof}
  We work in a Tait-style axiomatisation of ${\bf PA}$, where the induction scheme is replaced with
  by the rule:
  \begin{prooftree}
    \AxiomC{$\Gamma, A(0)$}
    \AxiomC{$\Gamma, \neg A(z), A(z + 1)$}
    \BinaryInfC{$\Gamma, A(t)$}
  \end{prooftree}
  where $z$ is not free in $\Gamma$ and $t$ is any term.
  As earlier, we assume that the ${\bf PA}$ proof of $\Gamma(\vec{x})$ is cut-free and
  the non-atomic cut formulas are the induction formulas.
  \begin{enumerate}
    \item If $\Gamma(\vec{x})$ is an axiom of ${\bf PA}$, then, given the substitution $\vec{x} := \vec{n}$, 
    there must be a true atom in $\Gamma(\vec{n})$. So we have a derivation of 
    $\max(\vec{n}) : N \vdash^{\alpha} \Gamma(\vec{n})$ for arbitrary $\alpha$.

    However, one needs to choose $\alpha$ properly in order to make the derivation term-controlled.
    But there should be a number $d$ such that for all $\vec{n}$, $B_{\omega \cdot d}(\max \vec{n})$
    bounds the value of every $t(\vec{n})$. So we choose $\alpha := \omega \cdot d$.
    \item If $\Gamma(\vec{x})$ arises by the $\wedge$, $\lor$ or Cut rule from the premises
    $\Gamma_0(\vec{x})$ and $\Gamma_1(\vec{x})$. By the induction hypothesis, we assume that there
    are infinitary derivations of $\max \vec{n} : N \vdash^{\omega \cdot d_0} \Gamma_0(\vec{n})$
    and $\max \vec{n} : N \vdash^{\omega \cdot d_1} \Gamma_1(\vec{n})$
    where $d_0$ and $d_1$ are some natural numbers that do not depend on $\vec{n}$.
    Choose $d := \max(d_0, d_1) + 1$ and observe that $\omega \cdot d_0, \omega \cdot d_1 \in \omega \cdot d[\max \vec{n}]$.
    Then we apply the relevant infinitary rule and obtain $\max \vec{n} : N \vdash^{\omega \cdot d} \Gamma(\vec{n})$
    and the corresponding derivation is term controlled as well.
    \item Suppose $\Gamma(\vec{x})$ is obtained by applying the $\forall$ rule from the premises
    $\Gamma_0(\vec{x}), A(\vec{x}, z)$, so $\Gamma(\vec{x})$ has the form $\Gamma_0(\vec{x}), \forall z A(\vec{x}, z)$.
    Assume we have some $d_0$ such that for all $\vec{n}$ and for all $m$ there exists a term controlled derivation
    of $\max(\vec{n}, m) : N \vdash^{\omega \cdot d_0} \Gamma_0(\vec{n}), A(\vec{n}, m)$.
    Then we put $d = d_0 + 1$, so we have $\omega \cdot d_0 \in \omega \cdot d[\max(\vec{n}, m)]$.
    Then we apply the infinitary $\forall$ rule and then we obtain 
    $\max \vec{n} : N \vdash^{\omega \cdot d} \Gamma(\vec{n})$, which is also term controlled.
    \item The case of existential quantifiers is considered similarly to $\forall$.
    \item Finally, suppose $\Gamma(\vec{x}) = \Gamma_0(\vec{x}), A(\vec{x}, t(\vec{x}))$
    is obtained from $\Gamma_0(\vec{x}), A(\vec{x}, 0)$ and
    $\Gamma(\vec{x}), \neg A(\vec{x}, z), A(\vec{x}, z + 1)$. By the induction hypothesis
    there are $d_0$ and $d_1$ such that for all $\vec{n}$ and for all $i$ we have term controlled derivations of

    \begin{center}
      $\max \vec{n} : N \vdash^{\omega \cdot d_0} \Gamma_0(\vec{n}), A(\vec{n}, 0)$

      $\max(\vec{n}, i) : N \vdash^{\omega \cdot d_1} \Gamma_0(\vec{n}), \neg A(\vec{n}, i), A(\vec{n}, i + 1)$.
    \end{center}
  Now let $d_2$ be a number bigger than $\max(d_0, d_1)$ such that $B_{\omega \cdot d_2}$ bounds
  every subterm of $t(\vec{x})$. Then for all $\vec{n}$, if $m$ is the numerical value of the term $t(\vec{n})$
  we have a term controlled derivation of
  \begin{center}
    $\max(\vec{n}, m) : N \vdash^{\omega \cdot (d_2 + 1)} \Gamma_0(\vec{n}), A(\vec{n}, m)$.
  \end{center}
  If $m = 0$ this follows from the first premise above by weakening the ordinal bound.

  If $m > 0$, then by successive cuts on $A(\vec{n}, i)$ for $i < m$ with probably weakenings, we obtain
  a term controlled derivation of
  \begin{center}
    $\max(\vec{n}, m) : N \vdash^{\omega \cdot d_2 + m} \Gamma(\vec{n}), A(\vec{n}, m)$.
  \end{center}
  As far as $m \in \omega[\max(\vec{n}, m)]$ we have the ordinal bound $\omega \cdot (d_2 + 1)$.
  Since by our choice $d_2$, $\max(\vec{n}, m) \leq B_{\omega \cdot d_2}(\max \vec{n})$
  we also have
  \begin{center}
    $\max \vec{n} : N \vdash^{\omega \cdot d_2} \max(\vec{n}, m) : N$
  \end{center}
  and then we obtain
  \begin{center}
    $\max \vec{n} : N \vdash^{\omega \cdot (d_2 + 2)} \Gamma_0(\vec{n}), A(\vec{n}, m)$.
  \end{center}

  So we replace the numeral $m$ by the term $t(\vec{n})$. Finally we have a term controlled derivation of
  \begin{center}
    $\max \vec{n} : N \vdash^{\omega \cdot d} \Gamma_0(\vec{n}), A(\vec{n}, t(\vec{n}))$.
  \end{center}
  \end{enumerate}
\end{proof}

\subsection{Cut Elimination}

\begin{lemma}[$\forall$-inversion] If $n : N \vdash^{\alpha} \Gamma, \forall a A(a)$,
  then for each $m < \omega$ we have $\max(n, m) : N \vdash^{\alpha} \Gamma, A(m)$.
\end{lemma}

\begin{proof}
  Let us proceed by induction on $\alpha$.

  If the sequent $n : N \vdash^{\alpha} \Gamma, \forall a A(a)$ is an axiom, so is 
  $n : N \vdash^{\alpha} \vdash^{\alpha} \Gamma$, then the result is obtained by the weakening rule.

  Suppose $n : N \vdash^{\alpha} \Gamma, \forall a A(a)$ is the consequence of the $\forall$-rule
  with $\forall a A(a)$. Then the premises are, for each $i < \omega$,
  \begin{center}
    $\max(n, i) : N \vdash^{\beta_i} \Gamma, A(i), \forall a A(a)$
  \end{center}
  where $\beta_i \in \alpha[\max(n, i)]$.
  So by applying the induction hypothesis, we obtain $\max(n, m) : N \vdash^{\beta_m} \Gamma, A(m)$.
  By the weakening rule, we obtain $\max(n,m) : N \vdash^{\alpha} \Gamma, A(m)$.
  The rest is simple.
\end{proof}

\section{${\bf RCA}_0$}

\section{${\bf WKL}_0$}

\section{${\bf ACA}_0$}

\section{${\bf ATR}$}

\section{${\bf \Pi_1^1}$-comprehension}

\section{Kripke-Platek Set Theory}


\bibliographystyle{alpha}
\bibliography{Text}

\end{document}