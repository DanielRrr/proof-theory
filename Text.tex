\documentclass[8pt]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsopn}
\usepackage{comment}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{bussproofs}
\usepackage[all, 2cell]{xy}
\usepackage[all]{xy}
\usepackage{rotating}
\usepackage{lscape}
\usepackage{minted}


\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]

\theoremstyle{definition}
\newtheorem{claim}{Claim}[section]

\theoremstyle{definition}
\newtheorem{ex}{Example}[section] 

\theoremstyle{definition}
\newtheorem{cons}{Construction}[section] 

\theoremstyle{definition}
\newtheorem{rem}{Remark}[section] 


\theoremstyle{definition}
\newtheorem{prop}{Proposition}[section]

\theoremstyle{definition}
\newtheorem{lemma}{Lemma}[section]

\theoremstyle{definition}
\newtheorem{fact}{Fact}[section]

\theoremstyle{definition}
\newtheorem{remark}{Remark}[section]

\theoremstyle{definition}
\newtheorem{notation}{Notation}[section]

\theoremstyle{definition}
\newtheorem{example}{Example}[section]

\theoremstyle{definition}
\newtheorem{col}{Corollary}[section]

\theoremstyle{question}
\newtheorem{question}{Question}

\let\strokeL\L
\renewcommand\L{\mathbf{L}}

\title{Some Notes on Proof Theory and Elements of Ordinal Analysis}
\author{Daniel Rogozin}
\date{ }

\begin{document}

\maketitle

\tableofcontents

\newpage

\section{Provable Recursion in ${\bf I}\Delta_0(\operatorname{exp})$}

${\bf I}\Delta_0(\operatorname{exp})$ is a theory in first-order logic in the language:
\begin{center}
  $\{ =, 0, S, P, +, \dot{-}, \cdot, exp_2 \}$
\end{center}
where $S$ and $P$ are successor and precessor functions respectively.
Further, we will denote $S(x)$ and $P(x)$ as $x + 1$ and $x \dot{-} 1$ respectively.
$2^x$ stands for $exp_2(x)$.

The non-logical axioms of ${\bf I}\Delta_0(\operatorname{exp})$ are the following list:

\vspace{\baselineskip}

\begin{minipage}{0.45\textwidth}
  \begin{itemize}
    \item $x + 1 \neq 0$
    \item $0 \dot{-} 1 = 0$
    \item $x + 0 = x$
    \item $x \dot{-} 0 = x$
    \item $x \cdot 0 = 0$
    \item $2^0 = 1$
  \end{itemize}
\end{minipage}%
\hfill
\begin{minipage}{0.45\textwidth}
  \begin{itemize}
    \item $x + 1 = y + 1 \to x = y$
    \item $(x + 1) \dot{-} 1 = x$
    \item $x + (y + 1) = (x + y) + 1$
    \item $x \dot{-} (y + 1) = x \dot{-} y \dot{-} 1$
    \item $x \cdot (y + 1) = x \cdot y + x$
    \item $2^{x + 1} = 2^x + 2^x$
  \end{itemize}
\end{minipage}

\vspace{\baselineskip}

along with the bounded induction scheme:
\begin{center}
  $B(0) \land \forall x (B (x) \to B(x + 1)) \to \forall x B(x)$
\end{center}
where $B$ is a \emph{$\Delta$-formula}, that is a formula one of the following forms (with bounded quantifiers only):
\begin{itemize}
  \item $B \eqcirc \forall x < t P(x) \equiv \forall x (x < t \to P(x))$ 
  \item $B \eqcirc \exists x < t P(x) \equiv \exists x (x < t \land P(x))$
\end{itemize}

A $\Sigma_1$-formula is a formula of the form:
\begin{center}
  $\exists \vec{x} B(\vec{x})$
\end{center}
where $B(\vec{x}) \in \Delta_0$.

\begin{lemma}
  ${\bf I}\Delta_0(\operatorname{exp})$ proves (the universal closures of):
  \begin{enumerate}
    \item $x = 0 \lor x = (x \dot{-} 1) + 1$
    \item $x + (y + z) = (x + y) + z$
    \item $x \cdot (y \cdot z) = (x \cdot y) \cdot z$
    \item $x \cdot (y + z) = x \cdot y + x \cdot z$
    \item $x + y = y + x$
    \item $x \cdot y = y \cdot x$
    \item $x \dot{-} (y + z) = (x \dot{-} y) \dot{-} z$
    \item $2^{x + y} = 2^x \cdot 2^y$
  \end{enumerate}
\end{lemma}

\begin{proof}
$ $

  \begin{enumerate}
    \item This is self-evident.
    \item If $z = 0$, then $x + y = x + y$. If $z = z' + 1$, then, by applying the IH and the relevant axioms:
    \begin{center}
      $(x + (y + (z' + 1))) = (x + ((y + z') + 1)) = (x + (y + z')) + 1 = ((x + y) + z') + 1 = (x + y) + (z' + 1)$
    \end{center}
    \item If $z = 0$, then $x \cdot (y \cdot 0) = (x \cdot y) \cdot 0$. If $z = z' + 1$, then:
    \begin{center}
    $x \cdot (y \cdot (z' + 1)) = x \cdot (y \cdot z' + y) = x \cdot (y \cdot z') + x \cdot y =
    (x \cdot y) \cdot z' + x \cdot y = (x \cdot y) \cdot (z' + 1)$
    \end{center}
    \item The rest of the cases are shown by induction on $z$. Consider the exponentiation law.
    If $y = 0$, then

    \begin{center}
    $2^{x + 0} = 2^{x} = 0 + 2^{x} = 2^{x} \cdot 0 + 2^{x} = 2^{x} \cdot (0 + 1) = 2^x \cdot 2^0$
    \end{center}

    If $y = y' + 1$, then:
    \begin{center}
      $2^{x + (y' + 1)} = 2^{(x + y') + 1} = 2^x \cdot 2^y + 2^x \cdot 2^y = 2^{x} \cdot 2^{y + 1}$
    \end{center}
  \end{enumerate}
\end{proof}

\begin{lemma}
  ${\bf I}\Delta_0(\operatorname{exp})$ proves (the universal closures of):

  \begin{enumerate}
    \item $\neg x < 0$
    \item $x \leq 0 \leftrightarrow x = 0$
    \item $0 \leq x$
    \item $x \leq x$
    \item $x < x + 1$
    \item $x < y + 1 \leftrightarrow x \leq y$
    \item $x \leq y \leftrightarrow x < y \lor x = y$ 
    \item $x \leq y \land y \leq z \to x \leq z$
    \item $x < y \land y < z \to x < z$
    \item $x \leq y \lor y < x$
    \item $x < y \to x + z < y + z$
    \item $x < y \to x \cdot (z + 1) < y \cdot (z + 1)$
    \item $x < 2^x$
    \item $x < y \to 2^x < 2^y$
  \end{enumerate}
\end{lemma}

\begin{proof}
  Straightforward induction.
\end{proof}

\begin{definition}
  A function $f : \mathbb{N}^k \to \mathbb{N}$ is \emph{provably $\Sigma_1$} or \emph{provably recursive}
  in an arithmetical theory if there is a $\Sigma_1$ formula $F(\vec{x}, y)$, a ``defining formula'' of $f$, such that:
  \begin{enumerate}
    \item $f(\vec{n}) = m$ iff $\omega \models f(\vec{n}) = m$
    \item $T \vdash \exists y F(\vec{x}, y)$
    \item $T \vdash F(\vec{x}, y) \land F(\vec{x}, y') \to y = y'$
  \end{enumerate}
\end{definition}
If a defining formula $F \in \Delta_0$, then a function $f$ is \emph{provably bounded} 
in $T$ if there is a term $t(\vec{x})$ such that $T \vdash F(\vec{x}, y) \to y < t(\vec{x})$.

\begin{theorem}
  Let $f$ be a provably recursive in $T$, then we can conservatively extend $T$
  by adding a new function symbol $f$ along with the defining axiom $F(\vec{x}, f(\vec{x}))$.
\end{theorem}

\begin{proof}
  Let $\mathcal{M} \models T$, $\mathcal{M}$ can be made into a model $(\mathcal{M}, f)$ where
  we interpret $f$ as the function which is uniquely determined by the second and third conditions
  of the definitions above.
  Let $\varphi$ be a statement not involving $f$ such that $\varphi$ is true
  in $(\mathcal{M}, f)$, so $\varphi$ is true in $\mathcal{M}$ as well.
  By compactness $T$ proves $\varphi$.
\end{proof}

\begin{lemma}
  Each term defines a provably bounded function of ${\bf I}\Delta_0(\operatorname{exp})$.
\end{lemma}
\begin{proof}
  Let $f$ be a function defined by some ${\bf I}\Delta_0(\operatorname{exp})$-term $t$, 
  that is, $f(\vec{x}) = t(\vec{x})$.
  Take $y = t(\vec{x})$ as the defining formula for $f$ since 
  $\exists y \: (y = t(\vec{x}))$ is derivable.
  If $y' = t(\vec{x}) \land y = t(\vec{x})$, then $y = y'$ by transitivity.
  A formula $y = t(\vec{x})$ is bounded and $y = t$ implies $y < t + 1$.
  Thus $f$ is provably bounded.
\end{proof}

\begin{lemma}~\label{upper:bound:elem}
  Define $2_k(x)$ as $2_0(x) = x$ and $2_{n + 1}(x) = 2^{2_n(x)}$. 
  Then for every term $t(x_1, \dots, x_n)$ built up from the constants $0, S, P, +, \dot{-}, \cdot, exp_2$ there exists $k < \omega$ such that:
  \begin{center}
    ${\bf I}\Delta_0(\operatorname{exp}) \vdash t(x_1, \dots, x_n) < 2_k(\sum \limits_{k = 0}^n x_k)$
  \end{center}
\end{lemma}

\begin{proof}
Let $t$ be a term constructed from subterms $t_0$ and $t_1$ by using one of the function constants.
Assume that inductively $t_0 < 2_{k_0}(s_0)$ and $t_1 < 2_{k_1}(s_1)$ are both provable for some $k_0, k_1 < \omega$, where
$s_i$ is the sum of the variables of $t_i$ for $i = 0, 1$.

Let $s$ be the sum of all variables appearing in either $t_0$ or $t_1$ and let $k = \max(k_0, k_1)$.
Then one can prove $t_0 < 2_{k}(s)$ and $t_1 < 2_{k}(s)$. So one needs to show the following:
\begin{enumerate}
  \item $t_0 + 1 < 2_{k + 1}(s)$
  \item $t_0 \dot{-} 1 < 2_{k}(s)$
  \item $t_0 \dot{-} t_1 < 2_{k}(s)$
  \item $t_0 \cdot t_1 < 2_{k}(s)$
  \item $t_0 + t_1 < 2_{k}(s)$
  \item $2^{t_0} < 2_{k}(s)$
\end{enumerate}
So ${\bf I}\Delta_0(\operatorname{exp}) \vdash t < 2_{k + 1}(s)$.
\end{proof}

\begin{lemma}
  Let $f$ be a function defined by composition:
  \begin{center}
    $f(\vec{x}) = g_0(g_1(\vec{x}), \dots, g_m(\vec{x}))$
  \end{center}
  where $g_0, g_1, \dots, g_m$ are functions each of which is provably bounded in ${\bf I}\Delta_0(\operatorname{exp})$.
  Then $f$ is provably bounded in ${\bf I}\Delta_0(\operatorname{exp})$.
\end{lemma}

\begin{proof}
  Each $g_i$ has a defining formula $G_i$ and, by Lemma~\ref{upper:bound:elem}, there is a number $k_i < \omega$ such that:
  \begin{center}
    ${\bf I}\Delta_0(\operatorname{exp}) \vdash \exists y < 2_{k_i}(s) \: G_i (\vec{x}, y)$
  \end{center}
  where $s$ is the sum of elements of $\vec{x}$. And for $i = 0$ one has:
  \begin{center}
    ${\bf I}\Delta_0(\operatorname{exp}) \vdash \exists y < 2_{k_0}(s_0) \: G_0 (y_1, \dots, y_m, y)$
  \end{center}
  where $s_0$ is the sum of $y_1, \dots, y_m$.

  Let $k = \max \{ k_i < \omega \: | \: i < m + 1 \}$ and let $F(\vec{x}, y)$ be the bounded formula:
  \begin{center}
    $\exists y_1 < 2_{k}(s) \: \dots \: \exists y_m < 2_{k}(s) \: C(\vec{x}, y_1, \dots, y_m, y)$
  \end{center}
  where $C(\vec{x}, y_1, \dots, y_m, y)$ is the conjunction:
  \begin{center}
    $G_1(\vec{x}, y_1) \land \dots \land G_m(\vec{x}, y_m) \land G_0 (y_1, \dots, y_m, y)$
  \end{center}

  $F$ is clearly a defining formula for $f$ such that ${\bf I}\Delta_0(\operatorname{exp}) \vdash \exists y F(\vec{x}, y)$.

  Moreover, each $G_i$ is unique, so ${\bf I}\Delta_0(\operatorname{exp})$ also proves:

  \vspace{\baselineskip}

  $\begin{array}{lll}
    & C(\vec{x}, y_1, \dots, y_m, y) \land C(\vec{x}, z_1, \dots, z_m, z) \to & \\
    & \to \bigwedge \limits_{j = 1}^m y_j = z_j \land G_0(y_1, \dots, y_m, y) \land G_0(y_1, \dots, y_m, z) \to & \\
    & \to y = z&
  \end{array}$

  \vspace{\baselineskip}

  so we have (by first order logic):
  \begin{center}
    ${\bf I}\Delta_0(\operatorname{exp}) \vdash F(\vec{x}, y) \land F(\vec{x}, z) \to y = z$
  \end{center}

  Thus $f$ is provably $\Sigma_1$ in ${\bf I}\Delta_0(\operatorname{exp})$, so the rest is to find its bounding term.

  ${\bf I}\Delta_0(\operatorname{exp})$ proves the following:

  \begin{center}
    $C(\vec{x}, y_1, \dots, y_m, y) \to \bigwedge \limits_{j = 1}^m y_j < 2_k(s) \land y < 2_k(y_1 + \dots + y_m)$
  \end{center}

  and

  \begin{center}
    $\bigwedge \limits_{j = 1}^m y_j < 2_k(s) \to y_1 + \dots + y_m < 2_k(s) \cdot m$
  \end{center}

  Put $t(\vec{x}) = 2_k(2_k(s) \cdot m)$, then we obtain

  \begin{center}
    ${\bf I}\Delta_0(\operatorname{exp}) \vdash C(\vec{x}, y_1, \dots, y_m, y) \to y < t(\vec{x})$
  \end{center}

  and so

  \begin{center}
    ${\bf I}\Delta_0(\operatorname{exp}) \vdash F(\vec{x}, y) \to y < t(\vec{x})$
  \end{center}
\end{proof}

\begin{lemma}
  Suppose $f$ is defined by bounded minimisation
  \begin{center}
    $f(\vec{n}, m) = \mu_{k < m} (g(\vec{n}, k) = 0)$
  \end{center}
  from a function $g$ which is provably bounded in ${\bf I}\Delta_0(\operatorname{exp})$.
  Then $f$ is provably bounded in ${\bf I}\Delta_0(\operatorname{exp})$.
\end{lemma}

\begin{proof}
   Let $G$ be a defining formula for $g$. Let $F(\vec{x}, z, y)$ be the bounded formula
   \begin{center}
    $y \leq z \land \forall i < y \neg G(\vec{x}, i, 0) \land (y = z \lor G(\vec{x}, y, 0))$
   \end{center}

   $\omega \models F(\vec{n}, m, k)$ iff either $k$ is the least number less than $m$ such that $g(\vec{n}, k) = 0$ or 
   there is no such and $k = m$. Thus it means that $k$ is the value of $f(\vec{n}, m)$, so $F$ is a defining formula for $f$.

   Furthermore
   \begin{center}
   ${\bf I}\Delta_0(\operatorname{exp}) \vdash F(\vec{x}, z, y) \to y < z + 1$
   \end{center}
   so $t(\vec{x}, z) = z + 1$ can be taken as a bounding term for $f$.

   We can prove:
   \begin{center}
    $F(\vec{x}, z, y) \land F(\vec{x}, z, y') \land y < y' \to G(\vec{x}, y, 0) \land \neg G(\vec{x}, y, 0)$
   \end{center}
   and similarly for interchanged $y$ and $y'$. So we can prove:
   \begin{center}
    $F(\vec{x}, z, y) \land F(\vec{x}, z, y') \to \neg y < y' \land \neg y' < y$
   \end{center}
   As far as $y < y' \lor y' < y \lor y = y'$, we have
   \begin{center}
    $F(\vec{x}, z, y) \land F(\vec{x}, z, y') \to y = y'$
   \end{center}

   Now we have to check that ${\bf I}\Delta_0(\operatorname{exp}) \vdash \exists y F(\vec{x}, z, y)$.
   We construct such $y$ by bounded induction on $z$.

   \begin{enumerate}
    \item $z = 0$.

    $F(\vec{x}, 0, 0)$ is provable since $y = 0 \leftrightarrow y \leq 0$ and $\neg i < 0$. 
    So ${\bf I}\Delta_0(\operatorname{exp}) \vdash F(\vec{x}, 0, y)$ is provable.
    \item Assume $\exists y F(\vec{x}, z, y)$ is provable, let show that that $\exists y F(\vec{x}, z + 1, y)$ is provable.

    We can show $y \leq z \to y + 1 \leq z + 1$ and, via $i < y + 1 \leftrightarrow i < y \lor i = y$,
    \begin{center}
      $\forall i < y \: \neg G(\vec{x}, i, 0) \land ((y = z) \land \neg G(\vec{x}, y, 0)) \to \forall i < y + 1 \: \neg G(\vec{x}, i, 0) \land y + 1 = z + 1$
    \end{center}
    Therefore
    \begin{center}
    $F(\vec{x}, z, y) \to F(\vec{x}, z + 1, y + 1) \lor F(\vec{x}, z + 1, y)$
    \end{center}
    and thus:
    \begin{center}
      $\exists y F(\vec{x}, z, y) \to \exists y F(\vec{x}, z + 1, y)$
    \end{center}
   \end{enumerate}
\end{proof}

\begin{theorem}~\label{provablysigma1:1}
  Every elementary function is provably bounded in ${\bf I}\Delta_0(\operatorname{exp})$.
\end{theorem}

\begin{proof}
  As we know from recursion theory, the class of elementary functions can be characterised
  as those functions which are definable from $0$, $S$, $P$, $\cdot$, $+$, $exp_2$, $\dot{-}$ and $\cdot$
  by composition and minimisation. And then we apply above lemmas.
\end{proof}

\subsection{Proof-theoretic Characterisation}

For this section we shall be using a Tait-style formalisation of ${\bf I}\Delta_0(\operatorname{exp})$.
We have the following logical rules:

\vspace{\baselineskip}

\begin{prooftree}
  \AxiomC{$ $}
  \RightLabel{{\bf Ax}}
  \UnaryInfC{$\Gamma, R\vec{t}, \neg R\vec{t}$}
\end{prooftree}
\begin{minipage}{0.45\textwidth}
  \begin{prooftree}
    \AxiomC{$\Gamma, A_0, A_1$}
    \RightLabel{$\vee$}
    \UnaryInfC{$\Gamma, A_0 \vee A_1$}
  \end{prooftree}

  \begin{prooftree}
    \AxiomC{$\Gamma, A(t)$}
    \RightLabel{$\exists$}
    \UnaryInfC{$\Gamma, \exists x A(x)$}
  \end{prooftree}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
  \begin{prooftree}
    \AxiomC{$\Gamma, A_0$}
    \AxiomC{$\Gamma, A_1$}
    \RightLabel{$\land$}
    \BinaryInfC{$\Gamma, A_0 \land A_1$}
  \end{prooftree}

  \begin{prooftree}
    \AxiomC{$\Gamma, A$}
    \RightLabel{$\forall$}
    \UnaryInfC{$\Gamma, \forall x A$}
  \end{prooftree}
\end{minipage}

\vspace{\baselineskip}

where $R\vec{t}$ is an atomic formula and $x$ is not free in $A$ in the $\forall$ rule.
Here $\Gamma$ stores all non-logical axioms of ${\bf I}\Delta_0(\exp)$ along with its negations.
We also have the bounded induction rule:
  \begin{prooftree}
    \AxiomC{$\Gamma, B(0)$}
    \AxiomC{$\Gamma, \neg B(n), B(n + 1)$}
    \RightLabel{${\bf BInd}$}
    \BinaryInfC{$\Gamma, B(t)$}
  \end{prooftree}
where $B$ is a bounded formula and $t$ is any term. 

Of course, the cut rule is admissible:
  \begin{prooftree}
    \AxiomC{$\Gamma, A$}
    \AxiomC{$\Gamma, \neg A$}
    \RightLabel{${\bf cut}$}
    \BinaryInfC{$\Gamma$}
  \end{prooftree}


\begin{definition}
  Let $\exists \vec{z} B(\vec{z})$ be a closed $\Sigma_1$-formula, then it is \emph{true at $m$}, written as 
  $m \models \exists \vec{z} B(\vec{z})$, if there exist natural numbers $m_1, \dots, m_l$ such that each 
  $m_i < m$ and $B(\vec{m})$ is true in the standard model.

  A finite set $\Gamma$ of closed $\Sigma_1$-formulas is true at $m$, written as $m \models \Gamma$ if at least one of them is true at $m$.
\end{definition}

If $\Gamma(x_1, \dots, x_k)$ is a finite set of $\Sigma_1$-formulas whose free variables occur amongst $x_1, \dots, x_k$.
Let $f : \mathbb{N}^{k} \to \mathbb{N}$, then $f \models \Gamma(x_1, \dots, x_k)$ we have $f(\vec{n}) \models \Gamma(x_1 := n_1, \dots, x_k := n_k)$
for each $\vec{n} = (n_1, \dots, n_k)$.

\begin{fact} {\bf (Persistence)}

  \begin{enumerate}
    \item If $m \leq m'$, then $m \models \exists \vec{z} B(\vec{z})$ implies $m' \models \exists \vec{z} B(\vec{z})$.
    \item If $\forall \vec{n} \in \mathbb{N}^{k}$ $f(\vec{n}) \leq f'(\vec{n})$, 
    then $f(\vec{n}) \models \Gamma(x_1 := n_1, \dots, x_k := n_k)$ implies $f'(\vec{n}) \models \Gamma(x_1 := n_1, \dots, x_k := n_k)$.
  \end{enumerate}
\end{fact}

\begin{lemma}~\label{truth:idelta0}
  Let $\Gamma(\vec{x})$ be a finite set of $\Sigma_1$ formulas such that 
  \begin{center}
  ${\bf I}\Delta_0(exp) \vdash \bigvee \limits_{\gamma(\vec{x}) \in \Gamma(\vec{x})} \gamma(\vec{x})$.
  \end{center}
  Then there is an elementary function $f$ such that $f \models \Gamma(\vec{x})$ and $f$ is strongly increasing on its variables.
\end{lemma}

\begin{proof}

  If $\Gamma$ is provable in ${\bf I}\Delta_0(exp)$, 
  then it is provable in the Tait-style version of ${\bf I}\Delta_0(exp)$, where all cut formulas are $\Sigma_1$.

  If $\Gamma$ is classically derivable from non-logical axioms $A_1, \dots, A_s$, then there is a cut-free proof
  in the Tait calculus of $\neg A_1, \Delta, \Gamma$, where $\Delta = \neg A_2, \dots, \neg A_s$. Let us show how to cancel $\neg A_1$ using a $\Sigma_1$-cut.

  If $A_1$ is an induction axiom on some formula $B$, then we have a cut-free proof of:
  \begin{center}
    $B(0) \land \forall y (\neg B(y) \lor B(y + 1)) \land \exists x \neg B(x), \Delta, \Gamma$
  \end{center}

  Thus we also have cut-free proofs of $B(0), \Delta, \Gamma$, $\neg B(y), B(y + 1), \Delta, \Gamma$
  and $\exists x \neg B(x), \Delta, \Gamma$. So we have
  \begin{prooftree}
    \AxiomC{$\Delta, \Gamma, B(0)$}
    \AxiomC{$\Delta, \Gamma, \neg B(y), B(y + 1)$}
    \RightLabel{${\bf BInd}$}
    \BinaryInfC{$\Delta, \Gamma, B(x)$}
    \RightLabel{$\forall$}
    \UnaryInfC{$\Delta, \Gamma, \forall x B(x)$}
    \AxiomC{$\exists x \neg B(x), \Delta, \Gamma$}
    \RightLabel{$\Sigma_1$-{\bf cut}}
    \BinaryInfC{$\Delta, \Gamma$}
  \end{prooftree}

  We can similarly cancel each of $\neg A_2, \dots, \neg A_s$ and so obtain the proof of $\Gamma$ with $\Sigma_1$-cuts only.

  Now we choose a proof of $\Gamma(\vec{x})$ and proceed by induction on the height of the proof and determine
  an elementary function $f$ such that $f \models \Gamma$.

  \begin{enumerate}
    \item If $\Gamma(\vec{x})$ is an axiom, then for all $\vec{n}$ $\Gamma(\vec{n})$ contains a true atom.
    So for any $f$ $f \models \Gamma$. Let us choose $f(\vec{n}) = n_1 + \dots + n_k$.
    \item If $\Gamma, B_0 \vee B_1$ is derivable, so is $\Gamma, B_0, B_1$. Note that $B_0$ and $B_1$ are both bounded.
    Let $f \models \Gamma, B_0, B_1$, then $f \models \Gamma, B_0 \vee B_1$.
    \item Assume $\Gamma, B_0 \land B_1$ is derivable, then $\Gamma, B_0$ and $\Gamma, B_1$
    By the induction hypothesis we have $f_0 \models \Gamma, B_0$ and $f_1 \models \Gamma, B_1$, so, by persistence,
    we have $\lambda \vec{n}. f_0(\vec{n}) + f_1(\vec{n}) \models \Gamma, B_0 \land B_1$.
    \item Assume $\Gamma, \forall y B(y)$ is derivable, then $\Gamma, B(y)$ is derivable and $y$ is not free in $\Gamma$.
    Since all the formulas are $\Sigma_1$, $\forall x B(y)$ must be bounded, so $B(y) \eqcirc \neg (y < t) \lor B'(y)$ for some
    term $t$ and for some bounded formula $B'$.
    By the induction hypothesis, assume $f_0 \models \Gamma, \neg (y < t), B'(y)$ for some increasing elementary function $f_0$.
    Then we have:
    \begin{center}
      $f_0(\vec{n}, k) \models \Gamma(\vec{n}), \neg (k < t(\vec{n})), B'(\vec{n}, k)$
    \end{center}
    Let $g$ be an increasing elementary function bounding $t$, define
    \begin{center}
      $f(\vec{n}) = \sum \limits_{k < g(\vec{n})} f(\vec{n}, k)$
    \end{center}
    We have either $f(\vec{n}) \models \Gamma(\vec{n})$ or, by persistence, $B'(\vec{n}, k)$ is true for every $k < t(\vec{n})$.
    So $f \models \Gamma, \forall y B(y)$ and $f$ is elementary.
    \item Assume $\Gamma, \exists y A(y, \vec{x})$ is derivable, so $\Gamma, A(t, \vec{x})$ is derivable for some term $t$.
    By the IH, there is elementary $f_0$ such that for all $\vec{n}$ one has
    \begin{center}
      $f_0(\vec{n}) \models \Gamma(\vec{n}), A(t(\vec{n}), \vec{n})$
    \end{center}
    Then either $f_0(\vec{n}) \models \Gamma(\vec{n})$ or else $f_0(\vec{n})$ bounds true witnesses for all
    existential quantifiers in $A(t(\vec{n}), \vec{n})$. Choose an elementary function $g$ which is bounding for $t$.
    Define $f(\vec{n}) = f_0(\vec{n}) + g(\vec{n})$, then for all $\vec{n}$ either 
    $f(\vec{n}) \models \Gamma(\vec{n})$ or $f(\vec{n}) \models \exists y A(y, \vec{n})$.
    \item Assume $\Gamma$ comes about by the cut rule with $\Sigma_1$ formula 
    $C \eqcirc \exists \vec{z} B(\vec{z})$, so the premises are
    $\Gamma, \forall \vec{z} \neg B(\vec{z})$ and $\Gamma, \exists \vec{z} B(\vec{z})$.

    Without increasing the height of a proof, we can invert all universal quantifiers
    in the first premise. So we have $\neg B(\vec{z})$. $B$ is bounded, so the induction hypothesis
    can be applied to this formula to obtain an elementary function $f_0$ such that, 
    for all assignments $[\vec{x} := \vec{n}]$ and $[\vec{z} := \vec{m}]$
    \begin{center}
      $f_0(\vec{n}, \vec{m}) \models \Gamma(\vec{n}), \neg B(\vec{n}, \vec{m})$
    \end{center}
    Now we apply the induction hypothesis to the second premise of the cut rule, so we have an elementary function
    $f_1$ such that for all $\vec{n}$ either 
    $f_1(\vec{n}) \models \Gamma(\vec{n})$ or there are fixed witnesses $\vec{m} < f_1(\vec{n})$
    such that $B(\vec{n}, \vec{m})$ is true.

    Define $f$ the following way:
    \begin{center}
      $f(\vec{n}) = f_0(\vec{n}, f_1(\vec{n}), \dots, f_1(\vec{n}))$
    \end{center}
    Furthermore $f \models \Gamma$. For otherwise there would be a tuple $\vec{n}$ such that
    $\Gamma(\vec{n})$ is not true at $f(\vec{n})$, so, by persistence, 
    $\Gamma(\vec{n})$ is not true at $f_1(\vec{n})$.
    Thus $B(\vec{n}, \vec{m})$ is true for particular numbers $\vec{m} < f_1(\vec{n})$.
    But then $f_0(\vec{n}, \vec{m}) < f(\vec{n})$, so, by persistence, $\Gamma(\vec{n})$ cannot be true at 
    $f_0(\vec{n}, \vec{m})$. Thus $B(\vec{n}, \vec{m})$ is false, so we have a contradiction.
    \item Finally suppose $\Gamma(\vec{x}), B(\vec{x}, t)$ comes from the induction rule on a bounded formula $B$.
    The premises of the rule $\Gamma(\vec{x}), B(\vec{x}, 0)$ and $\Gamma(\vec{x}), \neg B(\vec{x}, y), B(\vec{x}, y + 1)$.

    Let us apply the induction hypothesis to each of the premises, and then we obtain
    increasing elementary functions $f_0$ and $f_1$ such that for all $\vec{n}$ and for all $k$

    \begin{center}
      $f_0(\vec{n}) \models \Gamma(\vec{n}), B(\vec{n}, 0)$

      $f_1(\vec{n}, k) \models \Gamma(\vec{n}), \neg B(\vec{n}, k), B(\vec{n}, k + 1)$
    \end{center}

    Now let
    \begin{center}
      $f(\vec{n}) = f_0(\vec{n}) + \sum \limits_{k < g(\vec{n})} f_1(\vec{n}, k)$
    \end{center}
    where $g$ is an increasing elementary function which is bounding for the term $t$.
    $f$ is elementary and increasing, and, by persistence for $f_0$ and $f_1$, we have either
    $f(\vec{n}) \models \Gamma(\vec{n})$ or else
    $B(\vec{n}, 0)$ and $B(\vec{n}, k) \to B(\vec{n}, k + 1)$ are true for all $k < t(\vec{n})$.
    In either case, we have $f \models \Gamma(\vec{x}), B(\vec{x}, t(\vec{x}))$.
  \end{enumerate}
\end{proof}

\begin{theorem}
  A number-theoretic function is elementary iff $f$ is provably $\Sigma_1$ in ${\bf I}\Delta_0(exp)$.
\end{theorem}

\begin{proof}
  The only if part is in Theorem~\ref{provablysigma1:1}, so we show the if part only.
  Assume $f$ is provably $\Sigma_1$ in ${\bf I}\Delta_0(exp)$. Then we have a formula
  \begin{center}
    $F(\vec{x}, y) = \exists z_1 \dots \exists z_k B(\vec{x}, y, z_1, \dots, z_k)$
  \end{center}
  which defines $f$ and such that
  \begin{center}
  ${\bf I}\Delta_0(exp) \models \exists y F(\vec{x}, y)$
  \end{center}
  By Lemma~\ref{truth:idelta0}, there exists an elementary function $g$ such that for every
  tuple of arguments $\vec{n}$ there are numbers $m_0, \dots, m_k$
  less that $g(n)$ satisfying the bounded formula $B(\vec{n}, m_0, m_1, \dots, m_k)$.
  Apply the elementary sequence coding:
  \begin{center}
    $h(\vec{n}) = \langle g(\vec{n}), g(\vec{n}), \dots, g(\vec{n})\rangle$
  \end{center}
  so that if $m = \langle m_0, m_1, \dots, m_k \rangle$
  where $m_i < g(\vec{n})$ for each $i \in n + 1$, so $m < h(\vec{n})$.

  As far as $f(\vec{n})$ is the unique $m_0$ for which there are $m_1, \dots, m_k$ satisfying
  $B(\vec{n}, m_0, \dots, m_k)$, we define $f$ as:
  \begin{center}
    $f(\vec{n}) = (\mu_{m < h(\vec{n})} B(\vec{n}, (m)_0, (m)_1, \dots, (m)_k))_0$.
  \end{center}

  $B$ is a bounded formula of ${\bf I}\Delta_0(exp)$, $B$ is elementarily 
  decidable. Moreover, elementary functions are closed under composition and bounded minimisation,
  so $f$ is elementary.
\end{proof}

\section{Primitive Recursion and ${\bf I}\Sigma_1$}

${\bf I}\Sigma_1$ is an arithmetical theory where the induction scheme is
restructed to $\Sigma_1$ formulas.

\begin{lemma}
  Every primitive recursion is provably recursive in ${\bf I}\Sigma_1$.
\end{lemma}

\begin{proof}
  We have to show represent each primitive recursive function $f$ with a $\Sigma_1$ formula 
  $F(\vec{x}, y) := \exists z C(\vec{x}, y, z)$ such that:
  \begin{enumerate}
    \item $f(\vec{n}) = m$ iff $\omega \models F(\vec{x}, y)$.
    \item ${\bf I}\Sigma_1 \vdash \exists y F(\vec{x}, y)$.
    \item ${\bf I}\Sigma_1 \vdash F(\vec{x}, y) \land F(\vec{x}, y') \to y = y'$.
  \end{enumerate}

  In each case $C(\vec{x}, y, z)$ will be a $\Delta_0(exp)$-formula
  constructed via sequence encoding in ${\bf I}\Delta_0(exp)$.
  Such a formula expresses that $z$ is a uniquely determined sequence number encoding the computation of
  $f(\vec{x}) = y$ and containing the output value $y$ as its final element, so $y = \pi_2(z)$.

  Condition 1 will hold by the definition of $C$. 
  Condition 3 will be satisfied by the uniqueness of $z$. We consider five definitional schemes by which $f$ could be
  introduced:
  \begin{enumerate}
    \item $f$ is the constant-zero function, that is, $f(x) = 0$, no matter what $x$ is.
    Then we take $C := y = 0 \land z = \langle 0 \rangle$. All the conditions are obviously satisfied.
    \item If $f$ is the successor function $f(x) = x + 1$, we let
    \begin{center}
      $C(x,y,z) := y = x + 1 \land z = \langle x + 1 \rangle$
    \end{center}
    All the conditions are obvious.
    \item Now assume $f$ is the projection function $f(x_0, \dots, x_n) = x_i$ for some $i \in n + 1$.
    We let
    \begin{center}
      $C(\vec{x},y,z) := y = x_i \land z = \langle x_i \rangle$
    \end{center}
    \item Now assume $f$ is defined by substitution from previously generated primitive recursive functions $f_0, f_1, f_2$:
    \begin{center}
      $f(\vec{x}) = f_0(f_1(\vec{x}), f_2(\vec{x}))$
    \end{center}

    By the induction hypothesis, assume thatb $f_0, f_1, f_2$ are provably recursive and we have 
    $\Delta_0(exp)$-formulas $C_0, C_1, C_2$ encoding their computations ($\operatorname{len}(z) = 4$).
    For the function $f$ define:
    \begin{center}
      $C(\vec{x}, y, z) := \bigwedge \limits_{i \in \{ 1, 2\}} C_i(\vec{x}, \pi_2((z)_i), (z)_i) \land 
      C_0(\pi_2((z)_1), \pi_2((z)_2), y, (z)_0) \land (z)_3 = y$.
    \end{center}

    Let us check the required conditions:
    \begin{enumerate}
      \item Condition 1 holds since $f(\vec{n}) = m$ iff there are numbers $m_1$ and $m_2$ such that
      $f_1(\vec{n}) = m_1$, $f_2(\vec{n}) = m_2$ and $f_0(m_1, m_2) = m$.
      These hold if and only if there are number $k_1, k_2, k_0$ such that 
      $C_1(\vec{n}, m_1, k_1)$, $C_2(\vec{n}, m_2, k_2)$ and $C_0(m_1, m_2, m, k_0)$ are all true.
      And these hold if and only if $C(\vec{n}, m, \langle k_0, k_1, k_2, m \rangle)$ is true.
      Thus $f(\vec{n}) = m $ iff and only if $F(\vec{n}, m) = \exists z C(\vec{n}, m, z)$ is true.
      \item Condition 2 holds since from $C_1(\vec{x}, y_1, z_1)$, $C_2(\vec{x}, y_2, z_2)$
      and $C(y_1, y_2, y, z_0)$ we can derive 
      $C(\vec{x}, y, \langle z_0, z_1, z_2, y \rangle)$ in ${\bf I}\Delta_0$.
      So provided $\exists y \exists z C_1(\vec{x}, y, z)$, $\exists y \exists z C_2(\vec{x}, y, z)$ and
      $\forall y_1 \forall y_2 \exists y \exists z C(y_1, y_2, y, z)$, we can prove $\exists y F(\vec{x}, y) := 
      C(\vec{x}, y, z)$.
      \item Condition 3 is self-evident.
    \end{enumerate}
    \item Now assume that $f$ is defined from $f_1$ and $f_2$ by primitive recursion:
    \begin{center}
      $f(\vec{v}, 0) = f_0(\vec{v})$

      $f(\vec{v}, x + 1) = f_1(\vec{v}, x, f(\vec{v}, x))$
    \end{center}
    By the induction hypothesis $f_0$ and $f_1$ are provably recursive and they have associated $\Delta_0$-formulas
    $C_0$ and $C_1$. Define
    \begin{center}
    $\begin{array}{lll}
      & C(\vec{v}, x, y, z) := C_0(\vec{v}, \pi_2((z)_0), (z)_0) \land & \\
      & \:\:\:\: \forall i < x \:\: (C_i (\vec{v}, i, \pi_2((z)_i), \pi_2((z)_{i + 1}))) \land & \\
      & \:\:\:\: (z)_{x + 1} = y \land \pi_2((z)_x) = y &
    \end{array}$
  \end{center}

  Let us check that all the conditions are satisfied:
  \begin{enumerate}
    \item Condition 1 holds since $f(\vec{l}, n) = m$ if and only if there is a sequence number
    $k = \langle k_0, \dots, k_n, m \rangle$ such that $k_0$ encodes the computation of 
    $f(\vec{l}, 0)$ with the value $\pi_2(k_0)$, and for each $i < n$, 
    $k_{i + 1}$ codes the computation of $f(\vec{l}, i + 1) = f_1(\vec{l}, i, \pi_2(k_i))$
    with values $\pi_2(k_{i + 1})$ and $\pi_2(k_n) = m$.
    This is equivalent to $\models F(\vec{l}, n, m) \leftrightarrow \exists z C(\vec{l}, n,m, z)$.
    \item To show Condition 2 we have to prove the following in ${\bf I}\Delta_0$
    \begin{center}
      $C_0(\vec{v}, y, z) \to C(\vec{v}, 0, y, \langle z, y \rangle)$
    \end{center}
    and
    \begin{center}
      $C(\vec{v}, x, y, z) \land C_1(\vec{v}, x, y,y',z') \to C(\vec{v}, x + 1, y', t)$
    \end{center}
    for a suitable term $t$ which removes the end component $y$ of $z$ and replaces it by $z'$, 
    and then adds the final component $y'$. More specifically
    \begin{center}
      $t = \pi(\pi(\pi_1(z), z'), y')$
    \end{center}
    Hence from $\exists y \exists z C_0(\vec{v},y,z)$ we obtain 
    $\exists y \exists z C(\vec{v}, 0, y, z)$, and from $\forall y \exists y' \exists z' C_1(\vec{v}, x, y, y', z')$ one can derive
    \begin{center}
      $\exists y \exists z C(\vec{v}, x,y,z) \to \exists y \exists z C(\vec{v}, x + 1, y, z)$
    \end{center}
    We have assumed that $f_0$ and $f_1$ are primitive recursive, we can prove $\exists y F(\vec{v}, 0, y)$
    and $\exists y F(\vec{v}, x, y) \to \exists y F(\vec{v}, x + 1, y)$.
    Then we derive $\exists y F(\vec{v}, x, y)$ by using $\Sigma_1$-induction.
    \item To show Condition 3 assume $C(\vec{v}, x, y, z)$ and $C(\vec{v}, x, y',z')$, where
    $z$ and $z'$ are sequence numbers of the same length $x + 2$.
    Furthermore we have $C_0(\vec{v}, \pi_2((z)_0), (z)_0)$
    and $C_0(\vec{v}, \pi_2((z')_0), (z')_0)$, so we have $(z)_0 = (z')_0$.

    Similarly we have $\forall i < x \:\: C_1(\vec{v}, i, \pi_2((z)_i), \pi_2((z)_{i + 1}), (z)_{i + 1})$
    and the same formula where $z$ is replaced by $z'$.
    So if $(z)_i = (z')_i$, and one can deduce $(z)_{i + 1} = (z')_{i + 1}$ using the uniquness assumption
    for $C_1$. By $\Delta_0(exp)$-induction we obtain $\forall i \leq x \:\: ((z)_i = (z')_i)$.

    The final conjuncts in $C$ give $(z)_{x + 1} = \pi_2((z)_x) = y$ and the same formulas where $z$ is replaced by $z'$ 
    and where $y$ is replaced by $y'$. But since $(z)_x = (z')_x$ we have $y = y'$, since all the components are equal,
    $z = z'$. Thus we have $F(\vec{v}, x, y) \land F(\vec{v}, x, y') \to y = y'$.
  \end{enumerate}
  \end{enumerate}
\end{proof}

\subsection{${\bf I}\Sigma_1$ provable functions are primitive recursive}

\begin{definition}
  A closed $\Sigma_1$-formula $\exists \vec{z} B(z)$ with $B \in \Delta_0(exp)$ is said to be
  ``true at $m$'' (denoted as $m \models \exists \vec{z} B(z)$) if there are numbers
  $\vec{m} = (m_1, \dots, m_l)$ such that all $m_i < m$ for $i \in \{1, \dots, l\}$
  such that $B(\vec{m})$ is true in the standard model.
  
  A finite set of formulas $\Gamma$ of closed $\Sigma_1$-formulas is ``true at $m$'' 
  (denoted as $m \models \Gamma$) if at least one of them is true at $m$.

  If $\Gamma(x_1, \dots, x_k)$ is a finite set of $\Sigma_1$-formulas all of whose
  free variables occur amongst $x_1, \dots, x_k$ and if $f : \mathbb{N}^k \to \mathbb{N}$, then we write 
  $f \models \Gamma$ if for each assignments $\vec{n} = (n_1, \dots, n_k)$ to the variables  $x_1, \dots, x_k$
  we have $f(\vec{n}) \models \Gamma(\vec{n})$.
\end{definition}

Note that we have the persistence property for $\models$ 
which completely repeats persistence for ${\bf I}\Delta_0(exp)$.

We shall be using a Tait-style formalisation of ${\bf I}\Sigma_0$ where the induction rule

\begin{prooftree}
  \AxiomC{$\Gamma, A(0)$}
  \AxiomC{$\Gamma, \neg A(y), A(y + 1)$}
  \BinaryInfC{$\Gamma, A(t)$}
\end{prooftree}

where $y$ is not free in $\Gamma$, $t$ is any term and $A$ is any $\Sigma_1$-formula.

\begin{lemma} ($\Sigma_1$-induction)
  Let $\Gamma(\vec{x})$ be a finite set of $\Sigma_1$-formulas such that
  \begin{center}
    ${\bf I}\Sigma_1 \vdash \bigvee \Gamma(\vec{x})$
  \end{center}
  then there is a primitive recursive function $f$ such that $f \models \Gamma$ and $f$ is strictly increasing
  on its variables.
\end{lemma}

\begin{proof}
  We note that if $\Gamma$ is provable in this formalisation, then
it has a proof in which all the non-atomic cut formulas are induction $\Sigma_1$-formulas.
If $\Gamma$ is classically derivable from non-logical axioms $A_1, \dots, A_s$,
then there is a cut-free proof (\`{a} la Tait) of $\neg A_1, \Delta, \Gamma$ where $\Delta = A_2, \dots, A_s$.
Then if $A_1$ is an induction axiom on a formula $F$, then we have have a cut-free proof of
\begin{center}
  $F(0) \land \forall y (\neg F(y) \lor F(y + 1)) \land \neg F(t), \Delta, \Gamma$
\end{center}
and thus, by inversion, we have cut-free proofs of $F(0), \Delta, \Gamma$, 
$\neg F(y), F(y + 1), \Delta, \Gamma$ and $\neg F(t), \Delta, \Gamma$.

So we obtain $F(t), \Delta, \Gamma$ by the induction rule and then we obtain $\Delta, \Gamma$ by cutting $F(t)$.
One can detach $\neg A_2, \dots, \neg A_s$, so we finally have a proof of $\Gamma$ which uses cuts only
on $\Sigma_1$-induction formulas or on atoms arising from non-logical axioms. Such proofs are said to be ``free-cut'' free.

Let us choose such a proof for $\Gamma(\vec{x})$ and show by induction on the height of a proof that there exists
a primitive recursive function satisfying $f \models \Gamma$.

\begin{enumerate}
  \item Let $\Gamma(\vec{x})$ be an axiom, the for all $\vec{n}$ $\Gamma(\vec{n})$ contains a true atom.
  Choose $f(\vec{n}) = n_1 + \dots + n_k$, and $f$ is clearly primitive recursive, strictly incrasing and $f \models \Gamma$.
  \item Assume we have
  \begin{prooftree}
    \AxiomC{$\Gamma, B_0, B_1$}
    \RightLabel{$\lor$}
    \UnaryInfC{$\Gamma, B_0 \lor B_1$}
  \end{prooftree}
  Then both $B_0$ and $B_1$ are both $\Delta_0(exp)$-formulas, so any function $f$ satisfying 
  $f \models \Gamma, B_0, B_1$ also satisfies $\Gamma, B_0 \lor B_1$.
  \item Assume we have
  \begin{prooftree}
    \AxiomC{$\Gamma, B_0$}
    \AxiomC{$\Gamma, B_1$}
    \RightLabel{$\land$}
    \BinaryInfC{$\Gamma, B_0 \land B_1$}
  \end{prooftree}
  By the induction hypothesis we have $f_i(\vec{n}) \models \Gamma(\vec{n}), B_i(\vec{n})$ where $i \in \{0,1\}$ for all $\vec{n}$.
  By the persistence property, $\lambda \vec{n}. f_0(\vec{n}) + f_1(\vec{n}) \models \Gamma, B_0 \land B_1$.
  \item Assume we have
  \begin{prooftree}
    \AxiomC{$\Gamma, B(y)$}
    \RightLabel{$\forall$}
    \UnaryInfC{$\Gamma, \forall y B(y)$}
  \end{prooftree}
  where $y$ is not free in $\Gamma$. As far as all formulas are $\Sigma_1$, $\forall y B(y)$ 
  must be ${\bf I}\Delta_0(exp)$, so $B(y) \eqcirc \neg(y < t) \lor B'(y)$ for some elemetary or primitive recursive term $t$.
  Assume we have $f_0 \models \Gamma, \neg(y < t) \lor B'(y)$ for some increasing primitive recursive function $f_0$.
  Then, for any assignments $\vec{x} \mapsto \vec{n}$ and $y \mapsto k$, we have
  \begin{center}
    $f_0(\vec{n}, k) \models \Gamma(\vec{n}), \neg (k < t(\vec{n})), B'(\vec{n}, k)$.
  \end{center}
  We let
  \begin{center}
    $f(\vec{n}) = \sum \limits_{k < g(\vec{n})} f_0(\vec{n}, k)$
  \end{center}
  for some function $g$, which is increasing primitive recursive bounding the values of term $t$.
  So we have either $f(\vec{n}) \models \Gamma$ or $B'(\vec{n}, k)$ is true for every $k < t(\vec{n})$.
  Thus $f \models \Gamma, \forall y B(y)$ as required.
  \item Suppose we have
  \begin{prooftree}
    \AxiomC{$\Gamma, A(t)$}
    \RightLabel{$\exists$}
    \UnaryInfC{$\Gamma, \exists y A(y)$}
  \end{prooftree}
  where $A$ is a $\Sigma_1$-formula. By the induction hypothesis we have a function $f_0$ such that for all $\vec{n}$
  \begin{center}
    $f_0(\vec{n}) \models \Gamma(\vec{n}), A(t(\vec{n}), \vec{n})$
  \end{center}
  Then either $f_0(\vec{n}) \models \Gamma(\vec{n})$ or otherwise $f_0(\vec{n})$ 
  bounds true witnesses for all the existential quantifiers already in $A(t(\vec{n}, \vec{n}))$.
  Choose an elementary bounding function $g$ for the term $t$ and define 
  $f(\vec{n}) = f_0(\vec{n}) + g(\vec{n})$, so we have either 
  $f(\vec{n}) \models \Gamma(\vec{n})$ or $f(\vec{n}) \models \exists y A(y, \vec{n})$ for all $\vec{n}$.
  \item Assume we have
  \begin{prooftree}
    \AxiomC{$\Gamma, \forall \vec{z} \neg B(\vec{z})$}
    \AxiomC{$\Gamma, \exists \vec{z} B(\vec{z})$}
    \RightLabel{{\bf cut}}
    \BinaryInfC{$\Gamma$}
  \end{prooftree}
  where $\exists \vec{z} B(\vec{z})$ is a cut $\Sigma_1$-formula.
  
  Note that we have
  \begin{prooftree}
    \AxiomC{$\Gamma, \neg B(\vec{z})$}
    \RightLabel{$\forall$}
    \UnaryInfC{$\Gamma, \forall \vec{z} \neg B(\vec{z})$}
  \end{prooftree}
  Note $B$ is a $\Delta_0(\exp)$-formula, so let us apply the induction hypothesis to obtain a
  primitive recursive function $f_0$ such that for each assignments $\vec{x} \mapsto \vec{n}$ and $\vec{z} \mapsto \vec{m}$
  \begin{center}
    $f_0(\vec{n}, \vec{m}) \models \Gamma(\vec{n}), \neg B(\vec{n}, \vec{m})$.
  \end{center}
  We apply the induction hypothesis to the second premise to obtain a primitive recursive function $f_1$ such that
  for all $\vec{n}$ either $f_1(\vec{n}) \models \Gamma(\vec{n})$ or otherwise there are fixed witnesses 
  $\vec{m} < f_1(\vec{n})$ s.t. $B(\vec{n}, \vec{m})$ is true.
  Let us define $f$ by substitution:
  \begin{center}
    $f(\vec{n}) = f_0(\vec{n}, f_1(\vec{n}), \dots, f_1(\vec{n}))$
  \end{center}
  where $f$ is primitive recursive, greater or equal that $f_1$ (pointwise) and strictly increasing.
  Furthermore $f \models \Gamma$.
  
  For otherwise, let us suppose there exists a tuple $\vec{n}$ such that $\Gamma(\vec{n})$ is not true
  $f(\vec{n})$ and, thus, by persistence at $f_1(\vec{n})$. So $B(\vec{n}, \vec{m})$ is true for
  some $\vec{m} < f_1(\vec{n})$. Thus $f_0(\vec{n}, \vec{m}) < f(\vec{n})$, and then, by persistence, $\Gamma(\vec{n})$
  cannot be true at $f_0(\vec{n}, \vec{m})$. Then $B(\vec{n}, \vec{m})$, so we have a contradiction.
  \item Suppose we have
  \begin{prooftree}
    \AxiomC{$\Gamma(\vec{x}), A(\vec{x}, 0)$}
    \AxiomC{$\Gamma, \neg A(\vec{x}, y), A(\vec{x}, y + 1)$}
    \BinaryInfC{$\Gamma, A(\vec{x}, t)$}
  \end{prooftree}
  where $A(\vec{x}, y)$ is an induction $\Sigma_1$-formula of the form $\exists \vec{z} B(\vec{x}, y, \vec{z})$.
  Let us invert universal quantifiers in $\neg A(\vec{x}, y)$, the second premise of the rule becomes
   \begin{center}
     $\Gamma(\vec{x}), \neg B(\vec{x}, y, \vec{z}), A(\vec{x}, y + 1)$
   \end{center}
  which is now a set $\Sigma_1$-formulas. We can apply the induction hypothesis to each of the premises to
  have primitive recursive function $f_0$ and $f_1$ such that for each $\vec{n}$, $k$ and $\vec{m}$
  \begin{center}
    $f_0(\vec{n}) \models \Gamma(\vec{n}), A(\vec{n}, 0)$

    $f_1(\vec{n}, k, \vec{m}) \models \Gamma(\vec{n}), \neg B(\vec{n}, k, \vec{m}), A(\vec{n}, k + 1)$
  \end{center}
  Define $f$ by primitive recursion from $f_0$ and $f_1$ the following way
  \begin{center}
    $f(\vec{n}, 0) = f_0(\vec{n})$

    $f(\vec{n}, k + 1) = f_1(\vec{n}, k, f(\vec{n}, k), \dots, f(\vec{n}, k))$
  \end{center}

  Then for all $\vec{n}$ and for all $\vec{k}$ one has $f(\vec{n}, k) \models \Gamma(\vec{n}), A(\vec{n}, k)$ which is shown
  by induction on $k$.
  The base case holds by the definition of $f_0(\vec{n})$. For the induction step assume that 
  $f(\vec{n}, k) \models \Gamma(\vec{n}), A(\vec{n}, k)$. If $\Gamma(\vec{n})$ is not true at 
  $f(\vec{n}, k + 1)$. By persistence it is not true at $f(\vec{n}, k)$ and thus 
  $f(\vec{n}, k) \models A(\vec{n}, k)$.
  Therefore there are numbers $\vec{m} < f(\vec{n}, k)$ such that $B(\vec{n}, k, \vec{m})$ is true.
  Thus $f_1(\vec{n}, k, \vec{m}) \models \Gamma(\vec{n}), A(\vec{n}, k + 1)$ and since 
  $f_1(\vec{n}, k, \vec{m}) \leq f(\vec{n}, k + 1)$ we have, by persistence, $f(\vec{n}, k + 1) \models \Gamma(\vec{n}), A(\vec{n}, k + 1)$
  as required.

  So we substitute for the final argument $k$ in $f$ an elementary (or primitive recursive) function $g$
  which bounds the values of $t$, so that $f'(\vec{n}) = f(\vec{n}, g(\vec{n}))$, and thus
  we have $f(\vec{n}, t(\vec{n})) \models \Gamma(\vec{n}), A(\vec{n}, t(\vec{n}))$ for all $\vec{n}$ and thus, by persistence, $f' \models \Gamma(\vec{x}), A(\vec{x}, t)$.
\end{enumerate}
\end{proof}

\begin{theorem}
  The provably recursive functions of ${\bf I}\Sigma_1$ are exactly primitive recursive functions.
\end{theorem}

\begin{proof}
  We have already shown that all primitive recursive functions are provably recursive in ${\bf I}\Sigma_1$,
  so let us show the converse.

  Let $g : \mathbb{N}^k \to \mathbb{N}$ is $\Sigma_1$ be a function defined by a $\Sigma_1$-formula
  $F(\vec{x}, y) := \exists z C(\vec{x}, y, z)$ where $C$ is $\Delta_0(exp)$ and
  ${\bf I}\Sigma_1 \models \exists y F(\vec{x}, y)$.
  By the lemma above, there exists a primitive recursive function $f$ such that for all $n \in \mathbb{N}^k$
  \begin{center}
    $f(\vec{n}) \models \exists y \exists z C(\vec{n}, y, z)$.
  \end{center}
  That is, for every $\vec{n}$ there is an $m < f(\vec{n})$ and a $k < f(\vec{n})$
  such that $C(\vec{n}, m, k)$ is true and this $m$ is the value of $g(\vec{n})$.

  $g$ can be defined by primitive recursion from $f$ the following way:

  \begin{center}
    $g(\vec{n}) = (\mu_{m < h(\vec{n})} C(\vec{n}, (m)_0, (m)_1))$
  \end{center}
  where $h(\vec{n}) = \langle f(\vec{n}), f(\vec{n}) \rangle$.
\end{proof}

\section{$\varepsilon_0$-recursion in Peano Arithmetic}

We show that the provably recursive functions of Peano arithmetic are $\varepsilon_0$-recursive functions, that is, 
functions definable from the primitive recursive functions by substitutions and recursion over well-orderings
of natural numbers with order types strictly less than the ordinal
\begin{center}
  $\varepsilon_0 = \sup \{ \omega, \omega^{\omega}, \omega^{\omega^{\omega}}, \dots \}$
\end{center}
Equivalently, $\varepsilon_0$ can be defined as the least fixed point of the mapping $\alpha \mapsto \omega^{\alpha}$
where $\alpha$ is an ordinal.

Let us discuss first how one can represent ordinals below $\varepsilon_0$.

\subsection{Ordinals below $\varepsilon_0$}

Every ordinal $\alpha < \varepsilon_0$ is either $0$ or $\alpha$ can be represented uniquely in \emph{Cantor normal form}:
\begin{center}
  $\alpha = \omega^{\gamma_1} \cdot c_1 + \omega^{\gamma^{\gamma_1}} \cdot c_2 + \dots + \omega^{\gamma_k} \cdot c_k$
\end{center}
where $k < \omega$, $\gamma_k < \dots < \gamma_2 < \gamma_1 < \alpha$ and $c_1, \dots, c_k < \omega$ are coefficients.
If $\gamma_k = 0$, then $\alpha$ is a successor ordinal, written $\operatorname{Succ}(\alpha)$, 
and its predecessor $\alpha - 1$ the representation
\begin{center}
  $\alpha = \omega^{\gamma_1} \cdot c_1 + \omega^{\gamma^{\gamma_1}} \cdot c_2 + \dots + \omega^{\gamma_{k - 1}} \cdot c_{k - 1}$.
\end{center}

Otherwise $\alpha$ is a limit ordinal, written $\operatorname{Lim}(\alpha)$, and it has infinitely many possible
increasing sequences of smaller ordinals whose limit is $\alpha$.

We shall pick out one concrete sequence $\{ \alpha(n) \: | \: n < \omega \}$ for each limit ordinal $\alpha$ the following way.
First write $\alpha$ as $\delta + \omega^{\gamma}$ where
\begin{center}
  $\delta = \omega^{\gamma_1} \cdot c_1 + \dots + \omega^{\gamma_k} \cdot (c_k - 1)$

  $\gamma = \gamma_k$.
\end{center}

By induction we can assume that when $\gamma$ is a limit ordinal, its fundamental sequence 
$\{ \gamma(n) \: | \: n < \omega \}$ has been already specified. We let for each $n < \omega$
\begin{center}
  $\alpha(n) = \begin{cases}
    \delta + \omega^{\gamma - 1} \cdot (n + 1), \text{if $\operatorname{Succ}(\gamma)$} \\
    \delta + \omega^{\gamma(n)}, \text{if $\operatorname{Lim}(\gamma)$}.
  \end{cases}$
\end{center}
Clearly
\begin{center}
  $\alpha = \lim \limits_{n \to \omega} \alpha(n)$.
\end{center}

\begin{definition}
  Let $\alpha < \varepsilon_0$ and $n < \omega$, define a finite set of ordinals $\alpha[n]$ the following way:

  \begin{center}
    $\alpha[n] = \begin{cases}
      \emptyset, \text{if $\alpha = 0 $} \\
      (\alpha - 1)[n] \cup \{ \alpha - 1 \}, \text{if $\operatorname{Succ}(\alpha)$} \\
      \alpha(n)[n], \text{ if $\operatorname{Lim}(\alpha)$}
    \end{cases}$
  \end{center}
\end{definition}

\begin{lemma}~\label{ord:sum:1}
  For each $\alpha = \delta + \omega^{\gamma}$ and for each $n < \omega$
  \begin{center}
    $\alpha[n] = \delta[n] \cup \{ \delta + \omega^{\gamma_1} \cdot c_1 + \dots + \omega^{\gamma_k} \cdot c_k \: | \: \forall i (\gamma_i \in \gamma[n] \land c_i \leq n) \}$.
  \end{center}
\end{lemma}

\begin{proof}
  Induction on $\gamma$.

  \begin{enumerate}
    \item $\gamma = 0$, then $\gamma[n] = \emptyset$ and the right hand side is 
    $\delta[n] \cap \{ \delta \}$, which is the same as $\alpha[n] = (\delta + 1)[n]$.
    \item If $\gamma$ is limit, then $\gamma[n] = \gamma(n)[n]$, so the right hand side
    is the same as the one with $ \gamma(n)[n]$ instead of $\gamma[n]$. 
    By the induction hypothesis applied to $\alpha(n) = \delta + \omega^{\gamma(n)}$, which is equal to 
    $\alpha(n)[n]$, which is $\alpha[n]$ by definition.
    \item Suppose $\gamma$ is a successor. Then $\alpha$ is a limit and $\alpha[n] = \alpha(n)[n]$,
    where $\alpha(n) = \delta + \omega^{\gamma - 1} \cdot (n + 1)$.
    So we can write $\alpha(n) = \alpha(n - 1) + \omega^{\gamma - 1}$, where $\alpha(-1) = \delta$ when $n = 0$.
    By the induction hypothesis for $\gamma - 1$, the set $\alpha[n]$ equals
    \begin{center}
      $\alpha(n - 1)[n] \cup \{ \alpha(n - 1) + \omega^{\gamma_1} \cdot c_1 + \dots + \omega^{\gamma_k} \cdot c_k \: | \: \forall i (\gamma_1 \in (\gamma - 1)[n] \land c_i \leq n)\}$
    \end{center}
    and similarly for each $\alpha(n - 1)[n], \alpha(n - 2)[n], \dots, \alpha(1)[n]$. 
    For each $m \leq n$, $\alpha(m - q) = \delta + \omega^{\gamma - 1} \cdot m$. In turn, this last set is the same
    as
    \begin{center}
      $\delta[n] \cup \{ \delta + \omega^{\gamma - 1} \cdot m + \omega^{\gamma_1} \cdot c_1 + \dots + \omega^{\gamma_k} \cdot c_k \: | \: \forall i (\gamma_i \in (\gamma - 1)[n] \land c_i \leq n) \land m \leq n\}$
    \end{center}
    and this is the set since $\gamma[n] = (\gamma - 1)[n] \cup \{ \gamma - 1\}$.
  \end{enumerate}
\end{proof}

\begin{col}
  Let $\alpha < \varepsilon_0$ be a limit ordinal, then for every $0 \neq n < \omega$ $\alpha(n) \in \alpha[n + 1]$.
  Furthermore if $\beta \in \gamma[n]$, then $\omega^{\beta} \in \omega^{\gamma}[n]$.
\end{col}

\begin{definition}
The \emph{maximum coefficient} of $\beta = \omega^{\beta_1} \cdot b_1 + \dots + \omega^{\beta_l} \cdot b_l$
is defined by induction to be the maximum of all the $b_i$'s and all the maximum coefficients of the exponents $\beta_i$'s.
\end{definition}

\begin{lemma}
  If $\beta < \alpha$ and the maximum coefficient of $\beta$ is $\leq n$, so $\beta \in \alpha[n]$. 
\end{lemma}

\begin{proof}
  By induction on $\alpha$. Let $\alpha = \delta + \omega^{\gamma}$. If $\beta < \delta$,
  then $\beta \in \delta[n]$ by the induction hypothesis and $\delta[n] \subseteq \alpha[n]$ by Lemma~\ref{ord:sum:1}.
  Otherwise
  \begin{center}
    $\beta = \delta + \omega^{\beta_1} \cdot b_1 + \dots + \omega^{\beta_k} \cdot b_k$
  \end{center}
  for $\alpha > \gamma > \beta_1 > \dots > \beta_k$ and $b_i \leq n$. By induction hypothesis 
  $\beta_i \in \gamma[n]$, so $\beta \in \alpha[n]$ by Lemma~\ref{ord:sum:1}.
\end{proof}

\begin{definition}
  Let $G_{\alpha}(n)$ denote the cardinality of the finite set $\alpha[n]$.
  We have

  \begin{center}
  $G_{\alpha}(n) = \begin{cases}
    0, \text{if $\alpha = 0$} \\
    G_{\alpha - 1}(n + 1), \text{ if $\operatorname{Succ}(\alpha)$} \\
    G_{\alpha(n)}(n), \text{if $\operatorname{Lim}(\alpha)$} 
  \end{cases}$
  \end{center}
\end{definition}

The hierarchy of functions $G_{\alpha}$ is the \emph{slow-growing} hierarchy.

\begin{lemma} If $\alpha = \delta + \omega^{\gamma}$, then for all $n < \omega$
  \begin{center}
    $G_{\alpha}(n) = G_{\delta}(n) + (n + 1)^{G_{\gamma}(n)}$.
  \end{center}
  Thus for each $\alpha < \varepsilon_0$, $G_{\alpha}(n)$ is the elementary function
  which results by substituting $n + 1$ for every occurence of $\omega$ in the Cantor normal form $\omega$.
\end{lemma}

\begin{proof}
  Induction on $\gamma$.

  \begin{enumerate}
    \item If $\gamma = 0$, then $\alpha = \delta + 1$, thus
    \begin{center}
      $G_{\alpha}(n) = G_{\delta}(n) + 1 =  G_{\delta}(n) + (n + 1)^0$.
    \end{center}
    \item If $\gamma$ is a successor, then $\alpha = \delta + \omega^{\gamma}$ is limit
    and $\alpha(n) = \delta + \omega^{\gamma - 1} \cdot (n + 1)$, so
    we apply the induction hypothesis for $\gamma - 1$ $n + 1$ times and thus we have
    \begin{center}
      $G_{\alpha}(n) = G_{\alpha(n)}(n) = G_{\delta}(n) + (n + 1)^{G_{\gamma - 1}(n)} \cdot (n + 1) = G_{\delta}(n) + (n + 1)^{G_{\gamma}(n)}$
    \end{center}
    since $G_{\gamma - 1}(n) + 1 = G_{\gamma}(n)$.
    \item If $\gamma$ is a limit ordinal, then $\alpha(n) = \delta + \omega^{\gamma(n)}$, 
    so let us apply the induction hypothesis to $\gamma(n)$, then we have
    \begin{center}
      $G_{\alpha}(n) = G_{\alpha(n)}(n) = G_{\delta}(n) + (n + 1)^{G_{\gamma(n)}(n)}$
    \end{center}
    which gives the result since $\Gamma_{\gamma(n)}(n) = G_{\gamma}(n)$.
  \end{enumerate}
\end{proof}

\begin{definition} {\bf (Coding ordinals)}

  Let $\beta = \omega^{\beta_1} \cdot b_1 + \dots \omega^{\beta_l} \cdot b_l$ be an ordinal.
  A \emph{coding ordinal} is the sequence number $\overline{\beta}$ constructed recursively the following way
  \begin{center}
    $\overline{\beta} = \langle \langle \overline{\beta_1}, b_1 \rangle, \dots, \langle \overline{\beta_l}, b_l \rangle \rangle$.
  \end{center}
\end{definition}
where $0$ is coded by the empty sequence number. $\overline{\beta}$ is numerically greater than the maximum
coefficient of $\beta$ and greater than the codes $\overline{\beta_i}$ of all its exponents and their exponents, etc.

\begin{lemma}
  $ $
  \begin{enumerate}
    \item There exists an elementary function $h : \mathbb{N} \times \mathbb{N} \to \mathbb{N}$ such that, for each
    ordinal $\beta = \omega^{\beta_1} \cdot b_1 + \dots \omega^{\beta_l} \cdot b_l$:
    \begin{center}
      $h(\overline{\beta}, n) = \begin{cases}
        0, \text{if $\beta = 0$} \\
        \overline{\beta - 1}, \text{if $\operatorname{Succ}(\beta)$} \\
        \overline{\beta(n)}, \text{if $\operatorname{Lim}(\beta)$}
      \end{cases}$
    \end{center}
    \item For each ordinal $\alpha < \varepsilon_0$ there exists an elementary well-ordering 
    $\prec_{\alpha} \subset \mathbb{N} \times \mathbb{N}$ such that
    \begin{center}
      $\forall b, c \in \mathbb{N} \:\: b \prec_{\alpha} c \leftrightarrow \exists \beta, \gamma < \alpha \:\: \beta < \gamma \: \& \: b = \overline{\beta} \: \& \: c = \overline{\gamma}$.
    \end{center}
  \end{enumerate}
\end{lemma}

\begin{proof} $ $

  \begin{enumerate}
    \item First let
    \begin{center}
      $h(0, n) = 0$
    \end{center}
    for any $n$. Then let $0 < m < \omega$ be a non-zero sequence number.
    We first should see if the rightmost component $\pi_2$ is a pair $(m', n')$. 
    If so and $m' = 0$ and $n' \neq 0$, then $\beta$ is a successor and the code of its predecessor, 
    $h(m, n)$, is defined as the new sequence number that we obtain by reducing $n'$ by one or by removing
    this final component if $n' = 1$.

    If $\pi_2(m) = \langle m', n' \rangle$ where both $m'$ and $n'$ are non-zero, then $\beta$ is a limit ordinal of then form
    $\delta + \omega^{\gamma} \cdot n'$ where $m' = \overline{\gamma}$. Let $k$ be the code of
    $\delta + \omega^{\gamma} \cdot (n' - 1)$, which is obtained by reducing $n'$ by one inside $m$ 
    (or by deleting the final component from $m$ when $n' = 1$).

    At the ``right hand end'' of $\beta$ we have a ``spare'' $\omega^{\gamma}$ which must be either reduced to
    $\omega^{\gamma - 1} \cdot (n + 1)$ when $\operatorname{Succ}(\gamma)$ or to $\omega^{\gamma(n)}$ if $\operatorname{Lim}(\gamma)$.
    In either case we are able to produce $\beta(n)$. Thus the required code $h(m, n)$ of $\beta(n)$
    will be obtained by tagging on to the end of the sequence number $k$ one additinal pair encoding this additional term.

    If we assume inductively that $h(m', n)$ has been already defined for $m' < m$, then such an additional component
    is either $\langle h(m',n), n + 1\rangle$ if $\operatorname{Succ}(\gamma)$ or $\langle h(m',n), 1\rangle$
    if $\operatorname{Lim}(\gamma)$.

    This defines $h(m,n)$, but such a definition is actually primitive recursive so far. Let us chech that $h$ is
    elementarily bounded, i.e. $h$ is defined by limited recursion from elementary functions.
    Note that $h(m, n) < m$ whenever $m$ codes a successor ordinal. If $m$ codes a limit ordinal, 
    $h(m,n)$ is obtained from the sequence number $k < m$ by adding a new pair on the end.
    An extra item $i$ is tagged on the end of a sequence number $k$ by the function $\pi(k, i)$ which is quadratic in both argument.
    If the item added is the pair $\langle h(m', n), n + 1 \rangle$ where $\operatorname{Succ}(\gamma)$,
    then $h(m', n) < m$, so $h(m, n)$ is numerically bounded by some fixed polynomial in $m$ and $n$.
    In the other case, we can say that $h(m, n)$ is numerically bounded by some fixed polynomial of $m$ and $h(m',n)$.
    Since $m'$ codes an exponent in the Cantor normal form encoded by $m$, the second polynomial
    is iterated at most $d$ times, where $d$ is the ``exponential height'' of the normal form.
    Thus $h(m, n)$ is bounded by some $d$-times iterated polynomial of $m + n$.
    $d < m$, so $h(m, n)$ is bounded by the elementary function $2^{2^{c \cdot (m + n)}}$ for some $c < \omega$.
    Therefore $h$ is elementary as it is defined by bounded recursion.
    \item Let $\alpha < \varepsilon_0$ and let $d$ be the exponential height of its Cantor normal form.
    We use the function $h$ from the previous part, but we apply it to codes below $\alpha$ only. They have the exponential
    height $\leq d$, so we can consider $h$ as being bounded by some fixed polynomial of its two arguments.
    Define $g(0, n) = \overline{\alpha}$ and $g(i+1,n) = h(g(i, n), n)$ and notice that $g$ is therefore bounded by an $i$-times iterated polynomial,
    so $g$ is defined by an elementarily limited recursion from $h$, so it is elementary.
    
    Define $b \prec_{\alpha} c$ if and only if $c \neq 0$ and there are $i$ and $j$ such that 
    $0 < i < j \leq G_{\alpha}(\max(b, c) + 1)$ and $g(i, \max(b,c)) = c$ and $g(j, \max(b,c)) = b$.
    The function $g$ and $G_{\alpha}$ are elementary, so is the relation $\prec_{\alpha}$ since the quantifiers are
    bounded. By the properties of $h$ it is clear that if $i < j$ then $g(j, \max(b,c))$ codes an ordinal greater than 
    $g(j, \max(b,c))$. Hence $b \prec_{\alpha} c$, then $b = \overline{\beta}$ and $c = \overline{\gamma}$ for some
    $\beta < \gamma < \alpha$.

    Now assume $b = \overline{\beta}$, $c = \overline{\gamma}$ and $\beta < \gamma < \alpha$.
    The code of an ordinal is greater than its maximal coefficient, so we have
    $\beta \in \alpha[\max(b,c)]$ and $\gamma \in \alpha[\max(b,c)]$. Thus the sequence starting
    with $\alpha$ and at each stage descending from a $\delta$ to either $\delta - 1$ if
    $\operatorname{Succ}(\delta)$ or $\delta(\max(b,c))$ if $\operatorname{Lim}(\delta)$ necessarily 
    passes through $\gamma$ and then through $\beta$.
    In turn, it means there are $i, j < \omega$ such that $0 < i < j$, $g(i, \max(b, c)) = c$, $g(j, \max(b, c)) = b$.
    So $b \prec_{\alpha} c$ holds if we can show that $j \leq G_{\alpha}(\max(b,c) + 1)$.
    In the sequence described above, only the successor stages contribute an element $\delta - 1$ to
    $\alpha[\max(b,c)]$. At the limit stages $\delta(\max(b, c))$ does not get put it.
    Although $\delta(n)$ does not belong to $\delta[n]$, it does belong to $\delta[n + 1]$.
    Therefore all the ordinals in the descending sequence lie in $\alpha[\max(b,c) + 1]$, so $j$ can not be bigger
    than the cardinality of this set, which is $G_{\alpha}(\max(b,c) + 1)$.
  \end{enumerate}
\end{proof}

The moral is that the principles of transfinite induction and recursion over the initials segments of ordinals below
$\varepsilon_0$ can be expressed by means of ${\bf I}\Delta_0(exp)$.

\subsection{Introducing the fast-growing hierarchy}

\begin{definition}
  The \emph{Hardy hierarchy} $\{ H_{\alpha}\}_{\alpha < \varepsilon_0}$ is defined by recursion on $\alpha$:

  \begin{center}
    $H_{\alpha}(n) = \begin{cases}
      n, \text{ if $\alpha = 0 $} \\
      H_{\alpha - 1}(n + 1), \text{if $\operatorname{Succ}(\alpha)$} \\
      H_{\alpha(n)}(n), \text{if $\operatorname{Lim}(\alpha)$}
    \end{cases}$
  \end{center}

  The \emph{fast-growing hierarchy} $\{ F_{\alpha} \}_{\alpha < \varepsilon_0}$ is defined by recursion on $\alpha$:

  \begin{center}
    $F_{\alpha}(n) = \begin{cases}
      n + 1, \text{if $\alpha = 0$} \\
      F^{n + 1}_{\alpha - 1}(n), \text{if $\operatorname{Succ}(\alpha)$} \\
      F_{\alpha(n)}(n), \text{if $\operatorname{Lim}(\alpha)$}
    \end{cases}$
  \end{center}
  where $F^{n + 1}_{\alpha - 1}(n)$ is the $(n + 1)$-times iteration of $F_{\alpha - 1}$ on $n$.
\end{definition}

Note that $H_{\alpha}$ and $F_{\alpha}$ could be equivalently defined by purely number-theoretic means
by working over the well-orderings $\prec_{\alpha}$ instead of working over ordinals directly. So $H_{\alpha}$ and
$F_{\alpha}$ are $\varepsilon_0$-recursive.

\begin{lemma}
  For all $\alpha, \beta < \varepsilon_0$ and for all $n < \omega$,

  \begin{enumerate}
    \item $H_{\alpha + \beta}(n) = H_{\alpha}(H_{\beta}(n))$,
    \item $H_{\omega^{\alpha}}(n) = F_{\alpha}(n)$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  The first part is proved by induction on $\beta$. If $\beta = 0$, then the equation trivially holds.
  Assume $\operatorname{Succ}(\beta)$ and the induction hypothesis for $\beta -1$, then
  we have:
  \begin{center}
    $H_{\alpha + \beta}(n) = H_{\alpha + (\beta - 1)}(n + 1) = H_{\alpha}(H_{\beta - 1}(n + 1)) = H_{\alpha}(H_{\beta}(n))$.
  \end{center}

  If $\operatorname{Lim}(\beta)$, then we have (by using the induction hypothesis for $\beta(n)$):
  \begin{center}
    $H_{\alpha + \beta}(n) = H_{\alpha + \beta(n)}(n) = H_{\alpha}(H_{\beta(n)}(n)) = H_{\alpha}(H_{\beta}(n))$.
  \end{center}

  The second part is proven by induction on $\alpha$. If $\alpha = 0$, then

  \begin{center}
    $H_{\omega^0}(n) = H_1(n) = n + 1 = F_0(n)$
  \end{center}

  If $\operatorname{Succ}(\alpha)$, then

  \begin{center}
    $H_{\omega^{\alpha}}(n) = H_{\omega^{\alpha - 1} \cdot (n + 1)}(n) = H^{n + 1}_{\omega^{\alpha - 1}}(n) = F^{n + 1}_{\alpha - 1}(n) = F_{\alpha}(n)$.
  \end{center}

  The limit case is immediate.
\end{proof}

\begin{lemma}
  For each $\alpha < \varepsilon_0$, $H_{\alpha}$ is strictly increasing and $H_{\beta}(n) < H_{\alpha}(n)$
  for $\beta \in \alpha[n]$. The same holds for $F_{\alpha}$ for $n \neq 0$, for when $n = 0$ we have $F_{\alpha}(0) = 1$
  for each $\alpha$.
\end{lemma}

\begin{proof}
  Induction on $\alpha$. The case $\alpha = 0$ is trivial since $H_0$ is the identity function and $0[n] = \emptyset$.
  If $\operatorname{Succ}(\alpha)$, then $H_{\alpha}$ is $H_{\alpha - 1}$ composed with the successor function,
  it is strictly increasing by the induction hypothesis. 
  Take $\beta \in \alpha[n]$, then either $\beta \in (\alpha -1)[n]$ or $\beta = \alpha - 1$, thus, by using the induction hypothesis
  \begin{center}
    $H_{\beta}(n) \leq H_{\alpha - 1}(n) < H_{\alpha - 1}(n + 1) = H_{\alpha}(n)$.
  \end{center}

  If $\operatorname{Lim}(\alpha)$ then 
  \begin{center}
  $H_{\alpha}(n) = H_{\alpha(n)}(n) < H_{\alpha(n)}(n + 1)$
  \end{center}
  but $\alpha(n) \in \alpha[n + 1] = \alpha(n + 1)[n + 1]$, thus
  \begin{center}
    $H_{\alpha(n)}(n + 1) < H_{\alpha(n + 1)}(n + 1) = H_{\alpha}(n + 1)$
  \end{center}
  Thus $H_{\alpha}(n) < H_{\alpha}(n + 1)$. Furthermore if $b \in \alpha[n]$,
  then $\beta \in \alpha(n)[n]$ so $H_{\beta}(n) < H_{\alpha(n)}(n) = H_{\alpha}(n)$ by the induction hypothesis
  for $\alpha(n)$.

  The same holds for $F_{\alpha} = H_{\omega^{\alpha}}$ since if $\beta \in \alpha[n]$ we then have
  $\omega^{\beta} \in \omega^{\alpha}[n]$.
\end{proof}

\begin{lemma}
If $\beta \in \alpha[n]$, then $F_{\beta+1}(m) \leq F_{\alpha}(m)$ for all $m \geq n$.
\end{lemma}

\begin{proof}
Induction on $\alpha$. The zero case is trivial.
If $\operatorname{Succ}(\alpha)$, then either $\beta \in (\alpha - 1)[n]$ or $\beta = \alpha - 1$. In either case
we apply the induction hypothesis. If $\alpha$ is a limit, then we have $\beta \in \alpha(n)[n]$, so
by induction hypothesis $F_{\beta + 1}(m) \leq F_{\alpha(n)}(m)$, but $F_{\alpha(n)}(m) \leq F_{\alpha}(m)$.
\end{proof}

\subsection{$\alpha$-recursion and $\varepsilon_0$-recursion}

\begin{definition}[\bf $\alpha$-recursion]
  $ $

  \begin{enumerate}
    \item An \emph{$\alpha$-recursion} if a function definition of the following form, 
    defining $f : \mathbb{N}^{k + 1} \to \mathbb{N}$ from functions $g_0, g_1, \dots, g_s$ by the following equations:
    \begin{center}
      $f(0, \vec{m}) = g_0(\vec{m})$

      $f(n, \vec{m}) = T(g_1, \dots, g_s, f_{\prec n}, n, \vec{m})$ provided $n \geq 1$.
    \end{center}
    where $T(g_1, \dots, g_s, f_{\prec n}, n, \vec{m})$ is a fixed term built up from the number variables
    $n$ and $\vec{m}$ by applying functions $g_1, \dots, g_s$ and the function $f_{\prec n}$ defined as
    \begin{center}
      $f_{\prec n}(n', \vec{m}) = \begin{cases}
        f(n', \vec{m}), \text{if $n' \prec_{\alpha} n$} \\
        0, \text{otherwise}
      \end{cases}$
    \end{center}
    Note that it is assumed that $\alpha > 0$.
    \item An \emph{unnested} $\alpha$ is one of the special form:

    \begin{center}
      $f(0, \vec{m}) = g_0(\vec{m})$

      $f(n, \vec{m}) = g_1(n, \vec{m}, f(g_2(n, \vec{m}), \dots, g_{k + 1}(n, \vec{m})))$
    \end{center}
    with a single recursive call of $f$ where $g_2(n, \vec{m}) \prec_{\alpha} n$ for all $n$ and $\vec{m}$.
    \item Let $\varepsilon_0(0) = \omega$ and $\varepsilon_0(i + 1) = \omega^{\varepsilon_0(i)}$.
    For each particular $i$, a function is \emph{$\varepsilon_0(i)$-recursive} if it can be defined
    from primitive recursive functions by successive substitutions and $\alpha$-recursions with 
    $\alpha < \varepsilon_0(i)$. It is \emph{unnested $\varepsilon_0(i)$-recursive} if all the $\alpha$-recursions
    are unnested. It is \emph{$\varepsilon_0$-recursive} if it is $\varepsilon_0(i)$-recursive for some (any) $i$.
  \end{enumerate}
\end{definition}

\begin{lemma}[\bf Bounds for $\alpha$-recursion]
  Let $f$ be a function defined from $g_1, \dots, g_s$ by an $\alpha$-recursion:
  \begin{center}
    $f(0, \vec{m}) = g_0(\vec{m})$

    $f(n, \vec{m}) = T(g_1, \dots, g_s, f_{\prec n}, n, \vec{m})$
  \end{center}
  where for each $i \leq s$ $g_i(\vec{a}) < F_{\beta}(k + \max \vec{a})$ for all numerical arguments $\vec{a}$.
  Then there is a constant $d$ such that for all $n$, $\vec{m}$
  \begin{center}
    $f(n, \vec{m}) < F_{\alpha + \beta}(k + 2 d + \max(n, \vec{m}))$.
  \end{center}
\end{lemma}
Note that $\beta$ and $k$ are arbitrary constants, but it is assumed that the last exponent in the Cantor normal
form of $\beta$ is $\geq$ the first exponent in the normal form of $\alpha$, so that $\beta + \alpha$ is in Cantor normal
form by default.

\begin{proof}
  The constant $d$ will be actually the depth of nesting of the term $T$, where variables
  have depth have depth $0$ and each compositional term $g(T_1, \dots, T_l)$ has depth greater than the
  maximum depth of nesting of the subterms $T_j$.

  Assume $n$ lies in the field of the well-ordering $\prec_{\alpha}$. 
  Then $n = \overline{\gamma}$ for some $\gamma < \alpha$. Let us claim by induction on $\gamma$ that
  \begin{center}
    $f(n, \vec{m}) < F_{\beta + \gamma + 1}(k + 2 d+ \max(n, \vec{m}))$.
  \end{center}
  This is immediate when $n = 0$, because $g_0(\vec{m}) < F_{\beta}(k + \max \vec{m})$
  and $F_{\beta}$ is strictly increasing and bounded by $F_{\beta + 1}$.
  Assume $n \neq 0$ and assume the claim for all $n' = \vec{\delta}$ where $\delta < \gamma$.

  Let $T'$ be any subterm of $T(g_1, \dots, g_s, f_{\prec n},n, \vec{m})$ with depth of nesting $d'$,
  built up by application of one of the functions $g_1, \dots, g_s$ or $f_{\prec n}$ to subterms $T_1, \dots, T_l$.
  Assume (for a sub-induction on $d'$) that each of these $T_j$'s has numerical value $v_j$ less that 
  $F^{2 (d' - 1)}_{\beta + \gamma}(k + 2 d + \max(n, \vec{m}))$.

  If $T'$ is obtained by application of one of the functions $g_i$ then its numerical value will be
  \begin{center}
    $g_i(v_1, \dots, v_l) < F_{\beta}(k + 2^{d'-1}_{\beta + \gamma})(k + 2 d + \max(n, \vec{m})) < F^{2 d'}_{\beta + \gamma}(k + 2 d + \max(n, \vec{m}))$
  \end{center}
  since $k < u$ then $F_{\beta}(k + u) < F_{\beta}(2 u) < F^2_{\beta}(u)$ provided $\beta \neq 0$. On the other hand,
  if $T'$ is obtained by application of the function $f_{\prec n}$, its value will be $f(v_1, \dots, v_l)$
  if $v_1 \prec_{\alpha} n$ or $0$ otherwise. Suppose $v_1 = \overline{\delta} \prec_{\alpha} \overline{\gamma}$.
  So by the induction hypothesis:
  \begin{center}
    $f(v_1, \dots, v_l) < F_{\beta + \delta + 1}(k + 2 d + \max \vec{v}) \leq F_{\beta + \gamma}(k + 2 d + \max \vec{v})$
  \end{center}
  because $v_1$ is greater than the maximum coefficient of $\delta$, so $\delta \in \gamma[v_1]$, so
  $\beta + \delta \in (\beta + \gamma)[v_1]$ and hence $F_{\beta + \gamma + 1}$ is bounded by $F_{\beta + \gamma}$
  on arguments $\geq v_1$.
  TODO: complete the proof
\end{proof}

\section{${\bf RCA}_0$}

\section{${\bf WKL}_0$}

\section{${\bf ACA}_0$}

\section{${\bf ATR}$}

\section{${\bf \Pi_1^1}$-comprehension}

\section{Kripke-Platek Set Theory}


\bibliographystyle{alpha}
\bibliography{Text}

\end{document}